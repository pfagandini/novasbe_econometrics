[
  {
    "objectID": "03_Inference.html#review-1",
    "href": "03_Inference.html#review-1",
    "title": "Inference",
    "section": "Review",
    "text": "Review\n\nMoments and distribution for \\(\\hat{\\beta}\\)"
  },
  {
    "objectID": "03_Inference.html#exercises",
    "href": "03_Inference.html#exercises",
    "title": "Inference",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "03_Inference.html#exercise-3.1",
    "href": "03_Inference.html#exercise-3.1",
    "title": "Inference",
    "section": "Exercise 3.1",
    "text": "Exercise 3.1\n\nOne of the most important decisions business leaders face concerns the allocation of the firm‚Äôs budget to the different departments. To help in the decision of finding out how much money to allocate to the R&D department you decide to collect data for 32 firms in the chemical industry on the following variables: rdintensity (expenditures on r&d as % of sales), profmarg (profits as % of sales), and sales (in millions of USD). You then estimate the following equation (se in parenthesis):\n\n\n\\[\n\\widehat{profmarg}=\\underset{(6.3043)}{11.76}+\\underset{(0.7233)}{0.79}rdintensity-\\underset{(0.8814)}{0.63}\\log(sales)\n\\]\n\n\nWith \\(R^2=0.0464\\)"
  },
  {
    "objectID": "03_Inference.html#ex-3.1-questions",
    "href": "03_Inference.html#ex-3.1-questions",
    "title": "Inference",
    "section": "Exercise 3.1",
    "text": "Exercise 3.1\n\n\nInterpret the coefficient on rdintensity. Is this an economically significant effect?\nDoes R&D intensity have a statistical significant effect on profit margin? Do the 2-sided test at a 10% level.\nSteve Jobs once said: ‚ÄúInnovation has nothing to do with how many R&D dollars you have. When Apple came up with the Mac, IBM was spending at least 100 times more on R&D. It‚Äôs not about money. It‚Äôs about the people you have, how you‚Äôre led, and how much you get it.‚Äù Does this exercise provide evidence in favor or against Steve Jobs‚Äô claim?\n\n\nt-student\n\n\nIf rdintensity increases by 1 pp profit margin increases by 0.79pp on average, ceteris paribus.\n\\(H_0: \\beta_{rintensity}=0\\), \\(H_a: \\beta_{rintensity}\\neq0\\). \\(\\alpha=10\\%\\), \\(df=32-2-1=29\\), \\(t^c=1.699\\). \\(t=\\frac{0.79-0}{0.7233}=1.09&lt;t^c\\). Fail to reject \\(H_0\\) at 10%.\nWe do not find statistically significant effect of r&d on profit, maybe it is not only about the money."
  },
  {
    "objectID": "03_Inference.html#exercise-3.3",
    "href": "03_Inference.html#exercise-3.3",
    "title": "Inference",
    "section": "Exercise 3.3",
    "text": "Exercise 3.3\nConsider the estimated equation from a model explaining college GPA with the high school GPA (hsGPA), the achievenment test score (ACT) and average number of lectures missed per week (skipped) as explanatory variables (usual standar errors in parenthesis below the estimates):\n\n\\[\n\\widehat{colGPA}=\\underset{(0.33)}{1.39}+\\underset{(0.094)}{0.412}hsGPA + \\underset{(0.011)}{0.015}ACT-\\underset{(0.026)}{0.083}skipped\n\\]"
  },
  {
    "objectID": "03_Inference.html#ex-3.3-questions",
    "href": "03_Inference.html#ex-3.3-questions",
    "title": "Inference",
    "section": "Exercise 3.3",
    "text": "Exercise 3.3\n\nUsing the standard normal approximation, find the 95% confidence interval for \\(\\beta_{hsGPA}\\).\nCan you reject the hypothesis \\(H_0:\\beta_{hsGPA}=0.4\\) against the twosided alternative at the 5% significance level?\nCan you reject the hypothesis \\(H_0: \\beta_{hsGPA}=1\\) against the two-sided alternative at the 5% significance level?\n\nt-student\n\n\n\\(0.412\\pm 1.96\\times 0.094\\Rightarrow [0.228, 0.596]\\)\n\\(H_0: \\beta_1=0.4\\) cannot be rejected at 5% significance as 0.4 is in the confidence interval.\n\\(H_0: \\beta_1=1\\) is rejected at 5% significance as 1 falls out of the CI."
  },
  {
    "objectID": "03_Inference.html#exercise-3.4",
    "href": "03_Inference.html#exercise-3.4",
    "title": "Inference",
    "section": "Exercise 3.4",
    "text": "Exercise 3.4\nConsider the following model:\n\n\\[\n\\log(scrap)=\\beta_0+\\beta_1 hrsemp + \\beta_2 \\log(sales)+\\beta_3 \\log(employ)+u\n\\]\n\nwhere hrsemp is annual hours of training per employee, sales is annual firm sales (in dollars), and employ is the number of firm employees."
  },
  {
    "objectID": "03_Inference.html#ex-3.4-questions",
    "href": "03_Inference.html#ex-3.4-questions",
    "title": "Inference",
    "section": "Exercise 3.4",
    "text": "Exercise 3.4\n\nShow that the population model can also be written as\n\n\\[\n\\log(scrap)=\\beta_0+\\beta_1 hrsemp + \\beta_2 \\log(sales/employ)+\\theta_3 \\log(employ)+u\n\\]\n\nwhere \\(\\theta_3=\\beta_2+\\beta_3\\)\nInterpret the null hypothesis \\(H_0: \\theta_3=0\\).\n\n\n\nsubtract and add \\(\\beta_2\\log(employ)\\) and then you get the result.\n\\(H_0: \\theta_3=0\\) can be written as \\(\\beta_2+\\beta_3=0\\) or \\(\\beta_2=-\\beta_3\\), that is under the null hypothesis the effects cancel out, or are symmetric."
  },
  {
    "objectID": "03_Inference.html#ex-3.4-questions2",
    "href": "03_Inference.html#ex-3.4-questions2",
    "title": "Inference",
    "section": "Exercise 3.4",
    "text": "Exercise 3.4\n\nUsing 43 observations, you estimate the original equation:\n\n\\[\n\\widehat{\\log(scrap)}=\\underset{(4.57)}{11.74}+\\underset{(0.019)}{0.043} hrsemp - \\underset{(0.370)}{0.951}\\log(sales) + \\underset{(0.360)}{0.992}\\log(employ)\n\\]\n\nand when the second equation is estimated, we obtain:\n\n\\[\n\\widehat{\\log(scrap)}=\\underset{(4.57)}{11.74}+\\underset{(0.019)}{0.043} hrsemp - \\underset{(0.370)}{0.951}\\log(sales/employ) + \\underset{(0.205)}{0041}\\log(employ)\n\\]\n\nWith \\(R^2=0.310\\) for both regressions. Test if the effect of annual firm sales (in USD) on scrap rates is the symmetric of the effect of the number of firm employees on scrap rates.\n\nt-student\n\n\\(H_0:\\theta_3=0\\) and \\(H_a: \\theta_3\\neq 0\\). For \\(\\alpha=5\\%\\) and \\(dof=43-3-1=39\\) the critical value \\(t^c=2.023\\). In this case \\(t=\\frac{0.041}{0.205}=0.2\\). Because \\(t&lt;t^c\\) we fail to reject \\(H_0\\) at 5%."
  },
  {
    "objectID": "03_Inference.html#exercise-3.7",
    "href": "03_Inference.html#exercise-3.7",
    "title": "Inference",
    "section": "Exercise 3.7",
    "text": "Exercise 3.7\n\nWith the goal of evaluating the environmental impact of different management policies of Natural Parks, a study has analyzed data for 28 Natural Parks in country X. The following regression was estimated: \\[\n\\hat{y} = \\underset{(-1.085)}{-12.666}+\\underset{(17.299)}{3.743}x_{1}-\\underset{(-2.164)}{9.614}x_{2}\n\\]\nwhere \\(y\\) is the number of animals and plants in extintion danger in each park, \\(x_1\\) is the number of visits to the park (in thousands), and \\(x_2\\) is the expenditures on nature conservation (in million euro). Consider also that \\(Cov\\left(\\hat{\\beta}_1,\\hat{\\beta}_2\\right)=-0.8835\\). The numbers in parenthesis are t-statistics."
  },
  {
    "objectID": "03_Inference.html#exercise-3.7-1",
    "href": "03_Inference.html#exercise-3.7-1",
    "title": "Inference",
    "section": "Exercise 3.7",
    "text": "Exercise 3.7\n\nInterpret the parameter estimates.\nKnowing that \\(R^2=0.984\\), test the overall significance of the regression.\nThe director of the Natural Parks association is worried about the number of vists to the parks: ‚Äú‚Ä¶ if not for the nature, at least in financial terms we have to reduce the number of visits, because in order to compensate for an increase of one thousand visitors we have to spend 0.5 million euros more in maintenance‚Äù. Comment on this sentence.\n\n\n\nIf visitors increase by 1000 the N of endangered species increases by 3.73 on average ceteris paribus. If maintenance increases by 1 million euros, the number of endangered species decreases by 9.614, on average, ceteris paribus.\n\\(F=\\frac{R^2_{ur}/k}{(1-R^2_{ur})/(n-k-1)}\\sim F_{k,n-k-1}\\) here \\(F=\\frac{0.984/2}{(1-0.984)/(28-2-1)}=768.75\\) Since \\(F&gt;F^c=F_{5\\%,2.25}=3.39\\) we reject \\(H_0\\). The coefficients are jointly statistical significant, or the model is statistical significant.\nWe want to test if 0.5 million euro compensate the impact on the number of endangered species for having 1000 more visits to the park: \\(H_0: -\\beta_1=0.5\\beta_2\\) or \\(H_0=\\beta_1+0.5\\beta_2=0\\). Test for \\(\\hat{\\rho}=\\hat{\\beta}_1+0.5\\hat{\\beta}_2\\): \\(t=\\frac{\\hat{\\rho}}{se(\\hat{\\rho})}=\\frac{3.743-0.5(9.614)}{se(\\hat{\\rho})}\\) \\(se(\\hat{\\rho})=\\sqrt{V(\\hat{\\beta}_1+0.5\\hat{\\beta}_2)}\\) and that is \\(\\sqrt{V(\\hat{\\beta}_1)+0.5^2 V(\\hat{\\beta}_2)+2\\times 0.5Cov(\\hat{\\beta}_1, \\hat{\\beta}_2)}=2.024\\), leading to \\(t=-0.526\\). Thus, we fail to reject \\(H_0\\) as \\(|t|&lt;|t^c|\\), and we have evidence that spending 0.5 million eur in maintenance compensates the impact on the number of endangered species of having 1000 more visitors."
  },
  {
    "objectID": "03_Inference.html#app-t-table",
    "href": "03_Inference.html#app-t-table",
    "title": "Inference",
    "section": "t table",
    "text": "t table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf-\\(\\alpha/2\\)\n0.50\n0.25\n0.20\n0.15\n0.10\n0.05\n0.025\n0.01\n0.005\n0.001\n0.0005\n\n\n\n\n1\n0.000\n1.000\n1.376\n1.963\n3.078\n6.314\n12.71\n31.82\n63.66\n318.31\n636.62\n\n\n2\n0.000\n0.816\n1.061\n1.386\n1.886\n2.920\n4.303\n6.965\n9.925\n22.327\n31.599\n\n\n3\n0.000\n0.765\n0.978\n1.250\n1.638\n2.353\n3.182\n4.541\n5.841\n10.215\n12.924\n\n\n4\n0.000\n0.741\n0.941\n1.190\n1.533\n2.132\n2.776\n3.747\n4.604\n7.173\n8.610\n\n\n5\n0.000\n0.727\n0.920\n1.156\n1.476\n2.015\n2.571\n3.365\n4.032\n5.893\n6.869\n\n\n6\n0.000\n0.718\n0.906\n1.134\n1.440\n1.943\n2.447\n3.143\n3.707\n5.208\n5.959\n\n\n7\n0.000\n0.711\n0.896\n1.119\n1.415\n1.895\n2.365\n2.998\n3.499\n4.785\n5.408\n\n\n8\n0.000\n0.706\n0.889\n1.108\n1.397\n1.860\n2.306\n2.896\n3.355\n4.501\n5.041\n\n\n9\n0.000\n0.703\n0.883\n1.100\n1.383\n1.833\n2.262\n2.821\n3.250\n4.297\n4.781\n\n\n10\n0.000\n0.700\n0.879\n1.093\n1.372\n1.812\n2.228\n2.764\n3.169\n4.144\n4.587\n\n\n11\n0.000\n0.697\n0.876\n1.088\n1.363\n1.796\n2.201\n2.718\n3.106\n4.025\n4.437\n\n\n12\n0.000\n0.695\n0.873\n1.083\n1.356\n1.782\n2.179\n2.681\n3.055\n3.930\n4.318\n\n\n13\n0.000\n0.694\n0.870\n1.079\n1.350\n1.771\n2.160\n2.650\n3.012\n3.852\n4.221\n\n\n14\n0.000\n0.692\n0.868\n1.076\n1.345\n1.761\n2.145\n2.624\n2.977\n3.787\n4.140\n\n\n15\n0.000\n0.691\n0.866\n1.074\n1.341\n1.753\n2.131\n2.602\n2.947\n3.733\n4.073\n\n\n16\n0.000\n0.690\n0.865\n1.071\n1.337\n1.746\n2.120\n2.583\n2.921\n3.686\n4.015\n\n\n17\n0.000\n0.689\n0.863\n1.069\n1.333\n1.740\n2.110\n2.567\n2.898\n3.646\n3.965\n\n\n18\n0.000\n0.688\n0.862\n1.067\n1.330\n1.734\n2.101\n2.552\n2.878\n3.610\n3.922\n\n\n19\n0.000\n0.688\n0.861\n1.066\n1.328\n1.729\n2.093\n2.539\n2.861\n3.579\n3.883\n\n\n20\n0.000\n0.687\n0.860\n1.064\n1.325\n1.725\n2.086\n2.528\n2.845\n3.552\n3.850\n\n\n21\n0.000\n0.686\n0.859\n1.063\n1.323\n1.721\n2.080\n2.518\n2.831\n3.527\n3.819\n\n\n22\n0.000\n0.686\n0.858\n1.061\n1.321\n1.717\n2.074\n2.508\n2.819\n3.505\n3.792\n\n\n23\n0.000\n0.685\n0.858\n1.060\n1.319\n1.714\n2.069\n2.500\n2.807\n3.485\n3.768\n\n\n24\n0.000\n0.685\n0.857\n1.059\n1.318\n1.711\n2.064\n2.492\n2.797\n3.467\n3.745\n\n\n25\n0.000\n0.684\n0.856\n1.058\n1.316\n1.708\n2.060\n2.485\n2.787\n3.450\n3.725\n\n\n26\n0.000\n0.684\n0.856\n1.058\n1.315\n1.706\n2.056\n2.479\n2.779\n3.435\n3.707\n\n\n27\n0.000\n0.684\n0.855\n1.057\n1.314\n1.703\n2.052\n2.473\n2.771\n3.421\n3.690\n\n\n28\n0.000\n0.683\n0.855\n1.056\n1.313\n1.701\n2.048\n2.467\n2.763\n3.408\n3.674\n\n\n29\n0.000\n0.683\n0.854\n1.055\n1.311\n1.699\n2.045\n2.462\n2.756\n3.396\n3.659\n\n\n30\n0.000\n0.683\n0.854\n1.055\n1.310\n1.697\n2.042\n2.457\n2.750\n3.385\n3.646\n\n\n40\n0.000\n0.681\n0.851\n1.050\n1.303\n1.684\n2.021\n2.423\n2.704\n3.307\n3.551\n\n\n60\n0.000\n0.679\n0.848\n1.045\n1.296\n1.671\n2.000\n2.390\n2.660\n3.232\n3.460\n\n\n80\n0.000\n0.678\n0.846\n1.043\n1.292\n1.664\n1.990\n2.374\n2.639\n3.195\n3.416\n\n\n100\n0.000\n0.677\n0.845\n1.042\n1.290\n1.660\n1.984\n2.364\n2.626\n3.174\n3.390\n\n\n1000\n0.000\n0.675\n0.842\n1.037\n1.282\n1.646\n1.962\n2.330\n2.581\n3.098\n3.300\n\n\nZ\n0.000\n0.674\n0.842\n1.036\n1.282\n1.645\n1.960\n2.326\n2.576\n3.090\n3.291\n\n\n\n\nBack to Exercise 3.1 Exercise 3.3 Exercise 3.4"
  },
  {
    "objectID": "00_Statistical_revision.html#introduction-1",
    "href": "00_Statistical_revision.html#introduction-1",
    "title": "Statistical Revision",
    "section": "Introduction",
    "text": "Introduction\n\nüë®‚Äçüè´ Paulo Fagandini\n\nüìß paulo.fagandini@novasbe.pt\n\nüìñ Wooldridge, J. M. (2020). Introductory Econometrics: A Modern Approach"
  },
  {
    "objectID": "00_Statistical_revision.html#introduction-2",
    "href": "00_Statistical_revision.html#introduction-2",
    "title": "Statistical Revision",
    "section": "Introduction",
    "text": "Introduction\nAssessment key points:\n\n5 Random Quizzes, total 10%\n1 Midterm, 15%\nEmpirical project, 25% (min grade 8.00)\nExam, 50% (min grade 8.00)\n\n\nResit replaces the Regular Exam, it is not worth 100%. You cannot take resit if you missed the Regular Exam."
  },
  {
    "objectID": "00_Statistical_revision.html#introduction-3",
    "href": "00_Statistical_revision.html#introduction-3",
    "title": "Statistical Revision",
    "section": "Introduction",
    "text": "Introduction\n\nMedical justifications are not accepted by us. They must be presented directly to the School, which will then inform us of the situation.\nBecause of the random nature of the quizzes, missed quizzes cannot be justified. Since unforeseen circumstances may occur and you might miss one, only the best four (out of five) quiz scores will count toward the final grade."
  },
  {
    "objectID": "00_Statistical_revision.html#introduction-4",
    "href": "00_Statistical_revision.html#introduction-4",
    "title": "Statistical Revision",
    "section": "Introduction",
    "text": "Introduction\nPortability of work projects from previous semesters:\n\nWork projects executed in group cannot be resubmitted.\nWork projects done individually with a grade 12.00 or better can be reutilized in the next 2 semesters. Note, however, that their grade may be modified."
  },
  {
    "objectID": "00_Statistical_revision.html#introduction-5",
    "href": "00_Statistical_revision.html#introduction-5",
    "title": "Statistical Revision",
    "section": "Introduction",
    "text": "Introduction\nThe course has theory and practical lectures:\n\nTheory lectures cover theory. This will be present in the assessments.\nPractical lectures focuses mostly on applications, which also enter in the assessments.\n\n\nTheory and practical lectures are not substitutes!!\nA typical mistake: Focus only on the practical part; neglect the theory and book. Exam is usually 50%/50% each!"
  },
  {
    "objectID": "00_Statistical_revision.html#random-variables",
    "href": "00_Statistical_revision.html#random-variables",
    "title": "Statistical Revision",
    "section": "Random Variables",
    "text": "Random Variables\n\n\\(X\\) is a random variable (rv) if it takes on numerical values and has an outcome that is determined by an experiment:\n\nA discrete rv can take on only a finite or countably infinite number of values.\nA continuous rv can take on any real value with zero probability."
  },
  {
    "objectID": "00_Statistical_revision.html#random-variables-1",
    "href": "00_Statistical_revision.html#random-variables-1",
    "title": "Statistical Revision",
    "section": "Random Variables",
    "text": "Random Variables\n\nA probability distribution is associateed with each rv.\nExamples:\n\nCoin-flipping example: number of heads in 10 flips of a coin\nHeight of a selected student"
  },
  {
    "objectID": "00_Statistical_revision.html#expected-value",
    "href": "00_Statistical_revision.html#expected-value",
    "title": "Statistical Revision",
    "section": "Expected Value",
    "text": "Expected Value\n\nIf \\(X\\) is a rv, the expected value is a weighted average of all values of \\(X\\)\nThe probability density function (pdf) determines the weights\nLet \\(f(x)\\) denote the pdf of \\(X\\) and \\(X\\) be a discrete random variable over a finite number of values \\(\\{x_1,\\dots,x_n\\}\\) \\[\\mu(X)=E[X]\\equiv \\sum_{i=1}^n f(x_i)x_i\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#expected-value-properties",
    "href": "00_Statistical_revision.html#expected-value-properties",
    "title": "Statistical Revision",
    "section": "Expected Value: Properties",
    "text": "Expected Value: Properties\n\n\\(E[c]=c\\) for \\(c\\) a constant.\n\\(E\\left[E[X]\\right]=E[X]=\\mu(X)\\)\nFor any \\(a,b\\in\\mathbb{R}\\), \\(E[aX+b]=aE[X]+b\\)\n\\(E[X-\\mu(X)]=0\\)\n\\(E\\left[(a X)^2\\right]=a^2 E[X^2]\\)\n\\(X\\) and \\(Y\\) are independent \\(\\Leftrightarrow\\) \\(E[XY]=E[X]E[Y]\\)"
  },
  {
    "objectID": "00_Statistical_revision.html#expected-value-properties-1",
    "href": "00_Statistical_revision.html#expected-value-properties-1",
    "title": "Statistical Revision",
    "section": "Expected Value: Properties",
    "text": "Expected Value: Properties\n\nIf \\(\\{a_1,\\dots,a_n\\}\\) are constants and \\(X_1,\\dots,X_n\\) are rvs, then: \\[E\\left[\\sum_{i=1}^n a_i X_i\\right] = \\sum_{i=1}^n a_i E[X_i]\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#variance",
    "href": "00_Statistical_revision.html#variance",
    "title": "Statistical Revision",
    "section": "Variance",
    "text": "Variance\n\nThe variance of \\(X\\) is a measure of the dispersion of the distribution\nLet \\(\\mu_X=E[X]\\). Then the variance is the expected of the squared deviation from the mean: \\[\\sigma_X^2=V[X]=E\\left[(X-\\mu_X)^2\\right]\\]\nNotice that: \\[ E[(X-\\mu_X)^2]=E\\left[X^2\\right]-\\mu_X^2\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#standard-deviation",
    "href": "00_Statistical_revision.html#standard-deviation",
    "title": "Statistical Revision",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\nThe square root of \\(V[X]\\) is the standard deviation of \\(X\\): \\[\\sigma_X=sd(X)\\equiv \\sqrt{V[X]}\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#the-variance-properties",
    "href": "00_Statistical_revision.html#the-variance-properties",
    "title": "Statistical Revision",
    "section": "The variance: Properties",
    "text": "The variance: Properties\n\nFor any constant \\(c\\), \\(V[c]=0\\)\nFor \\(a,b\\) constants, and \\(X\\) a rv, \\(V[aX+b]=a^2V[X]\\)\n\\(V[X+Y]=V[X]+V[Y]+2Cov[X,Y]\\)\n\\(V[X-Y]=V[X]+V[Y]-2Cov[X,Y]\\)\n\\(X\\) and \\(Y\\) independent \\(\\Leftrightarrow\\) \\(V[X\\pm Y]=V[X]+V[Y]\\)"
  },
  {
    "objectID": "00_Statistical_revision.html#the-covariance",
    "href": "00_Statistical_revision.html#the-covariance",
    "title": "Statistical Revision",
    "section": "The Covariance",
    "text": "The Covariance\n\nConcerns the relationship between two variables describing a population\nLet \\(\\mu_X=E[X]\\) and \\(\\mu_Y=E[Y]\\) with \\(X\\) and \\(Y\\) some rvs. \\[\n\\begin{aligned}\n  \\sigma_{XY}=Cov(X,Y)&\\equiv E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]\\\\\n  &=E[XY]-\\mu_X\\mu_Y\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#the-covariance-properties",
    "href": "00_Statistical_revision.html#the-covariance-properties",
    "title": "Statistical Revision",
    "section": "The Covariance: Properties",
    "text": "The Covariance: Properties\n\nIf \\(X\\) and \\(Y\\) are independent, then \\(Cov(X,Y)=0\\)\nFor any constants \\(a_1,b_1\\) and \\(a_2,b_2\\): \\[Cov(a_1 X+b_1,a_2 Y+b_2)=a_1a_2Cov(X,Y)\\]\nCauchy-Schwartz Inequality: \\[|Cov(X,Y)|\\leq \\sigma_X \\sigma_Y\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#the-correlation-coefficient",
    "href": "00_Statistical_revision.html#the-correlation-coefficient",
    "title": "Statistical Revision",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\nThe covariance between two random variables depends on the units of measurement.\nThe correlation coefficient overcomes this issue:\\[\\rho_{XY}=Corr(X,Y)\\equiv \\frac{\\sigma_{XY}}{\\sigma_X\\sigma_Y}=\\]\nSince \\(\\sigma_X\\) and \\(\\sigma_Y\\) are positive, then \\(Cov(X,Y)\\) and \\(Corr(X,Y)\\) have the same sign.\n\\(Corr(X,Y)=0\\) \\(\\Leftrightarrow\\) \\(Cov(X,Y)=0\\)"
  },
  {
    "objectID": "00_Statistical_revision.html#the-correlation-coefficient-1",
    "href": "00_Statistical_revision.html#the-correlation-coefficient-1",
    "title": "Statistical Revision",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\n\\(-1\\leq Corr(X,Y)\\leq 1\\)\n\n\\(Corr(X,Y)=Cov(X,Y)=0\\ \\Rightarrow\\ \\) \\(X\\) and \\(Y\\) are independent.\n\\(Corr(X,Y)=1\\), then \\(X\\) and \\(Y\\) are perfectly positively correlated.\n\\(Corr(X,Y)=-1\\), then \\(X\\) and \\(Y\\) are perfectly negatively correlated\n\nFor any constants \\(a_1, b_1, a_2, b_2\\) with \\(a_1\\neq 0\\) and \\(a_2\\neq 0\\):\\[Corr(a_1X+b_1,a_2 Y + b_2)=\\frac{a_1 a_2}{|a_1 a_2|} Corr(X,Y)\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#variance-of-sums-of-rvs",
    "href": "00_Statistical_revision.html#variance-of-sums-of-rvs",
    "title": "Statistical Revision",
    "section": "Variance of Sums of RVs",
    "text": "Variance of Sums of RVs\n\nFor any constants \\(a\\) and \\(b\\): \\[V[aX+bY]=a^2V[X]+b^2V[Y]=2ab Cov(X,Y)\\]\nIf \\(X\\) and \\(Y\\) are uncorrelated, \\[V[X\\pm Y]=V[X]+V[Y]\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#variance-of-sums-of-rvs-1",
    "href": "00_Statistical_revision.html#variance-of-sums-of-rvs-1",
    "title": "Statistical Revision",
    "section": "Variance of Sums of RVs",
    "text": "Variance of Sums of RVs\nGeneralizing, * Let \\(\\{X_1,\\dots,X_n\\}\\) be pairwise uncorrelated rvs and \\(\\{a_1,\\dots,a_n\\}\\) constants, then:\\[V\\left[\\sum_{i=1}^n a_i X_i\\right]=\\sum_{i=1}^n a_i^2 V[X_i] \\]"
  },
  {
    "objectID": "00_Statistical_revision.html#conditional-expectation",
    "href": "00_Statistical_revision.html#conditional-expectation",
    "title": "Statistical Revision",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\nFor any function \\(c(X)\\) \\(E[c(X)|X=x]=c(X)\\)\nFor any functions \\(a(X)\\) and \\(b(X)\\):\\[E[a(x)Y+b(X)|X]=a(X)E[Y|X]+b(X)\\]\nIf \\(X\\) and \\(Y\\) are independent, then \\(E[Y|X]=E[Y]\\)."
  },
  {
    "objectID": "00_Statistical_revision.html#conditional-expectation-1",
    "href": "00_Statistical_revision.html#conditional-expectation-1",
    "title": "Statistical Revision",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\\(E\\left[E[Y|X]\\right]=E[Y]\\)\n\\(E[Y|X]=E\\left[E[Y|X,Z]|X\\right]\\)\n\\(E[Y|X]=E[X]\\) then \\(Cov(X,Y)=Corr(X,Y)=0\\)"
  },
  {
    "objectID": "00_Statistical_revision.html#conditional-variation",
    "href": "00_Statistical_revision.html#conditional-variation",
    "title": "Statistical Revision",
    "section": "Conditional Variation",
    "text": "Conditional Variation\n\n\\(V[Y|X=x]=E[Y^2|X=x]-\\left[E[Y|X=x]^2\\right]\\)\nIf \\(X\\) and \\(Y\\) are independent, then \\(V[Y|X]=V[Y]\\)."
  },
  {
    "objectID": "00_Statistical_revision.html#vector-operations",
    "href": "00_Statistical_revision.html#vector-operations",
    "title": "Statistical Revision",
    "section": "Vector Operations",
    "text": "Vector Operations\nLet \\(v\\) and \\(u\\) be \\(n\\times 1\\) vectors:\n\nFor \\(a\\in\\mathbb{R}\\), \\((av)_i=a v_i\\).\n\\((v+u)_i=v_i+u_i\\).\n\\(v^T\\) is a \\(1\\times n\\) vector\n\\(v^T v = \\sum_{i=1}^n v_i^2\\)\n\\(v^T u = u^T v = \\sum_{i=1}^n u_i v_u\\). This is crucial, it is a linear combination of the elements of \\(v\\) or \\(u\\)."
  },
  {
    "objectID": "00_Statistical_revision.html#vector-operations-1",
    "href": "00_Statistical_revision.html#vector-operations-1",
    "title": "Statistical Revision",
    "section": "Vector Operations",
    "text": "Vector Operations\nLet \\(1_n\\) be a vector of 1, of size \\(n\\times 1\\), then:\n\nFor \\(a\\in\\mathbb{R}\\), \\((a 1_n)_i=a\\)\n\\(v^T 1_n = \\sum_{i=1}^n v_i\\)"
  },
  {
    "objectID": "00_Statistical_revision.html#system-of-equations",
    "href": "00_Statistical_revision.html#system-of-equations",
    "title": "Statistical Revision",
    "section": "System of equations",
    "text": "System of equations\nThe system:\n\\[\n\\begin{aligned}\n    y_{1}&=&\\beta_0 &+& \\beta_1 x_1 &+& u_1\\\\\n    y_{2}&=&\\beta_0 &+& \\beta_1 x_2 &+& u_2\\\\\n    \\vdots &=& \\vdots &+& \\vdots &+& \\vdots \\\\\n    y_{n}&=& \\beta_0 &+& \\beta_1 x_n &+& u_n\n\\end{aligned}\n\\]\nIs equivalent to \\[Y = \\beta_0 1_n + \\beta_1 X + u\\] where \\(Y\\), \\(X\\), and \\(u\\) are vectors \\(n\\times 1\\)."
  },
  {
    "objectID": "00_Statistical_revision.html#matrix-multiplication",
    "href": "00_Statistical_revision.html#matrix-multiplication",
    "title": "Statistical Revision",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nIf you have a matrix \\(M\\) of size \\(m\\times n\\), then it can be:\n\npre multiplied by a matrix or vector, as long as it has \\(m\\) columns.\npost multiplied by a matrix or vector, as long as it has \\(n\\) rows."
  },
  {
    "objectID": "00_Statistical_revision.html#matrix-multiplication-1",
    "href": "00_Statistical_revision.html#matrix-multiplication-1",
    "title": "Statistical Revision",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nSay \\(A=\\begin{pmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{pmatrix}\\) and \\(B=\\begin{pmatrix}b_{11} & b_{12}\\\\ b_{21} & b_{22}\\end{pmatrix}\\), we have that\n\\[\nA\\times B = \\begin{pmatrix}a_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22}\\\\a_{21}b_{11}+a_{22}b_{21}&a_{21}b_{12}+a_{22}b_{22}\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#matrix-multiplication-2",
    "href": "00_Statistical_revision.html#matrix-multiplication-2",
    "title": "Statistical Revision",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nMore generally, the element \\((i,j)\\) of the matrix multiplication \\(A\\) of size \\(n_a\\times T\\), and \\(B\\) of size \\(T\\times m_b\\) is:\n\\[\\left[AB\\right]_{ij}=\\sum_{q=1}^m a_{im}b_{mj}\\]\nOr the sum of the products of the elements of the \\(i^{th}\\) row of \\(A\\) with the elements of the \\(j^{th}\\) column of \\(B\\). Better, a linear combination of the elements of the \\(i^{th}\\) row of \\(A\\), or a linear combination of the \\(j^{th}\\) column of \\(B\\)."
  },
  {
    "objectID": "00_Statistical_revision.html#system-of-equations-1",
    "href": "00_Statistical_revision.html#system-of-equations-1",
    "title": "Statistical Revision",
    "section": "System of equations",
    "text": "System of equations\nThe system of equations \\[Y = \\beta_0 1_n + \\beta_1 X + u\\] where \\(Y\\), \\(X\\), and \\(u\\) are vectors \\(n\\times 1\\) can be rewritten as:\n\\[Y = \\begin{pmatrix}1_n & X\\end{pmatrix} \\begin{pmatrix}\\beta_0 \\\\ \\beta_1\\end{pmatrix} + u\\]"
  },
  {
    "objectID": "00_Statistical_revision.html#system-of-equations-2",
    "href": "00_Statistical_revision.html#system-of-equations-2",
    "title": "Statistical Revision",
    "section": "System of equations",
    "text": "System of equations\nIf we rename \\(X\\) as the matrix \\(\\begin{pmatrix}1_n & X\\end{pmatrix}\\) and we call \\(\\beta=\\begin{pmatrix}\\beta_0\\\\ \\beta_1\\end{pmatrix}\\) we have that the system can be written as: \\[Y=X\\beta+u\\]\nWhere \\(X\\) is now a matrix size \\(n\\times (k+1)\\), where \\(n\\) will be the number of observations, and \\(k\\) the number of variables we use to explain \\(Y\\) (we will see in the future that we can have more than 1 variable \\(X\\)). In any case, this representation encompases all the cases!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "temp",
    "section": "",
    "text": "This website has the materials for the practical lectures of Econometrics at the bachelor programs at Nova SBE."
  },
  {
    "objectID": "renv.html",
    "href": "renv.html",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "A quick reference for using renv (R package management) from the terminal without RStudio.\n\n\n# Start R in your project directory\nR\n\n# In R console, initialize renv\nrenv::init()\n\n# Exit R\nquit()\n\n\n\n# Activate environment (if not auto-activated)\nR -e \"renv::activate()\"\n\n# Install packages (in R console)\nR\n&gt; install.packages(\"package_name\")\n&gt; renv::snapshot()  # Save current state\n&gt; quit()\n\n# Restore environment from lockfile\nR -e \"renv::restore()\"\n\n\n\nRun these commands in the R console:\nrenv::status()      # Check project status\nrenv::snapshot()    # Update lockfile with current packages\nrenv::restore()     # Install packages from lockfile\nrenv::clean()       # Remove unused packages\nrenv::deactivate()  # Deactivate renv for this session\nrenv::activate()    # Reactivate renv\n\n\n\nyour_project/\n‚îú‚îÄ‚îÄ renv.lock        # Package versions (like requirements.txt)\n‚îú‚îÄ‚îÄ .Rprofile        # Auto-activates renv\n‚îî‚îÄ‚îÄ renv/            # Private library directory\n    ‚îú‚îÄ‚îÄ library/     # Project-specific packages\n    ‚îî‚îÄ‚îÄ settings.dcf # renv settings\n\n\n\n\nrenv.lock: Contains exact package versions and dependencies (commit to git)\n.Rprofile: Auto-activates renv when starting R in the project directory\nrenv/: Private library where packages are stored\n\n\n\n\n# 1. Create new project\nmkdir my_r_project && cd my_r_project\n\n# 2. Initialize renv\nR -e \"renv::init()\"\n\n# 3. Install packages\nR -e \"install.packages(c('dplyr', 'ggplot2')); renv::snapshot()\"\n\n# 4. Work on your project with any editor\n# Your R scripts will use the renv environment automatically\n\n# 5. Share project - others run:\nR -e \"renv::restore()\"\n\n\n\n\nAlways run renv::snapshot() after installing new packages\nThe .Rprofile file automatically activates renv when you start R\nUse renv::status() to check if packages need to be snapshotted\nShare renv.lock with collaborators (like requirements.txt in Python)\nKeep renv/ folder out of version control (add to .gitignore)\nUse renv::clean() periodically to remove unused packages\n\n\n\n\n# If renv isn't working properly\nrenv::diagnostics()\n\n# Reset renv (use with caution)\nrenv::init(force = TRUE)\n\n# Check renv version\nrenv::version()"
  },
  {
    "objectID": "renv.html#basic-setup",
    "href": "renv.html#basic-setup",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "# Start R in your project directory\nR\n\n# In R console, initialize renv\nrenv::init()\n\n# Exit R\nquit()"
  },
  {
    "objectID": "renv.html#daily-workflow",
    "href": "renv.html#daily-workflow",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "# Activate environment (if not auto-activated)\nR -e \"renv::activate()\"\n\n# Install packages (in R console)\nR\n&gt; install.packages(\"package_name\")\n&gt; renv::snapshot()  # Save current state\n&gt; quit()\n\n# Restore environment from lockfile\nR -e \"renv::restore()\""
  },
  {
    "objectID": "renv.html#key-commands",
    "href": "renv.html#key-commands",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "Run these commands in the R console:\nrenv::status()      # Check project status\nrenv::snapshot()    # Update lockfile with current packages\nrenv::restore()     # Install packages from lockfile\nrenv::clean()       # Remove unused packages\nrenv::deactivate()  # Deactivate renv for this session\nrenv::activate()    # Reactivate renv"
  },
  {
    "objectID": "renv.html#file-structure-created",
    "href": "renv.html#file-structure-created",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "your_project/\n‚îú‚îÄ‚îÄ renv.lock        # Package versions (like requirements.txt)\n‚îú‚îÄ‚îÄ .Rprofile        # Auto-activates renv\n‚îî‚îÄ‚îÄ renv/            # Private library directory\n    ‚îú‚îÄ‚îÄ library/     # Project-specific packages\n    ‚îî‚îÄ‚îÄ settings.dcf # renv settings"
  },
  {
    "objectID": "renv.html#important-files",
    "href": "renv.html#important-files",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "renv.lock: Contains exact package versions and dependencies (commit to git)\n.Rprofile: Auto-activates renv when starting R in the project directory\nrenv/: Private library where packages are stored"
  },
  {
    "objectID": "renv.html#common-workflow-example",
    "href": "renv.html#common-workflow-example",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "# 1. Create new project\nmkdir my_r_project && cd my_r_project\n\n# 2. Initialize renv\nR -e \"renv::init()\"\n\n# 3. Install packages\nR -e \"install.packages(c('dplyr', 'ggplot2')); renv::snapshot()\"\n\n# 4. Work on your project with any editor\n# Your R scripts will use the renv environment automatically\n\n# 5. Share project - others run:\nR -e \"renv::restore()\""
  },
  {
    "objectID": "renv.html#tips-best-practices",
    "href": "renv.html#tips-best-practices",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "Always run renv::snapshot() after installing new packages\nThe .Rprofile file automatically activates renv when you start R\nUse renv::status() to check if packages need to be snapshotted\nShare renv.lock with collaborators (like requirements.txt in Python)\nKeep renv/ folder out of version control (add to .gitignore)\nUse renv::clean() periodically to remove unused packages"
  },
  {
    "objectID": "renv.html#troubleshooting",
    "href": "renv.html#troubleshooting",
    "title": "renv Terminal Cheat Sheet",
    "section": "",
    "text": "# If renv isn't working properly\nrenv::diagnostics()\n\n# Reset renv (use with caution)\nrenv::init(force = TRUE)\n\n# Check renv version\nrenv::version()"
  },
  {
    "objectID": "09_Time Series.html#exercises",
    "href": "09_Time Series.html#exercises",
    "title": "Time Series",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "09_Time Series.html#exercise-9.1",
    "href": "09_Time Series.html#exercise-9.1",
    "title": "Time Series",
    "section": "Exercise 9.1",
    "text": "Exercise 9.1\n\nLet \\(cGDP_t\\) denote the annual percentage change in gross domestic product and let \\(int_t\\) denote a short-term interest rate. Suppose that \\(cGDP_t\\) is related to interest rate by\n\\[cGDP_t=\\alpha_0+\\delta_0 int_t + \\delta_1 int_{t-1}+u_t\\]\nwhere \\(u_t\\) is uncorreltaed with \\(int_t\\), \\(int_{t-1}\\), and all other past values of interest rates. Suppose that the Federal Reserve follows the policy rule\n\\[int_t=\\gamma_0 + \\gamma_1 (cGDP_{t-1}-3)+v_t\\]\nwhere \\(\\gamma_1&gt;0\\). If \\(v_t\\) is uncorrelated with all past values of \\(int_t\\) and \\(u_t\\), argue that \\(int_t\\) must be correlated with \\(u_{t-1}\\). Which Gauss-Markov assumption does this violate?\n\n\n\\(cGDP_t=\\alpha_0+\\delta_0 int_t+\\delta_1 int_{t-1}+u_t\\) \\(cGDP_{t-1}=\\alpha_0+\\delta_0 int_{t-1}+\\delta_1 int_{t-2}+u_{t-1}\\)\n\\(int_t=\\gamma_0+\\gamma_1(cGDP_{t-1}-3)+v_t\\) \\(int_t = \\gamma_0 + \\gamma_1(\\alpha_0+\\delta_0 int_{t-1}+\\delta_1 int_{t-2}+u_{t-1}-3)+v_t\\)\n\\(Cov(int_t, u_{t-1})=E[int_t u_{t-1}]=\\) \\(E[[ \\gamma_0 + \\gamma_1(\\alpha_0+\\delta_0 int_{t-1}+\\delta_1 int_{t-2}+u_{t-1}-3)+v_t]u_{t-1}]\\) \\(E[\\gamma_1 u_{t-1}^2]=\\gamma_1 E[u_{t-1}^2]=\\gamma_1 \\sigma_u^2\\)\nThis means that \\(u_{t-1}\\) and \\(int_t\\) are correlated, violating extrict exogeneity, and zero conditional mean, TS3."
  },
  {
    "objectID": "09_Time Series.html#exercise-9.3",
    "href": "09_Time Series.html#exercise-9.3",
    "title": "Time Series",
    "section": "Exercise 9.3",
    "text": "Exercise 9.3\nUsing daily price and quantity observations on fish prices at Big Fish market in Lisbon we obtained the estimation results given below, where \\(LAVGPRC\\) is the logarithm of the average price of fish, \\(MON\\), \\(TUES\\), \\(WED\\), and \\(THURS\\) are daily dummy variables, and \\(T\\) is a linear time trend."
  },
  {
    "objectID": "09_Time Series.html#exercise-9.3-1",
    "href": "09_Time Series.html#exercise-9.3-1",
    "title": "Time Series",
    "section": "Exercise 9.3",
    "text": "Exercise 9.3"
  },
  {
    "objectID": "09_Time Series.html#exercise-9.3-2",
    "href": "09_Time Series.html#exercise-9.3-2",
    "title": "Time Series",
    "section": "Exercise 9.3",
    "text": "Exercise 9.3\n\nInterpret the coefficient estimate on the linear time trend.\nIs there evidence that prices vary systematically within a week? How would you formally test for this hypothesis?\n\n\na \\(\\hat{\\beta}_t=-0.00399\\): the average daily percentage change in the fish price after accounting for seasonal effects is equal to -0.3991%.\nb No, because the daily dummy variables are not statistically significant (the p-values associated with the four daily dummy variables are greater than 10%).\nTo formally test for the presence of seasonality,w e need to perform a jount significance test of the four daily dummy variables:\n\\(H_0: \\beta_{mon}=\\beta_{tue}=...=0\\), \\(H_a: Not\\) \\(F=\\frac{\\frac{R_u^2-R_R^2}{4}}{\\frac{1-R_u^2}{97-5-1}}\\sim F(4,91)\\)\nIf \\(F&gt;F^c\\) we reject the null hypothesis. The restricted model is \\(lavgprc=\\beta_0+\\beta_1 t + u_t\\)."
  },
  {
    "objectID": "09_Time Series.html#exercise-9.3-3",
    "href": "09_Time Series.html#exercise-9.3-3",
    "title": "Time Series",
    "section": "Exercise 9.3",
    "text": "Exercise 9.3\n\nNow consider the table below here we include two new variables \\(WAVE2\\) and \\(WAVE3\\), which are measures of wave heights over the past few days."
  },
  {
    "objectID": "09_Time Series.html#exercise-9.3-4",
    "href": "09_Time Series.html#exercise-9.3-4",
    "title": "Time Series",
    "section": "Exercise 9.3",
    "text": "Exercise 9.3\n\nAre these variables individually significant? Describe an economic mechanism by which story seas woudl increase the price of fish.\nWhat happened to the time trend when the two new variables were included in the regression? What must be going on?\nWhy do you expect all thej explanatory variables to be strictly exogenous? Explain briefly the difference between conteporaneus exogeneity and strict exogeneity.\n\n\nc \\(WAVE2\\) and \\(WAVE3\\) are individually statistically significant at 5% level, because their associated p-values are below 5%. Stormy seas lead to less supply of fish and tehrefore for the same demand the price of fish would increase.\nd In the extended model \\(\\hat{\\beta}_t=-0.0011575\\) and the corresponding p-value is greater than 5% and therefore the time trend is not statistically significant in this model. In the initial model \\(\\hat{\\beta}_t=-0.003991\\) and the associated p-value is lower than 5%, i.e., it is individuall statistically significant.\nFrom the omitted variable bias expression we know that \\(-0.003991 = -0.00011575+bias\\). Then, the bias is negative. This means that the height of the waves must be correlated with the time trend.\ne The time trend and the daily dummy variables are strictly exogenous because they are just functions of time and the calendar. Furthermore, the height of waves is not influenced by past, present or future unexpected changes in the (log) average price of fish.\nContemporaneous exogeneity \\(E[u_t|x_t]=0\\): the error term and the set of explanatory variables are contemporaneously uncorrelated. Strict exogeneity: \\(E[u_t|x_t]=0\\) the error term at time \\(t\\) is uncorrelated with each explanatory variable at every time period (past or future)."
  },
  {
    "objectID": "09_Time Series.html#exercise-10.2",
    "href": "09_Time Series.html#exercise-10.2",
    "title": "Time Series",
    "section": "Exercise 10.2",
    "text": "Exercise 10.2\nLet \\(e_t\\) be a sequence of independent identically distributed rvs with mean 0 and variance 1. Define a stochastic process by:\n\\[ x_t=e_T-0.5 e_{t-1}+0.5 e_{t-2}\\]\nfor \\(t=1,2,...\\)\n\nFind \\(E[x_t]\\) and \\(V[x_t]\\). Do either of these depend on \\(t\\)?\nShow that \\(Corr(x_t,x_{t+1})=-0.5\\) and \\(Corr(x_t,x_{t+2})=1/3\\)\n\n\na \\(E[x_t]=E[e_t-0.5 e_{t-1}+0.5 e_{t-2}]=E[e_t]-0.5 E[e_{t-1}]+0.5 E[e_{t-2}]=0\\) \\(V[x_t]=V[e_t-0.5 e_{t-1}+0.5 e_{t-2}]=V[e_t]+0.25 V[e_{t-1}]+0.25 V[e_{t-2}]=3/2\\)\nb \\(Cov(x_t,x_{t+1})=E[x_t x_{t+1}]=E\\left[(e_t+0.5 e_{t-1}-0.5 e_{t-2})(e_{t+1}+0.5 e_{t}-0.5 e_{t-1})\\right]\\) \\(=-0.5 E[e_t^2]+0.5^2 E[e_{t-1}^2]=0.5 V[e_t]-0.5^2 V[e_{t-1}]=-3/4\\) \\(Corr(x_t,x_{t+1})=\\frac{Cov(x_t,x_{t+1})}{V[x_t]}=-\\frac{3/4}{3/2}=-\\frac{1}{2}\\) \\(Cov(x_t,x_{t+2})=E[x_t,x_{t+2}]=E[(e_t-0.5e_{t-1}+0.5e_{t-2})(e_{t+2}-0.5e_{t+1}+0.5 e_t)]\\) \\(=0.5 E[e_t^2]=\\frac{1}{2}\\)\n\\(Corr(x_t,x_{t+2})=\\frac{Cov(x_t,x_{t+2})}{V{x_t}}=\\frac{1/2}{3/2}=\\frac{1}{3}\\)"
  },
  {
    "objectID": "09_Time Series.html#exercise-10.2-1",
    "href": "09_Time Series.html#exercise-10.2-1",
    "title": "Time Series",
    "section": "Exercise 10.2",
    "text": "Exercise 10.2\n\nWhat is \\(Corr(x_t,x_{t+h})\\) for \\(h&gt;2\\)&gt;\nIs \\(x_t\\) an asymptotically uncorrelated process?\n\n\nc \\(Cov(x_t,x_{t+3})=E[x_t,x_{t+3}]=E[(\\dots)(\\dots)]=0\\) because \\(x_t\\) only depends on \\(e_{t+j}\\) \\(j\\leq 0\\), and \\(x_{t+j}\\) only depends on \\(e_{t+j}\\) \\(j&gt;0\\). Then \\(Corr(x_t,x_{t+h})=0\\) \\(\\forall h&gt;2\\), because \\(e_t\\) is iid with zero mean and \\(E[e_t,e_s]=0\\) \\(\\forall t\\neq s\\).\nd Asymptotically uncorrelated process: a covariance stationary process where \\(Cov(x_t,x_{t+h})\\rightarrow 0\\) as \\(h\\rightarrow\\infty\\) Yes, because terms more than two periods apart are actually uncorrelated, and so it is clear thant \\(Corr(x_t,x_{t+h})\\rightarrow 0\\) as \\(h\\rightarrow\\infty\\)."
  },
  {
    "objectID": "07_heteroskedasticity_review.html",
    "href": "07_heteroskedasticity_review.html",
    "title": "Review for lecture Heteroskedasticity",
    "section": "",
    "text": "Testing for Heteroskedasticity\nBreusch-Pagan test\nWhite test\nLazy White test\nWeighted Least Squares (WLS)\nRobust Standard Errors\nFeasible Generalized Least Squares\n\nConsider the classical model:\n\\[\ny = X\\beta + u\n\\]\nFrom where we estimate \\(\\hat{\\beta}\\) as\n\\[\\hat{\\beta}=(X'X)^{-1}X'y\\]\nNow let‚Äôs compute the variance of \\(\\hat{\\beta}\\)\n\\[\n\\begin{aligned}\nV[\\hat{\\beta}|X]&=V\\left[(X'X)^{-1}X'y|X\\right]\\\\\n&= V\\left[(X'X)^{-1}X'(X\\beta+u)|X\\right]\\\\\n&=V\\left[(X'X)^{-1}X'X\\beta+(X'X)^{-1}X'u|X\\right]\\\\\n&=V\\left[\\beta+(X'X)^{-1}X'u|X\\right]=V\\left[(X'X)^{-1}X'u|X\\right]\n\\end{aligned}\n\\]\nAnd then\n\\[\nV[\\hat{\\beta}|X]=E[(X'X)^{-1}X'uu'X(X'X)^{-1}|X]\n\\]\nNote that if we are in the presence of homoskedasticity, then \\(V[u_i|X]=\\sigma^2\\), in particular \\(E[uu'|X]=\\sigma^2 I_n\\). With homoskedasticity:\n\\[\nV[\\hat{\\beta}|X]=E[(X'X)^{-1}X'\\sigma^2 I_n'X(X'X)^{-1}|X]=\\sigma^2(X'X)^{-1}\n\\]\nHowever, with heteroskedasticity:\n\\[\nV[\\hat{\\beta}|X]=E[(X'X)^{-1}X'uu'X(X'X)^{-1}|X]\n\\]\nwhere \\(E[uu'|X]\\) is the variance-covariance matrix of the error, \\(\\Omega\\). If errors are uncorrelated, the covariance is zero, however the diagonal has different \\(\\sigma_i^2\\).\nNote that this does not create a bias, as still \\(E[u|X]=0\\), and we still have\n\\[\nE[\\hat{\\beta}]=E\\left[(X'X)^{-1}X'y|X\\right]=E\\left[\\beta+(X'X)^{-1}X'u|X\\right]=\\beta+E\\left[(X'X)^{-1}X'u|X\\right]\n\\]\nAnd the bias, \\((X'X)^{-1}X'E[u|X]=0\\).\nGoing back to \\(V[\\hat{\\beta}|X]\\)\n\\[\nV[\\hat{\\beta}|X]=(X'X)^{-1}X'\\Omega X(X'X)^{-1}\n\\]\nTry this estimator: \\(\\hat{\\beta}_{robust}=(X'\\Omega^{-1} X)^{-1}X'\\Omega^{-1} y\\). I will ignore the subscript \\(robust\\) from now on, but keep in mind that \\(\\hat{\\beta}=\\hat{\\beta}_{robust}\\):\n\\[\nV[\\hat{\\beta}|X]=(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}\\Omega\\Omega^{-1} X(X'\\Omega^{-1}X)^{-1}=(X'\\Omega^{-1} X)^{-1}\n\\]"
  },
  {
    "objectID": "06_IV_2SLS_review.html",
    "href": "06_IV_2SLS_review.html",
    "title": "Review for lecture 2SLS",
    "section": "",
    "text": "Suppose you have the following model\n\\[\n    y = X\\beta + u\n\\]\nThe problem is that we have endogeneity, i.e. \\(E[X'u]\\neq 0\\). This will give as a biased and inconsistent \\(\\beta_{OLS}\\) estimator.\n\\[\n\\hat{\\beta}=(X'X)^{-1}X'y=(X'X)^{-1}X'(X\\beta+u)=\\beta+\\underbrace{(X'X)^{-1}X'u}_{\\neq 0}\\neq \\beta\n\\]\nAssume there is a variable \\(Z\\) such that:\n\n\\(rank(E[Z'X])=k\\), this is, \\(Z\\) is relevant, it is correlated with \\(X\\)\n\\(E[Z'u]=0\\), this is, \\(Z\\) is exogeneous.\n\n\\(Z\\) is what we will call the instrument.\nLet‚Äôs predict \\(X\\) using this instrument. If we regress\n\\[X=Z\\gamma + \\epsilon\\]\nwe get\n\\[\n    \\hat{\\gamma}=(Z'Z)^{-1}Z'X\n\\]\nAnd because \\(\\hat{X}=Z\\hat{\\gamma}=Z(Z'Z)^{-1}Z'X\\). Note that \\(Z(Z'Z)^{-1}Z'\\) is the projection matrix \\(P_Z\\), so \\(P_Z X\\) is the part of \\(X\\) that can be explained with the variable \\(Z\\).\nNow let‚Äôs regress\n\\[\n    y = \\hat{X}\\beta_{2SLS} + u\n\\]\nFrom where we get:\n\\[\n    \\hat{\\beta}_{2SLS}=(\\hat{X}'\\hat{X})^{-1}\\hat{X}'y\n\\]\nReplace \\(\\hat{X}=P_ZX\\)\n\\[\n    \\hat{\\beta}_{2SLS}=\\left((P_ZX)'P_ZX\\right)^{-1}(P_ZX)'y = \\left(X'P_Z'P_ZX\\right)^{-1}X'P_Z'y\n\\]\nNote now that \\(P_Z'=P_Z\\) and further, that \\(P_Z^2=P_Z\\), we get:\n\\[\n    \\hat{\\beta}_{2SLS}=\\left(X'P_ZX\\right)^{-1}X'P_Z'y\n\\]\nWhy this helps?\n\\[\n\\begin{aligned}\n    E[\\hat{\\beta}_{2SLS}|Z]=&E\\left[\\left(X'P_ZX\\right)^{-1}X'P_Z'(X\\beta+u)|Z\\right]\\\\\n    =&E\\left[\\left(X'P_ZX\\right)^{-1}X'P_Z'X\\beta+\\left(X'P_ZX\\right)^{-1}X'P_Z'u|Z\\right]\\\\\n    =&E\\left[\\beta+\\left(X'P_ZX\\right)^{-1}X'Z(Z'Z)^{-1}\\underbrace{Z'u}_{=0}|Z\\right]=\\beta\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "06_IV_2SLS_review.html#sls",
    "href": "06_IV_2SLS_review.html#sls",
    "title": "Review for lecture 2SLS",
    "section": "",
    "text": "Suppose you have the following model\n\\[\n    y = X\\beta + u\n\\]\nThe problem is that we have endogeneity, i.e. \\(E[X'u]\\neq 0\\). This will give as a biased and inconsistent \\(\\beta_{OLS}\\) estimator.\n\\[\n\\hat{\\beta}=(X'X)^{-1}X'y=(X'X)^{-1}X'(X\\beta+u)=\\beta+\\underbrace{(X'X)^{-1}X'u}_{\\neq 0}\\neq \\beta\n\\]\nAssume there is a variable \\(Z\\) such that:\n\n\\(rank(E[Z'X])=k\\), this is, \\(Z\\) is relevant, it is correlated with \\(X\\)\n\\(E[Z'u]=0\\), this is, \\(Z\\) is exogeneous.\n\n\\(Z\\) is what we will call the instrument.\nLet‚Äôs predict \\(X\\) using this instrument. If we regress\n\\[X=Z\\gamma + \\epsilon\\]\nwe get\n\\[\n    \\hat{\\gamma}=(Z'Z)^{-1}Z'X\n\\]\nAnd because \\(\\hat{X}=Z\\hat{\\gamma}=Z(Z'Z)^{-1}Z'X\\). Note that \\(Z(Z'Z)^{-1}Z'\\) is the projection matrix \\(P_Z\\), so \\(P_Z X\\) is the part of \\(X\\) that can be explained with the variable \\(Z\\).\nNow let‚Äôs regress\n\\[\n    y = \\hat{X}\\beta_{2SLS} + u\n\\]\nFrom where we get:\n\\[\n    \\hat{\\beta}_{2SLS}=(\\hat{X}'\\hat{X})^{-1}\\hat{X}'y\n\\]\nReplace \\(\\hat{X}=P_ZX\\)\n\\[\n    \\hat{\\beta}_{2SLS}=\\left((P_ZX)'P_ZX\\right)^{-1}(P_ZX)'y = \\left(X'P_Z'P_ZX\\right)^{-1}X'P_Z'y\n\\]\nNote now that \\(P_Z'=P_Z\\) and further, that \\(P_Z^2=P_Z\\), we get:\n\\[\n    \\hat{\\beta}_{2SLS}=\\left(X'P_ZX\\right)^{-1}X'P_Z'y\n\\]\nWhy this helps?\n\\[\n\\begin{aligned}\n    E[\\hat{\\beta}_{2SLS}|Z]=&E\\left[\\left(X'P_ZX\\right)^{-1}X'P_Z'(X\\beta+u)|Z\\right]\\\\\n    =&E\\left[\\left(X'P_ZX\\right)^{-1}X'P_Z'X\\beta+\\left(X'P_ZX\\right)^{-1}X'P_Z'u|Z\\right]\\\\\n    =&E\\left[\\beta+\\left(X'P_ZX\\right)^{-1}X'Z(Z'Z)^{-1}\\underbrace{Z'u}_{=0}|Z\\right]=\\beta\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "05_dummies.html#review-1",
    "href": "05_dummies.html#review-1",
    "title": "Binary Data",
    "section": "Review",
    "text": "Review\nWe will need this:\n\\[\nF = \\frac{(R^2_{ur} - R^2_{r})/q}{(1 - R^2_{ur})/(n-k-1)}\n\\]"
  },
  {
    "objectID": "05_dummies.html#exercises",
    "href": "05_dummies.html#exercises",
    "title": "Binary Data",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "05_dummies.html#exercise-5.5",
    "href": "05_dummies.html#exercise-5.5",
    "title": "Binary Data",
    "section": "Exercise 5.5",
    "text": "Exercise 5.5\n\nThe players of a basketball team can play in three positions: guard, forward or center. The variable exper is the number of years as a professional player, points is the number of points scored per game, guard is a binary variable equal to one if the player is a guard and 0 otherwise, and forward is a binary variable equal to one if the player is a forward and 0 otherwise. Standard errors are reported in parentheses. Consider the following estimated equation:\n\\[\n\\widehat{points}=\n  \\underset{(1.18)}{4.76} +\n  \\underset{(0.33)}{1.28} exper -\n  \\underset{(0.024)}{0.072} exper^2 +\n  \\underset{(1.00)}{2.31}guard +\n  \\underset{(1.00)}{1.54}forward\n\\] With \\(n=269\\), \\(R^2=0.091\\), \\(\\bar{R}^2=0.077\\)"
  },
  {
    "objectID": "05_dummies.html#question-5.5-a",
    "href": "05_dummies.html#question-5.5-a",
    "title": "Binary Data",
    "section": "Question 5.5 a",
    "text": "Question 5.5 a\nInterpret the regression coefficients:\n\n\\[\n\\widehat{points}=\n  \\underset{(1.18)}{4.76} +\n  \\underset{(0.33)}{1.28} exper -\n  \\underset{(0.024)}{0.072} exper^2 +\n  \\underset{(1.00)}{2.31}guard +\n  \\underset{(1.00)}{1.54}forward\n\\]\n\n\\(exper\\)\n\\(guard\\), \\(forward\\)\n\n\n\nNote: It is not possible to interpret \\(\\hat{\\beta}_1\\) individually because it has no ceteris paribus interpretation: it is not possible to change experience holding squared experience fixed. Take the partial derivative of the dependent variable with respect to the independent variables.\nThe marginal impact of experience is given by \\(1.28 ‚àí 2 √ó 0.071 exper\\). Therefore, each additional year of experience increases the number of points scored per game at a decreasing rate (up until approximately 9 years of experience). For example: the fifth year increases the predicted number of points by \\(1.28 - 2\\times0.072\\times4=0.704\\), on average, ceteris paribus.\nThe players of a basketball team can play in three positions: guard, forward or center. Because center is not included in the model as a variable, it is the reference or comparison group.\nGuard and forward are interpreted in comparison with the reference group, center. A guard player scores on average 2.31 more points per game relatively to a center player, ceteris paribus. A forward player scores on average 1.54 more points per game relatively to a center player, ceteris paribus."
  },
  {
    "objectID": "05_dummies.html#question-5.5-b",
    "href": "05_dummies.html#question-5.5-b",
    "title": "Binary Data",
    "section": "Question 5.5 b",
    "text": "Question 5.5 b\nWhy can‚Äôt we include in the model a binary variable center equal to one if the player is a center and zero otherwise?\n\n\\[\n\\widehat{points}=\n  \\underset{(1.18)}{4.76} +\n  \\underset{(0.33)}{1.28} exper -\n  \\underset{(0.024)}{0.072} exper^2 +\n  \\underset{(1.00)}{2.31}guard +\n  \\underset{(1.00)}{1.54}forward\n\\]\n\n\\(exper\\)\n\\(guard\\), \\(forward\\)\n\n\n\nBecause of the dummy variables trap. It would introduce perfect collinearity in the model with intercept (each player plays in one of the three categories and the overall intercept (\\(\\beta_0\\)) is for centers). \\(guard=1‚àíforward‚àícenter\\). An alternative would be to include the three dummies excluding the intercept but then it would not be as straightforward to test differences between groups."
  },
  {
    "objectID": "05_dummies.html#question-5.5-c",
    "href": "05_dummies.html#question-5.5-c",
    "title": "Binary Data",
    "section": "Question 5.5 c",
    "text": "Question 5.5 c\nHolding experience fixed, does a guard score more than a center? How much more? Is the difference statistically significant?\n\n\\[\n\\widehat{points}=\n  \\underset{(1.18)}{4.76} +\n  \\underset{(0.33)}{1.28} exper -\n  \\underset{(0.024)}{0.072} exper^2 +\n  \\underset{(1.00)}{2.31}guard +\n  \\underset{(1.00)}{1.54}forward\n\\] With \\(n=269\\), \\(R^2=0.091\\), \\(\\bar{R}^2=0.077\\)\n\n\nA guard player is estimated to score on average 2.31 more points per game relatively to a center player, ceteris paribus. Is the difference statistically significant?\n\\[\nH_0 : \\beta_3 = 0\\\\\nH_a : \\beta_3 &gt; 0\n\\]\nThe \\(t\\)-ratio equals \\(2.31/1=2.31\\) and the critical value for a one-tailed test with 5% significance level is \\(t \\sim t(269 ‚àí 4 ‚àí 1) = 1.645\\). Therefore, we reject the null hypothesis."
  },
  {
    "objectID": "05_dummies.html#question-5.5-d",
    "href": "05_dummies.html#question-5.5-d",
    "title": "Binary Data",
    "section": "Question 5.5 d",
    "text": "Question 5.5 d\nConsidering also the following estimated equation, test if experience influences points.\n\n\\[\n\\widehat{points}=\n  \\underset{(1.18)}{4.76} +\n  \\underset{(0.33)}{1.28} exper -\n  \\underset{(0.024)}{0.072} exper^2 +\n  \\underset{(1.00)}{2.31}guard +\n  \\underset{(1.00)}{1.54}forward\n\\]\nWith \\(n=269\\), \\(R^2=0.091\\), \\(\\bar{R}^2=0.077\\)\n\n\n\\[\n\\widehat{points}=\n  \\underset{(0.86)}{8.52} +\n  \\underset{(1.03)}{2.40}guard +\n  \\underset{(1.03)}{1.68}forward\n\\]\n\nwith \\(n=269\\) and \\(R^2=0.020\\), \\(\\bar{R}^2=0.012\\)\n\n\\(H_0: \\beta_1=0, \\beta_2=0\\), \\(H_a: Not\\ H_0\\)\nThen we can use an F-test:\n\\(F = [(0.091-0.020)/2]/[(1-0.091)/(269-4-1)]=10.31\\)\nThe critical value \\(F \\sim F (2, 264)\\) for a 5% significance level equals 3.00 and therefore we reject the null hypothesis."
  },
  {
    "objectID": "05_dummies.html#question-5.5-e",
    "href": "05_dummies.html#question-5.5-e",
    "title": "Binary Data",
    "section": "Question 5.5 e",
    "text": "Question 5.5 e\nConsider a new equation with marital status (marr) as an additional explanatory variable:\n\n\\[\n\\widehat{points} =\n  \\underset{(1.18)}{4.70} +\n  \\underset{(0.33)}{1.23} exper -\n  \\underset{(0.02)}{0.07} exper^2 +\n  \\underset{(1.00)}{2.29} guard +\n  \\underset{(1.00)}{1.54} forward +\n  \\underset{(0.74)}{0.58} marr\n\\]\n\nHolding position and experience fixed, do married plyaers score more points?\n\n\\(H_0:\\beta_5=0\\), \\(H_a: \\beta_5&gt;0\\)\nThe t-ratio equals \\(0.58/0.74=0.789\\) and therefore for such a small value we fail to reject the null hypothesis."
  },
  {
    "objectID": "05_dummies.html#question-5.5-f",
    "href": "05_dummies.html#question-5.5-f",
    "title": "Binary Data",
    "section": "Question 5.5 f",
    "text": "Question 5.5 f\nConsider the estimation output given in the table below for a new model that includes interactions of marital status with both experience variables. Is there strong evidence that marital status affects points per game?\n\n\nRemember that we need to consider every variable that depends on the marital status variable.\n\\(H_0 : \\beta_5 = \\beta_6 = \\beta_7 = 0\\), \\(H_a : Not\\ H_0\\)\nTo test this null hypothesis we compute an F-test:\n\\(F=[(0.1058‚àí0.091)/(3)]/[(1‚àí0.1058)/(269‚àí7‚àí1)]= 1.44\\)\nThe \\(F \\sim F (3, 261)\\) and therefore we fail to reject the null hypothesis in favor of the alternative hypothesis and there is no statistically significant evidence that marital status is important."
  },
  {
    "objectID": "05_dummies.html#question-5.5-g",
    "href": "05_dummies.html#question-5.5-g",
    "title": "Binary Data",
    "section": "Question 5.5 g",
    "text": "Question 5.5 g\nGiven this last estimated equation, what is the optimal number of years as a professional player if that player (for both married and not married players)?\n\nWe are looking for \\((\\partial \\widehat{points})/(\\partial exper)=0\\)\nIf \\(married=1\\), \\(0.70-2\\times 0.0295\\times exper + 1.28 - 2 \\times 0.09 \\times exper = 0 \\Leftrightarrow exper^*=9.25\\)\nIf \\(married=0\\), \\(0.70-2\\times 0.0295\\times exper = 0 \\Leftrightarrow exper^*=11.67\\)"
  },
  {
    "objectID": "05_dummies.html#question-5.9",
    "href": "05_dummies.html#question-5.9",
    "title": "Binary Data",
    "section": "Question 5.9",
    "text": "Question 5.9\nConsider the following estimated equation:\n\n\\[\n\\widehat{sat}=\n  \\underset{(6.29)}{1029.10} +\n  \\underset{(3.83)}{19.30} hsize -\n  \\underset{(0.53)}{2.19} hsize^2 -\n  \\underset{(4.29)}{45.09} female -\n  \\underset{(12.71)}{169.81} black +\n  \\underset{(18.15)}{62.31} female\\times black\n\\] With \\(n=4137\\) and \\(R^2=0.0858\\).\n\nThe variable sat is the combined school average test (SAT) score, hsize is size of the student‚Äôs high school graduating class, in hundreds, female is a gender dummy variable, and black is a race dummy variable equal to one for black students and zero otherwise."
  },
  {
    "objectID": "05_dummies.html#question-5.9-a",
    "href": "05_dummies.html#question-5.9-a",
    "title": "Binary Data",
    "section": "Question 5.9 a",
    "text": "Question 5.9 a\nIs there strong evidence that \\(hsize^2\\) should be included in the model? From this equation, what is the optimal high school size?\n\n\\[\n\\widehat{sat}=\n  \\underset{(6.29)}{1029.10} +\n  \\underset{(3.83)}{19.30} hsize -\n  \\underset{(0.53)}{2.19} hsize^2 -\n  \\underset{(4.29)}{45.09} female -\n  \\underset{(12.71)}{169.81} black +\n  \\underset{(18.15)}{62.31} female\\times black\n\\] With \\(n=4137\\) and \\(R^2=0.0858\\).\n\n\n\\(H_0:\\beta_2=0\\) and \\(H_a: \\beta\\neq 0\\).\nThe t-statistic equals \\(t = ‚àí 2.19/0.53=-4.13\\). For a 5% significance level and \\((4, 137 ‚àí 5 ‚àí 1)\\) degrees of freedom, the critical value is \\(t_c = 1.96\\). Because the \\(|t|&gt;|tc|\\) we reject the null hypothesis.\nWhat is the optimal high school size? This is a simple optimisation problem:\\(\\partial \\widehat{sat}/\\partial size = 0\\)\n\\(19.3-2\\times 2.19 \\times hsize = 0 \\Rightarrow hsize^*=4.41\\)\nImportantly, this means that the optimal school size is 4.41 hundreds of students, i.e., 441 students. [Remember that it is very important to look at the units of measurement of the variables]"
  },
  {
    "objectID": "05_dummies.html#question-5.9-b",
    "href": "05_dummies.html#question-5.9-b",
    "title": "Binary Data",
    "section": "Question 5.9 b",
    "text": "Question 5.9 b\nHolding hsize fixed what is the estimated difference in SAT score between nonblack females and nonblack males? How statistically significant is this estimated difference?\n\n\\[\n\\widehat{sat}=\n  \\underset{(6.29)}{1029.10} +\n  \\underset{(3.83)}{19.30} hsize -\n  \\underset{(0.53)}{2.19} hsize^2 -\n  \\underset{(4.29)}{45.09} female -\n  \\underset{(12.71)}{169.81} black +\n  \\underset{(18.15)}{62.31} female\\times black\n\\] With \\(n=4137\\) and \\(R^2=0.0858\\).\n\n\nLet‚Äôs write the expected SAT score for the two groups holding \\(hsize\\) (and therefore \\(hsize^2\\) ) fixed:\n\\(E(sat|female = 1, black = 0) = \\hat{\\beta}_0 + \\hat{\\beta}_3\\) (nonblack females)\n\\(E(sat|female = 0, black = 0) = \\hat{\\beta}_0\\) (nonblack males)\nTherefore, the estimated difference is \\(\\hat{beta}_3= ‚àí45.09\\). Coefficient interpretation: The predicted SAT scores of nonblack females are on average 45.09 points lower than the SAT scores of nonblack males, on average, ceteris paribus. To test whether the difference between these two groups is statistically significant we consider the following null and alternative hypotheses:\n\\(H_0=\\beta_3=0\\) and \\(H_a: \\beta_3\\neq 0\\)\nThe t-statistic equals \\(t=-45.09/4.29=-10.51\\). For a 5% significance level (\\(\\alpha=5%\\)) and (4,137-5-1) dof, the critical value is \\(t_c=1.96\\). Because \\(|t|&gt;|t_c|\\) we reject the null, and \\(\\hat{\\beta}_3\\) is statistically significant."
  },
  {
    "objectID": "05_dummies.html#question-5.9-c",
    "href": "05_dummies.html#question-5.9-c",
    "title": "Binary Data",
    "section": "Question 5.9 c",
    "text": "Question 5.9 c\nWhat is the estimated difference in SAT score between nonblack males and black males? Test the null hypothesis that there is no difference between their scores, against the alternative that there is a difference.\n\n\\[\n\\widehat{sat}=\n  \\underset{(6.29)}{1029.10} +\n  \\underset{(3.83)}{19.30} hsize -\n  \\underset{(0.53)}{2.19} hsize^2 -\n  \\underset{(4.29)}{45.09} female -\n  \\underset{(12.71)}{169.81} black +\n  \\underset{(18.15)}{62.31} female\\times black\n\\] With \\(n=4137\\) and \\(R^2=0.0858\\).\n\n\nLet‚Äôs write the expected SAT score for the two groups holding \\(hsize\\) (and therefore \\(hsize^2\\) ) fixed: \\(E(sat|female = 0, black = 1) = \\hat{\\beta}_0 + \\hat{\\beta}_4 (black males)\\) \\(E(sat|female = 0, black = 0) = \\hat{\\beta}_0 (nonblack males)\\) Coefficient interpretation: on average, the predicted SAT scores of black males males are 169.81 points lower than the SAT scores of nonblack males, ceteris paribus. The null and alternative hypotheses are the following: \\(H_0 : \\beta_4 = 0\\) and \\(H_a : \\beta_4 \\neq 0\\) respetively The t-statistic equals \\(t = ‚àí 169.81/12.71=-13.36\\). For a 5% significance level and (4, 137 ‚àí 12.715 ‚àí 1) degrees of freedom, the critical value is \\(t_c = 1.96\\). Because the \\(|t| &gt; |t_c|\\) we reject the null hypothesis in favor of the alternative hypothesis and \\(\\hat{\\beta}_4\\) is individually statistically significant."
  },
  {
    "objectID": "05_dummies.html#question-5.9-d",
    "href": "05_dummies.html#question-5.9-d",
    "title": "Binary Data",
    "section": "Question 5.9 d",
    "text": "Question 5.9 d\nWhat is the estimated difference in SAT score between black females and nonblack females? What would you need to do to test whether the difference is statistically significant?\n\n\\[\n\\widehat{sat}=\n  \\underset{(6.29)}{1029.10} +\n  \\underset{(3.83)}{19.30} hsize -\n  \\underset{(0.53)}{2.19} hsize^2 -\n  \\underset{(4.29)}{45.09} female -\n  \\underset{(12.71)}{169.81} black +\n  \\underset{(18.15)}{62.31} female\\times black\n\\] With \\(n=4137\\) and \\(R^2=0.0858\\).\n\n\n\\(E(sat|female = 1, black = 1) = \\hat{\\beta}_0 + \\hat{\\beta}_3 + \\hat{\\beta}_4 + \\hat{\\beta}_5\\) (black females) \\(E(sat|female = 1, black = 0) = \\hat{\\beta}_0 + \\hat{\\beta}_3\\) (nonblack females)\nThe difference between the groups is \\(\\hat{\\beta}_3+\\hat{\\beta}_4+\\hat{\\beta}_5‚àí\\hat{\\beta}_3=\\hat{\\beta}_4+\\hat{\\beta}_5\\). Then the null hypothesis can be written as \\(H_0 : \\beta_4 + \\beta_5 = 0\\)\nCoefficient interpretation: According to the estimates, on average, the predicted SAT scores of black females are 107.5 points lower than the SAT scores of nonblack females, ceteris paribus.\nThe null and alternative hypotheses are the following: \\(H_0:\\beta_4+\\beta_5=0\\) and \\(H_a:Not\\ H_0\\)\nTo test this null hypothesis using a t-statistic we would need the covariance between \\(\\hat{\\beta}_4\\) and \\(\\hat{\\beta}_5\\) : \\(t=(\\hat{\\beta}_4+\\hat{\\beta}_5)/\\sqrt{V[\\hat{\\beta}_4]+V[\\hat{\\beta}_5]+2Cov(\\hat{\\beta}_4,\\hat{\\beta}_5)}\\)"
  },
  {
    "objectID": "05_dummies.html#app-t-table",
    "href": "05_dummies.html#app-t-table",
    "title": "Binary Data",
    "section": "t table",
    "text": "t table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf-\\(\\alpha/2\\)\n0.50\n0.25\n0.20\n0.15\n0.10\n0.05\n0.025\n0.01\n0.005\n0.001\n0.0005\n\n\n\n\n1\n0.000\n1.000\n1.376\n1.963\n3.078\n6.314\n12.71\n31.82\n63.66\n318.31\n636.62\n\n\n2\n0.000\n0.816\n1.061\n1.386\n1.886\n2.920\n4.303\n6.965\n9.925\n22.327\n31.599\n\n\n3\n0.000\n0.765\n0.978\n1.250\n1.638\n2.353\n3.182\n4.541\n5.841\n10.215\n12.924\n\n\n4\n0.000\n0.741\n0.941\n1.190\n1.533\n2.132\n2.776\n3.747\n4.604\n7.173\n8.610\n\n\n5\n0.000\n0.727\n0.920\n1.156\n1.476\n2.015\n2.571\n3.365\n4.032\n5.893\n6.869\n\n\n6\n0.000\n0.718\n0.906\n1.134\n1.440\n1.943\n2.447\n3.143\n3.707\n5.208\n5.959\n\n\n7\n0.000\n0.711\n0.896\n1.119\n1.415\n1.895\n2.365\n2.998\n3.499\n4.785\n5.408\n\n\n8\n0.000\n0.706\n0.889\n1.108\n1.397\n1.860\n2.306\n2.896\n3.355\n4.501\n5.041\n\n\n9\n0.000\n0.703\n0.883\n1.100\n1.383\n1.833\n2.262\n2.821\n3.250\n4.297\n4.781\n\n\n10\n0.000\n0.700\n0.879\n1.093\n1.372\n1.812\n2.228\n2.764\n3.169\n4.144\n4.587\n\n\n11\n0.000\n0.697\n0.876\n1.088\n1.363\n1.796\n2.201\n2.718\n3.106\n4.025\n4.437\n\n\n12\n0.000\n0.695\n0.873\n1.083\n1.356\n1.782\n2.179\n2.681\n3.055\n3.930\n4.318\n\n\n13\n0.000\n0.694\n0.870\n1.079\n1.350\n1.771\n2.160\n2.650\n3.012\n3.852\n4.221\n\n\n14\n0.000\n0.692\n0.868\n1.076\n1.345\n1.761\n2.145\n2.624\n2.977\n3.787\n4.140\n\n\n15\n0.000\n0.691\n0.866\n1.074\n1.341\n1.753\n2.131\n2.602\n2.947\n3.733\n4.073\n\n\n16\n0.000\n0.690\n0.865\n1.071\n1.337\n1.746\n2.120\n2.583\n2.921\n3.686\n4.015\n\n\n17\n0.000\n0.689\n0.863\n1.069\n1.333\n1.740\n2.110\n2.567\n2.898\n3.646\n3.965\n\n\n18\n0.000\n0.688\n0.862\n1.067\n1.330\n1.734\n2.101\n2.552\n2.878\n3.610\n3.922\n\n\n19\n0.000\n0.688\n0.861\n1.066\n1.328\n1.729\n2.093\n2.539\n2.861\n3.579\n3.883\n\n\n20\n0.000\n0.687\n0.860\n1.064\n1.325\n1.725\n2.086\n2.528\n2.845\n3.552\n3.850\n\n\n21\n0.000\n0.686\n0.859\n1.063\n1.323\n1.721\n2.080\n2.518\n2.831\n3.527\n3.819\n\n\n22\n0.000\n0.686\n0.858\n1.061\n1.321\n1.717\n2.074\n2.508\n2.819\n3.505\n3.792\n\n\n23\n0.000\n0.685\n0.858\n1.060\n1.319\n1.714\n2.069\n2.500\n2.807\n3.485\n3.768\n\n\n24\n0.000\n0.685\n0.857\n1.059\n1.318\n1.711\n2.064\n2.492\n2.797\n3.467\n3.745\n\n\n25\n0.000\n0.684\n0.856\n1.058\n1.316\n1.708\n2.060\n2.485\n2.787\n3.450\n3.725\n\n\n26\n0.000\n0.684\n0.856\n1.058\n1.315\n1.706\n2.056\n2.479\n2.779\n3.435\n3.707\n\n\n27\n0.000\n0.684\n0.855\n1.057\n1.314\n1.703\n2.052\n2.473\n2.771\n3.421\n3.690\n\n\n28\n0.000\n0.683\n0.855\n1.056\n1.313\n1.701\n2.048\n2.467\n2.763\n3.408\n3.674\n\n\n29\n0.000\n0.683\n0.854\n1.055\n1.311\n1.699\n2.045\n2.462\n2.756\n3.396\n3.659\n\n\n30\n0.000\n0.683\n0.854\n1.055\n1.310\n1.697\n2.042\n2.457\n2.750\n3.385\n3.646\n\n\n40\n0.000\n0.681\n0.851\n1.050\n1.303\n1.684\n2.021\n2.423\n2.704\n3.307\n3.551\n\n\n60\n0.000\n0.679\n0.848\n1.045\n1.296\n1.671\n2.000\n2.390\n2.660\n3.232\n3.460\n\n\n80\n0.000\n0.678\n0.846\n1.043\n1.292\n1.664\n1.990\n2.374\n2.639\n3.195\n3.416\n\n\n100\n0.000\n0.677\n0.845\n1.042\n1.290\n1.660\n1.984\n2.364\n2.626\n3.174\n3.390\n\n\n1000\n0.000\n0.675\n0.842\n1.037\n1.282\n1.646\n1.962\n2.330\n2.581\n3.098\n3.300\n\n\nZ\n0.000\n0.674\n0.842\n1.036\n1.282\n1.645\n1.960\n2.326\n2.576\n3.090\n3.291"
  },
  {
    "objectID": "05_dummies.html#app-f-table",
    "href": "05_dummies.html#app-f-table",
    "title": "Binary Data",
    "section": "F table",
    "text": "F table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf2-Œ±=0.05\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1.000\n161.448\n199.500\n215.707\n224.583\n230.162\n233.986\n236.768\n238.883\n240.543\n241.882\n\n\n2.000\n18.513\n19.000\n19.164\n19.247\n19.296\n19.330\n19.353\n19.371\n19.385\n19.396\n\n\n3.000\n10.128\n9.552\n9.277\n9.117\n9.013\n8.941\n8.887\n8.845\n8.812\n8.786\n\n\n4.000\n7.709\n6.944\n6.591\n6.388\n6.256\n6.163\n6.094\n6.041\n5.999\n5.964\n\n\n5.000\n6.608\n5.786\n5.409\n5.192\n5.050\n4.950\n4.876\n4.818\n4.772\n4.735\n\n\n6.000\n5.987\n5.143\n4.757\n4.534\n4.387\n4.284\n4.207\n4.147\n4.099\n4.060\n\n\n7.000\n5.591\n4.737\n4.347\n4.120\n3.972\n3.866\n3.787\n3.726\n3.677\n3.637\n\n\n8.000\n5.318\n4.459\n4.066\n3.838\n3.687\n3.581\n3.500\n3.438\n3.388\n3.347\n\n\n9.000\n5.117\n4.256\n3.863\n3.633\n3.482\n3.374\n3.293\n3.230\n3.179\n3.137\n\n\n10.000\n4.965\n4.103\n3.708\n3.478\n3.326\n3.217\n3.135\n3.072\n3.020\n2.978\n\n\n11.000\n4.844\n3.982\n3.587\n3.357\n3.204\n3.095\n3.012\n2.948\n2.896\n2.854\n\n\n12.000\n4.747\n3.885\n3.490\n3.259\n3.106\n2.996\n2.913\n2.849\n2.796\n2.753\n\n\n13.000\n4.667\n3.806\n3.411\n3.179\n3.025\n2.915\n2.832\n2.767\n2.714\n2.671\n\n\n14.000\n4.600\n3.739\n3.344\n3.112\n2.958\n2.848\n2.764\n2.699\n2.646\n2.602\n\n\n15.000\n4.543\n3.682\n3.287\n3.056\n2.901\n2.790\n2.707\n2.641\n2.588\n2.544\n\n\n16.000\n4.494\n3.634\n3.239\n3.007\n2.852\n2.741\n2.657\n2.591\n2.538\n2.494\n\n\n17.000\n4.451\n3.592\n3.197\n2.965\n2.810\n2.699\n2.614\n2.548\n2.494\n2.450\n\n\n18.000\n4.414\n3.555\n3.160\n2.928\n2.773\n2.661\n2.577\n2.510\n2.456\n2.412\n\n\n19.000\n4.381\n3.522\n3.127\n2.895\n2.740\n2.628\n2.544\n2.477\n2.423\n2.378\n\n\n20.000\n4.351\n3.493\n3.098\n2.866\n2.711\n2.599\n2.514\n2.447\n2.393\n2.348\n\n\n21.000\n4.325\n3.467\n3.072\n2.840\n2.685\n2.573\n2.488\n2.420\n2.366\n2.321\n\n\n22.000\n4.301\n3.443\n3.049\n2.817\n2.661\n2.549\n2.464\n2.397\n2.342\n2.297\n\n\n23.000\n4.279\n3.422\n3.028\n2.796\n2.640\n2.528\n2.442\n2.375\n2.320\n2.275\n\n\n24.000\n4.260\n3.403\n3.009\n2.776\n2.621\n2.508\n2.423\n2.355\n2.300\n2.255\n\n\n25.000\n4.242\n3.385\n2.991\n2.759\n2.603\n2.490\n2.405\n2.337\n2.282\n2.236\n\n\n26.000\n4.225\n3.369\n2.975\n2.743\n2.587\n2.474\n2.388\n2.321\n2.265\n2.220\n\n\n27.000\n4.210\n3.354\n2.960\n2.728\n2.572\n2.459\n2.373\n2.305\n2.250\n2.204\n\n\n28.000\n4.196\n3.340\n2.947\n2.714\n2.558\n2.445\n2.359\n2.291\n2.236\n2.190\n\n\n29.000\n4.183\n3.328\n2.934\n2.701\n2.545\n2.432\n2.346\n2.278\n2.223\n2.177\n\n\n30.000\n4.171\n3.316\n2.922\n2.690\n2.534\n2.421\n2.334\n2.266\n2.211\n2.165\n\n\n40.000\n4.085\n3.232\n2.839\n2.606\n2.449\n2.336\n2.249\n2.180\n2.124\n2.077\n\n\n60.000\n4.001\n3.150\n2.758\n2.525\n2.368\n2.254\n2.167\n2.097\n2.040\n1.993\n\n\n120.000\n3.920\n3.072\n2.680\n2.447\n2.290\n2.175\n2.087\n2.016\n1.959\n1.910\n\n\n1000000000.000\n3.841\n2.996\n2.605\n2.372\n2.214\n2.099\n2.010\n1.938\n1.880\n1.831"
  },
  {
    "objectID": "04_OVB_IV.html#review-1",
    "href": "04_OVB_IV.html#review-1",
    "title": "OVB and IV",
    "section": "Review",
    "text": "Review\n\nOVB\nIV Estimator"
  },
  {
    "objectID": "04_OVB_IV.html#exercises",
    "href": "04_OVB_IV.html#exercises",
    "title": "OVB and IV",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "04_OVB_IV.html#exercise-2.6",
    "href": "04_OVB_IV.html#exercise-2.6",
    "title": "OVB and IV",
    "section": "Exercise 2.6",
    "text": "Exercise 2.6\n\nFor each of the following situations, determine the sign of the expected bias due to variable omission:\n\nIn an equation for the demand for peanut butter, the impact on the coefficient of disposable income of omitting the price of peanut butter variable.\nIn an earnings equation, the impact on the coefficient of experience of omitting the variable age.\nIn a production function, the impact on the coefficient of labour of omitting the capital variable.\nIn an annual equation for corn yields per acre (in year \\(t\\)), the impact on the coefficient of rainfall in year \\(t\\) of omitting average temperature that year.\nIn an equation for annual consumption of apples in the US, the impact on the coefficient of the price of bananas of omitting the price of oranges.\nIn an equation for student grades ont he first midterm in this class, the impact on the coefficient of total hours studied (for the test) of omitting hours slept the night before the test.\n\n\n\n\n\\(Corr(Z,Y)&lt;0\\) \\(Corr(X,Z)&gt;0\\), bias \\(-\\)\n\\(Corr(Z,Y)&gt;0\\) \\(Corr(X,Z)&gt;0\\), bias \\(+\\)\n\\(Corr(Z,Y)&gt;0\\) \\(Corr(X,Z)&gt;0\\), bias \\(+\\)\n? ? ?\n\\(Corr(Z,Y)&gt;0\\) \\(Corr(X,Z)&gt;0\\), bias \\(+\\)\n\\(Corr(Z,Y)&gt;0\\) \\(Corr(X,Z)&lt;0\\), bias \\(-\\)"
  },
  {
    "objectID": "04_OVB_IV.html#exercise-99.9",
    "href": "04_OVB_IV.html#exercise-99.9",
    "title": "OVB and IV",
    "section": "Exercise 99.9",
    "text": "Exercise 99.9\nSuppose you want to estimate the effect of Price on Ice Cream sales. You collect 200 datapoints.\n\\[\nsales = \\beta_0 + \\beta_{price} price + u\n\\]"
  },
  {
    "objectID": "04_OVB_IV.html#exercise-99.9-1",
    "href": "04_OVB_IV.html#exercise-99.9-1",
    "title": "OVB and IV",
    "section": "Exercise 99.9",
    "text": "Exercise 99.9\n\nols &lt;- lm(sales ~ price, data = data)\nsummary(ols)\n\n\nCall:\nlm(formula = sales ~ price, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2422 -0.6196  0.0101  0.5171  2.9061 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 198.83877    0.35918   553.6   &lt;2e-16 ***\nprice        -7.90647    0.02968  -266.4   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9398 on 198 degrees of freedom\nMultiple R-squared:  0.9972,    Adjusted R-squared:  0.9972 \nF-statistic: 7.096e+04 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\n\nCan you trust this estimate \\(\\beta_1=-7.91\\)?"
  },
  {
    "objectID": "04_OVB_IV.html#exercise-99.9-2",
    "href": "04_OVB_IV.html#exercise-99.9-2",
    "title": "OVB and IV",
    "section": "Exercise 99.9",
    "text": "Exercise 99.9\nLet‚Äôs use \\(temp\\) as an instrument\n\nlibrary(AER)\niv &lt;- ivreg(sales ~ price | temp, data = data)\nsummary(iv)\n\n\nCall:\nivreg(formula = sales ~ price | temp, data = data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-2.642106 -0.622538  0.003097  0.609989  2.754196 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 199.92686    0.37612   531.6   &lt;2e-16 ***\nprice        -7.99796    0.03111  -257.1   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.962 on 198 degrees of freedom\nMultiple R-Squared: 0.9971, Adjusted R-squared: 0.9971 \nWald test: 6.611e+04 on 1 and 198 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "04_OVB_IV.html#exercise-99.9-3",
    "href": "04_OVB_IV.html#exercise-99.9-3",
    "title": "OVB and IV",
    "section": "Exercise 99.9",
    "text": "Exercise 99.9\n\n\nCov(temp, U): 0.033\n\n\nCov(price, temp): 16.006\n\n\nCov(sales, temp): -128.015\n\n\nSo the instrument is exogenous and relevant!\n\\[\\hat{\\beta}_{iv}=\\frac{-128.015}{16.006}=-7.998\\]"
  },
  {
    "objectID": "02_MLR.html#review-1",
    "href": "02_MLR.html#review-1",
    "title": "MLR",
    "section": "Review",
    "text": "Review\n\nMatrix/vector SLR\nMLR\n\\(\\beta_i=\\frac{\\sum \\hat{r}y}{\\sum \\hat{r}^2}\\) with \\(\\hat{r}\\) the residuals of \\(x_i=\\gamma_0+\\sum_{j\\neq i} \\gamma_j x_j+\\varepsilon\\)"
  },
  {
    "objectID": "02_MLR.html#exercises",
    "href": "02_MLR.html#exercises",
    "title": "MLR",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "02_MLR.html#exercise-2.1",
    "href": "02_MLR.html#exercise-2.1",
    "title": "MLR",
    "section": "Exercise 2.1",
    "text": "Exercise 2.1\nOn 18th September of 2015, during apractical class taking place at Nova SBE, a survey was delivered to 52 promising future leaders. This survey collected information on their GPA and on how many hours they usually spend per day in each of the five activities: study, sleep, work, sport, and leisure. Any activity was put into one of the five categories, so that ofr each student the sum of hours in the five activities was 24. Consider the following population regression model:\n\\[GPA=\\beta_0+\\beta_1 study+ \\beta_2 sleep + \\beta_3 work + \\beta_4 sport + \\beta_5 leisure + u\\]"
  },
  {
    "objectID": "02_MLR.html#exercise-2.1-1",
    "href": "02_MLR.html#exercise-2.1-1",
    "title": "MLR",
    "section": "Exercise 2.1",
    "text": "Exercise 2.1\n\nDoes it make sense to hold sleep, work, sport, and leisure fixed, while changing study?\nExplain why this model violates assumption MLR.3 (No perfect collinearity). What is the implication for the OLX estimators?\nHow could you reformulate the model so that its parameters have a useful interpretation and it satisfies Assumption MLR.3?\n\n\n\nBy definition \\(sleep+study+work+sport+leisure=24\\). Changing study requires something else to change.\nEach indep. var. is a l.c. of all others. It is not possible to obtain estimates.\nDrop any variable, for example sleep. Then the \\(\\beta_{study}\\) can be interpreted while holding work, sport, and leisure constant."
  },
  {
    "objectID": "02_MLR.html#exercise-2.1-2",
    "href": "02_MLR.html#exercise-2.1-2",
    "title": "MLR",
    "section": "Exercise 2.1",
    "text": "Exercise 2.1\n\nAfter omitting the variable sleep the model was estimated by OLS. What would your advice to these promising future leaders?\n\n\n\\[ \\widehat{GPA}=22.81-0.57 study - 0.20 work - 0.38 sport - 0.51 leisure\\]\n\n\nStudying one more hour a day, implies sleeping one hour less! Thus studying one more hour a day (while sleeping one hour less) reduces GPA by 0.57, on average, certeris paribus."
  },
  {
    "objectID": "02_MLR.html#exercise-2.3",
    "href": "02_MLR.html#exercise-2.3",
    "title": "MLR",
    "section": "Exercise 2.3",
    "text": "Exercise 2.3\nThe following model is a simplified version of the multiple regression model used by Biddle and Hamermesh (1990) to study how total hours work, years of education and age affect sleeping:\n\n\\[sleep=\\beta_0+\\beta_1 totwrk + \\beta_2 educ + \\beta_3 age + u\\]\n\nwhere sleep and totwrk are measured in minutes per week and educ and age are measured in years."
  },
  {
    "objectID": "02_MLR.html#exercise-2.3-1",
    "href": "02_MLR.html#exercise-2.3-1",
    "title": "MLR",
    "section": "Exercise 2.3",
    "text": "Exercise 2.3\n\nIf adults trade off sleep for work what is the sign of \\(\\beta_1\\)?\nWhat signs do you think \\(\\beta_2\\) and \\(\\beta_3\\) will have?\n\n\n\nIf there is a trade-off \\(\\beta_1&lt;0\\)\nIt is unclear!"
  },
  {
    "objectID": "02_MLR.html#exercise-2.3-2",
    "href": "02_MLR.html#exercise-2.3-2",
    "title": "MLR",
    "section": "Exercise 2.3",
    "text": "Exercise 2.3\n\nUsing real data, the authors have estimated the following relationship:\n\\[\\widehat{sleep}=3,638.25-0.148totwrk-11.13educ+2.20age\\]\nWith \\(n=706\\) and \\(R^2=0.113\\).\n\nIf someone works five more hours per week, by how many minutes is sleep predicted to fall? Is this a large trade-off?\nDiscuss the sign and magnitude of the estimated coefficient on educ.\nWould you say totwrk, educ, and age explain much of the variation in sleep? What other factors might affect the time spent sleeping? Are these likely to be correlated with totwrk?\n\n\n\n\n\\(\\Delta sleep=-0.148(5\\times 60)=-44.4\\) minutes. Not a very large trade-off\n1 more year of education reduces sleeping time by 11 minutes, on average, ceteris paribus.\nThe three variables only explain 11% of the variability/variation of sleep. Marital stautyes, number/age of children, type of job, health, ‚Ä¶, and are likely correlated with total work."
  },
  {
    "objectID": "01_SLR_review.html",
    "href": "01_SLR_review.html",
    "title": "Review for lecture SLR",
    "section": "",
    "text": "Assume the model:\n\\[ y = \\beta_0 + \\beta_1 x + u\\]\nWith the classical assumptions on \\(u_i\\).\nThe estimators come from minimizing \\(\\sum_i u_i^2\\), for that define isolate \\(u_i\\)\n\\[\n\\begin{aligned}\n    u_i &= y_i-\\beta_0-\\beta_1 x_i\\\\\n    u_i^2 &= (y_i-\\beta_0-\\beta_1 x_i)^2 \\\\\n    \\sum_{i} u_i^2 &= \\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2\n\\end{aligned}\n\\]\nThen, the optimization problem becomes:\n\\[\n\\min_{\\beta_0,\\beta_1} F(\\beta_0,\\beta_1)=\\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2\n\\]\nFrom where we can compute the first order conditions \\(\\partial F/\\partial \\beta_0=0\\) and \\(\\partial F/\\partial \\beta_1=0\\).\n\n\n\\(\\partial F/\\partial \\beta_0=0\\) implies:\n\\[\n\\begin{aligned}\n    \\frac{\\partial}{\\partial \\beta_0}\\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2 = 0 \\ &\\Rightarrow\\ 2(-1)\\sum_i (y_i-\\beta_0-\\beta_1 x_i)=0\\\\\n    \\sum_i (y_i-\\beta_0-\\beta_1 x_i)=0\\ &\\Rightarrow\\ \\sum_i y_i-\\beta_0\\sum_i 1 -\\beta_1 \\sum_i x_i=0 \\\\\n    n\\beta_0= \\sum_i y_i -\\beta_1 \\sum_i x_i\\ &\\Rightarrow\\ \\beta_0 = \\frac{\\sum_i y_i}{n} -\\beta_1 \\frac{\\sum_i x_i}{n}=\\overline{y}-\\beta_1 \\overline{x}\\\\\n    \\beta_0 &= \\overline{y}-\\beta_1 \\overline{x}\n\\end{aligned}\n\\]\nA consequence of this, is that the original regression would be the same (same residuals, same slope) than regressing the deviations of \\(y\\) from its mean on the deviation of \\(x\\) around its mean, without a constant, or in other words, if we center \\(y\\) and \\(x\\) around their means.\n\\[y=\\beta_0+\\beta_1 x + u_i\\ \\Leftrightarrow\\ (y-\\overline{y})=\\beta_1(x-\\overline{x})+u_i\\]\n\n\n\n\\(\\partial F/\\partial \\beta_1=0\\) implies:\n\\[\n\\begin{aligned}\n    \\frac{\\partial}{\\partial \\beta_1}\\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2 = 0 \\ &\\Rightarrow\\ 2(-1)\\sum_i x_i(y_i-\\beta_0-\\beta_1 x_i)=0\\\\\n    \\sum_i x_i(y_i-\\beta_0-\\beta_1 x_i)=0\\ &\\Rightarrow\\ \\sum_i x_iy_i-\\beta_0\\sum_i x_i-\\beta_1 \\sum_i x_i^2=0\n\\end{aligned}\n\\]\nReplacing \\(\\beta_0\\) and noting that \\(\\sum_i x_i=n\\overline{x}\\):\n\\[\n\\begin{aligned}\n    \\sum_i x_iy_i-\\beta_0\\sum_i x_i-\\beta_1 \\sum_i x_i^2=0\\ &\\Rightarrow\\ \\sum_i x_iy_i-(\\overline{y}-\\beta_1\\overline{x})n\\overline{x}-\\beta_1 \\sum_i x_i^2=0\\\\\n    \\sum_i x_iy_i-n\\overline{y}\\overline{x}+n \\beta_1\\overline{x}^2-\\beta_1 \\sum_i x_i^2=0\\ &\\Rightarrow\\ \\sum_i x_iy_i-n\\overline{y}\\overline{x}-\\beta_1\\left(\\sum_i x_i^2-n\\overline{x}^2\\right)=0 \\\\\n    \\sum_i x_iy_i-n\\overline{y}\\overline{x}=\\beta_1\\left(\\sum_i x_i^2-n\\overline{x}^2\\right)\\ &\\Rightarrow\\ \\beta_1 = \\frac{\\sum_i x_iy_i-n\\overline{y}\\overline{x}}{\\sum_i x_i^2-n\\overline{x}^2}\n\\end{aligned}\n\\]\nNow note that \\(n\\overline{y}\\overline{x}=\\sum_i y_i\\overline{x}=\\sum_i x_i\\overline{y}=\\sum_i \\overline{x}\\overline{y}\\):\n\\[\n\\begin{aligned}\n\\sum_i x_iy_i-n\\overline{y}\\overline{x}&=\\sum_i x_iy_i-n\\overline{y}\\overline{x}+n\\overline{y}\\overline{x}-n\\overline{y}\\overline{x}\\\\\n\\sum_i x_iy_i-\\sum_i x_i\\overline{y}+\\sum_i y_i\\overline{x}-\\sum_i\\overline{y}\\overline{x}&=\\sum_ix_i(y_i-\\overline{y})-\\sum_i\\overline{x}(y_i-\\overline{y})\\\\\n\\sum_i(x_i-\\overline{x})(y_i-\\overline{y})&=n Cov(X,Y)\n\\end{aligned}\n\\]\nNow,\n\\[\n\\begin{aligned}\n    \\sum_i x_i^2-n\\overline{x}^2 &= \\sum_i x_i^2-n\\overline{x}^2 + n\\overline{x}^2 - n\\overline{x}^2\\\\\n    \\sum_i x_i^2-2 n\\overline{x}^2 + n\\overline{x}^2 &= \\sum_i x_i^2-2 \\sum_i x_i\\overline{x} + \\sum_i\\overline{x}^2\\\\\n    \\sum_i \\left(x_i^2-2 x_i\\overline{x} + \\overline{x}^2\\right) &= \\sum_i \\left(x_i- \\overline{x}\\right)^2 = nV[X]\n\\end{aligned}\n\\]\nFinally \\[ \\beta_1 = \\frac{n Cov(X,Y)}{nV[X]}=\\frac{Cov(X,Y)}{V[X]}\\]"
  },
  {
    "objectID": "01_SLR_review.html#derivation-of-ols-estimators",
    "href": "01_SLR_review.html#derivation-of-ols-estimators",
    "title": "Review for lecture SLR",
    "section": "",
    "text": "Assume the model:\n\\[ y = \\beta_0 + \\beta_1 x + u\\]\nWith the classical assumptions on \\(u_i\\).\nThe estimators come from minimizing \\(\\sum_i u_i^2\\), for that define isolate \\(u_i\\)\n\\[\n\\begin{aligned}\n    u_i &= y_i-\\beta_0-\\beta_1 x_i\\\\\n    u_i^2 &= (y_i-\\beta_0-\\beta_1 x_i)^2 \\\\\n    \\sum_{i} u_i^2 &= \\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2\n\\end{aligned}\n\\]\nThen, the optimization problem becomes:\n\\[\n\\min_{\\beta_0,\\beta_1} F(\\beta_0,\\beta_1)=\\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2\n\\]\nFrom where we can compute the first order conditions \\(\\partial F/\\partial \\beta_0=0\\) and \\(\\partial F/\\partial \\beta_1=0\\).\n\n\n\\(\\partial F/\\partial \\beta_0=0\\) implies:\n\\[\n\\begin{aligned}\n    \\frac{\\partial}{\\partial \\beta_0}\\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2 = 0 \\ &\\Rightarrow\\ 2(-1)\\sum_i (y_i-\\beta_0-\\beta_1 x_i)=0\\\\\n    \\sum_i (y_i-\\beta_0-\\beta_1 x_i)=0\\ &\\Rightarrow\\ \\sum_i y_i-\\beta_0\\sum_i 1 -\\beta_1 \\sum_i x_i=0 \\\\\n    n\\beta_0= \\sum_i y_i -\\beta_1 \\sum_i x_i\\ &\\Rightarrow\\ \\beta_0 = \\frac{\\sum_i y_i}{n} -\\beta_1 \\frac{\\sum_i x_i}{n}=\\overline{y}-\\beta_1 \\overline{x}\\\\\n    \\beta_0 &= \\overline{y}-\\beta_1 \\overline{x}\n\\end{aligned}\n\\]\nA consequence of this, is that the original regression would be the same (same residuals, same slope) than regressing the deviations of \\(y\\) from its mean on the deviation of \\(x\\) around its mean, without a constant, or in other words, if we center \\(y\\) and \\(x\\) around their means.\n\\[y=\\beta_0+\\beta_1 x + u_i\\ \\Leftrightarrow\\ (y-\\overline{y})=\\beta_1(x-\\overline{x})+u_i\\]\n\n\n\n\\(\\partial F/\\partial \\beta_1=0\\) implies:\n\\[\n\\begin{aligned}\n    \\frac{\\partial}{\\partial \\beta_1}\\sum_{i} (y_i-\\beta_0-\\beta_1 x_i)^2 = 0 \\ &\\Rightarrow\\ 2(-1)\\sum_i x_i(y_i-\\beta_0-\\beta_1 x_i)=0\\\\\n    \\sum_i x_i(y_i-\\beta_0-\\beta_1 x_i)=0\\ &\\Rightarrow\\ \\sum_i x_iy_i-\\beta_0\\sum_i x_i-\\beta_1 \\sum_i x_i^2=0\n\\end{aligned}\n\\]\nReplacing \\(\\beta_0\\) and noting that \\(\\sum_i x_i=n\\overline{x}\\):\n\\[\n\\begin{aligned}\n    \\sum_i x_iy_i-\\beta_0\\sum_i x_i-\\beta_1 \\sum_i x_i^2=0\\ &\\Rightarrow\\ \\sum_i x_iy_i-(\\overline{y}-\\beta_1\\overline{x})n\\overline{x}-\\beta_1 \\sum_i x_i^2=0\\\\\n    \\sum_i x_iy_i-n\\overline{y}\\overline{x}+n \\beta_1\\overline{x}^2-\\beta_1 \\sum_i x_i^2=0\\ &\\Rightarrow\\ \\sum_i x_iy_i-n\\overline{y}\\overline{x}-\\beta_1\\left(\\sum_i x_i^2-n\\overline{x}^2\\right)=0 \\\\\n    \\sum_i x_iy_i-n\\overline{y}\\overline{x}=\\beta_1\\left(\\sum_i x_i^2-n\\overline{x}^2\\right)\\ &\\Rightarrow\\ \\beta_1 = \\frac{\\sum_i x_iy_i-n\\overline{y}\\overline{x}}{\\sum_i x_i^2-n\\overline{x}^2}\n\\end{aligned}\n\\]\nNow note that \\(n\\overline{y}\\overline{x}=\\sum_i y_i\\overline{x}=\\sum_i x_i\\overline{y}=\\sum_i \\overline{x}\\overline{y}\\):\n\\[\n\\begin{aligned}\n\\sum_i x_iy_i-n\\overline{y}\\overline{x}&=\\sum_i x_iy_i-n\\overline{y}\\overline{x}+n\\overline{y}\\overline{x}-n\\overline{y}\\overline{x}\\\\\n\\sum_i x_iy_i-\\sum_i x_i\\overline{y}+\\sum_i y_i\\overline{x}-\\sum_i\\overline{y}\\overline{x}&=\\sum_ix_i(y_i-\\overline{y})-\\sum_i\\overline{x}(y_i-\\overline{y})\\\\\n\\sum_i(x_i-\\overline{x})(y_i-\\overline{y})&=n Cov(X,Y)\n\\end{aligned}\n\\]\nNow,\n\\[\n\\begin{aligned}\n    \\sum_i x_i^2-n\\overline{x}^2 &= \\sum_i x_i^2-n\\overline{x}^2 + n\\overline{x}^2 - n\\overline{x}^2\\\\\n    \\sum_i x_i^2-2 n\\overline{x}^2 + n\\overline{x}^2 &= \\sum_i x_i^2-2 \\sum_i x_i\\overline{x} + \\sum_i\\overline{x}^2\\\\\n    \\sum_i \\left(x_i^2-2 x_i\\overline{x} + \\overline{x}^2\\right) &= \\sum_i \\left(x_i- \\overline{x}\\right)^2 = nV[X]\n\\end{aligned}\n\\]\nFinally \\[ \\beta_1 = \\frac{n Cov(X,Y)}{nV[X]}=\\frac{Cov(X,Y)}{V[X]}\\]"
  },
  {
    "objectID": "01_SLR_review.html#logs-in-the-regression",
    "href": "01_SLR_review.html#logs-in-the-regression",
    "title": "Review for lecture SLR",
    "section": "Logs in the regression",
    "text": "Logs in the regression"
  },
  {
    "objectID": "01_SLR_review.html#sstssrsse",
    "href": "01_SLR_review.html#sstssrsse",
    "title": "Review for lecture SLR",
    "section": "\\(SST=SSR+SSE\\)",
    "text": "\\(SST=SSR+SSE\\)\nStart with the regression:\n\\[y_i=\\beta_0+\\beta_1 x_i+u_i\\]\nNote that \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1 x_i\\) and rewrite:\n\\[\n\\begin{aligned}\n    y_i&=\\hat{y}_i+\\hat{u}_i\\\\\n    y_i-\\overline{y}&=\\hat{y}_i-\\overline{y}+\\hat{u}_i\\\\\n    \\left(y_i-\\overline{y}\\right)^2&=\\left((\\hat{y}_i-\\overline{y})+\\hat{u}_i\\right)^2\\\\\n    %\\left(y_i-\\overline{y}\\right)^2&=\\left(\\hat{y}_i-\\overline{y}\\right)^2+2(\\hat{y}_i-\\overline{y})u_i+\\hat{u}_i^2\\\\\n    \\sum_i \\left(y_i-\\overline{y}\\right)^2&=\\sum_i\\left(\\hat{y}_i-\\overline{y}\\right)^2+2\\sum_i\\left(\\hat{y}_i-\\overline{y}\\right)\\hat{u}_i+\\sum_i\\hat{u}_i^2\\\\\n    SST &= SSR + 2\\sum_i\\left(\\hat{y}_i-\\overline{y}\\right)\\hat{u}_i + SSE\n\\end{aligned}\n\\]\nWhere \\(SST\\) is the sum of squares of the total, \\(SSR\\) is the sum of squares of the regression, and \\(SSE\\) is the sum of squares of the error.\nLet‚Äôs check that \\(2\\sum_i\\left(\\hat{y}_i-\\overline{y}\\right)\\hat{u}_i=0\\). First, the \\(2\\) can be cancelled out, and then the term becomes\n\\[\n\\begin{aligned}\n    \\sum_i\\left(\\hat{y}_i-\\overline{y}\\right)\\hat{u}_i &= \\sum_i\\hat{y}_i\\hat{u}_i-\\overline{y}\\sum_i\\hat{u}_i\n\\end{aligned}\n\\]\nBy assumption \\(E[u|X]=0\\), that means that \\(\\frac{1}{n}\\sum \\hat{u}_i=0\\) and therefore \\(\\sum \\hat{u}_i=0\\), so the second term must be zero. Now consider that \\(\\hat{y}_i=\\beta_0+\\beta_1x_i\\), and therefore \\(\\sum_i \\hat{y}_i \\hat{u}_i=\\beta_0 \\sum_i \\hat{u}_i + \\beta_1\\sum_i x_i \\hat{u}_i\\), but by assumption \\(x_i\\) and \\(\\hat{u}_i\\) are uncorrelated, and therefore \\(\\sum_i x_i \\hat{u}_i=0\\). The first term is zero also because \\(E[u]=0\\). Then, the whole term is zero and we have our result:\n\\[SST=SSR+SSE\\]"
  },
  {
    "objectID": "02_MLR_review.html",
    "href": "02_MLR_review.html",
    "title": "Review for lecture SLR",
    "section": "",
    "text": "The original equation:\n\\(y_i=\\beta_0+\\beta_1 x_i + u_i\\)\nThis holds for \\(i=1,2,...,n\\) therefore you can write\n\\[\n\\begin{pmatrix}y_1\\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\beta_0\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} +\n\\beta_1\\begin{pmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{pmatrix} +\n\\begin{pmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_n\\end{pmatrix}\n\\]\nThis is the same as: \\[\n\\begin{pmatrix}y_1\\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\\\ \\vdots \\\\ 1 & x_n\\end{pmatrix} \\begin{pmatrix}\\beta_0 \\\\ \\beta_1\\end{pmatrix}+\n\\begin{pmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_n\\end{pmatrix}\n\\]\nOn the order of appearece, this is equivalent to the following expression:\n\\[\nY = X\\beta + u\n\\]\nWhere \\(Y\\) and \\(u\\) are the first and last vectors, \\(X\\) is the matrix and \\(\\beta\\) is the vector of \\(\\beta\\)s.\nWhy is this useful?\nRemember that the sum of squares is the inner product \\(&lt;u,u&gt;=u'u=\\sum u_i^2\\). Then:\n\\[\n\\begin{aligned}\nu &= Y - X\\beta\\\\\n\\sum u_i^2=u'u &= \\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right)\\\\\n&= Y'Y - Y'X\\beta - \\beta'X' Y + \\beta' X'X \\beta\n\\end{aligned}\n\\]\nNote that \\(Y'X\\beta\\) has dimension \\(1\\) (\\(1\\times n\\) times \\(n\\times 2\\) times \\(2\\times 1\\)), and also that \\(\\left(Y'X\\beta\\right)'=\\beta' X' Y\\), but a scalar (a vector of dimension 1 is actually a scalar, a number) trasposed is the same number and therefore \\(Y'X\\beta=\\beta'X'Y\\). Finally, we can write:\n\\[\n\\begin{aligned}\n\\sum u^2=u'u&= Y'Y - 2 X'Y\\beta + \\beta' X'X \\beta\n\\end{aligned}\n\\]\nTo minimize the squared residuals, use the first order condition to obtain:\n\\[\n\\begin{aligned}\n-2X'Y + 2X'X\\beta &= 0\\\\\n\\beta = (X'X)^{-1}X'Y\n\\end{aligned}\n\\]\nWhere obviously implies that \\(X'X\\) must have full rank (no linearly dependent columns) to be invertible, which implies \\(X\\) must have no lineraly dependent columns.\n\n\n\n\n\n\nNoneNote about \\(\\frac{\\partial \\beta' X'X\\beta}{\\partial \\beta}\\)\n\n\n\nNote that \\(X\\beta=\\hat{y}\\) and therefore \\(\\beta' X'X\\beta=\\hat{y}'\\hat{y}=\\sum \\hat{y}^2\\) or \\(\\sum(\\beta_0 + \\beta_1 x_1)^2\\). The derivative with respect to \\(\\beta_0\\) is \\(\\sum 2 \\hat{y}\\cdot1\\) and the derivative with respect to \\(\\beta_1\\) is \\(\\sum 2 \\hat{y} x\\). You can factor out the 2, and note that \\(\\begin{pmatrix}1 &x\\end{pmatrix}'\\hat{y} = \\begin{pmatrix}\\sum \\hat{y}\\cdot 1 \\\\ \\sum \\hat{y} x \\end{pmatrix}\\). Finally recall that \\(\\hat{y}=X\\beta\\), and \\(\\begin{pmatrix}1 &x\\end{pmatrix}'=X'\\) to obtain \\(2X'X\\beta\\)."
  },
  {
    "objectID": "02_MLR_review.html#matrixvector-slr",
    "href": "02_MLR_review.html#matrixvector-slr",
    "title": "Review for lecture SLR",
    "section": "",
    "text": "The original equation:\n\\(y_i=\\beta_0+\\beta_1 x_i + u_i\\)\nThis holds for \\(i=1,2,...,n\\) therefore you can write\n\\[\n\\begin{pmatrix}y_1\\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\beta_0\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} +\n\\beta_1\\begin{pmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_n\\end{pmatrix} +\n\\begin{pmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_n\\end{pmatrix}\n\\]\nThis is the same as: \\[\n\\begin{pmatrix}y_1\\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{pmatrix} =\n\\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\\\ \\vdots \\\\ 1 & x_n\\end{pmatrix} \\begin{pmatrix}\\beta_0 \\\\ \\beta_1\\end{pmatrix}+\n\\begin{pmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ \\vdots \\\\ u_n\\end{pmatrix}\n\\]\nOn the order of appearece, this is equivalent to the following expression:\n\\[\nY = X\\beta + u\n\\]\nWhere \\(Y\\) and \\(u\\) are the first and last vectors, \\(X\\) is the matrix and \\(\\beta\\) is the vector of \\(\\beta\\)s.\nWhy is this useful?\nRemember that the sum of squares is the inner product \\(&lt;u,u&gt;=u'u=\\sum u_i^2\\). Then:\n\\[\n\\begin{aligned}\nu &= Y - X\\beta\\\\\n\\sum u_i^2=u'u &= \\left(Y-X\\beta\\right)'\\left(Y-X\\beta\\right)\\\\\n&= Y'Y - Y'X\\beta - \\beta'X' Y + \\beta' X'X \\beta\n\\end{aligned}\n\\]\nNote that \\(Y'X\\beta\\) has dimension \\(1\\) (\\(1\\times n\\) times \\(n\\times 2\\) times \\(2\\times 1\\)), and also that \\(\\left(Y'X\\beta\\right)'=\\beta' X' Y\\), but a scalar (a vector of dimension 1 is actually a scalar, a number) trasposed is the same number and therefore \\(Y'X\\beta=\\beta'X'Y\\). Finally, we can write:\n\\[\n\\begin{aligned}\n\\sum u^2=u'u&= Y'Y - 2 X'Y\\beta + \\beta' X'X \\beta\n\\end{aligned}\n\\]\nTo minimize the squared residuals, use the first order condition to obtain:\n\\[\n\\begin{aligned}\n-2X'Y + 2X'X\\beta &= 0\\\\\n\\beta = (X'X)^{-1}X'Y\n\\end{aligned}\n\\]\nWhere obviously implies that \\(X'X\\) must have full rank (no linearly dependent columns) to be invertible, which implies \\(X\\) must have no lineraly dependent columns.\n\n\n\n\n\n\nNoneNote about \\(\\frac{\\partial \\beta' X'X\\beta}{\\partial \\beta}\\)\n\n\n\nNote that \\(X\\beta=\\hat{y}\\) and therefore \\(\\beta' X'X\\beta=\\hat{y}'\\hat{y}=\\sum \\hat{y}^2\\) or \\(\\sum(\\beta_0 + \\beta_1 x_1)^2\\). The derivative with respect to \\(\\beta_0\\) is \\(\\sum 2 \\hat{y}\\cdot1\\) and the derivative with respect to \\(\\beta_1\\) is \\(\\sum 2 \\hat{y} x\\). You can factor out the 2, and note that \\(\\begin{pmatrix}1 &x\\end{pmatrix}'\\hat{y} = \\begin{pmatrix}\\sum \\hat{y}\\cdot 1 \\\\ \\sum \\hat{y} x \\end{pmatrix}\\). Finally recall that \\(\\hat{y}=X\\beta\\), and \\(\\begin{pmatrix}1 &x\\end{pmatrix}'=X'\\) to obtain \\(2X'X\\beta\\)."
  },
  {
    "objectID": "02_MLR_review.html#mlr",
    "href": "02_MLR_review.html#mlr",
    "title": "Review for lecture SLR",
    "section": "MLR",
    "text": "MLR\nThis can trivially be generalized to \\(k\\) regressors, just defining \\(X\\) as:\n\\[\n\\begin{aligned}\nX =\n\\begin{pmatrix}\n    1 & x_1^1 & x_2^1 & x_3^1 & \\dots & x_k^1 \\\\\n    1 & x_1^2 & x_2^2 & x_3^2 & \\dots & x_k^2 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1 & x_1^n & x_2^n & x_3^n & \\dots & x_k^n\n\\end{pmatrix}\\quad\\quad\n\\beta =\n\\begin{pmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\vdots \\\\ \\beta_k\n\\end{pmatrix}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "02_MLR_review.html#beta_j-as-a-regression-of-y-on-the-residuals-of-x_j-on-x_-j",
    "href": "02_MLR_review.html#beta_j-as-a-regression-of-y-on-the-residuals-of-x_j-on-x_-j",
    "title": "Review for lecture SLR",
    "section": "\\(\\beta_j\\) as a regression of \\(y\\) on the residuals of \\(x_j\\) on \\(x_{-j}\\)",
    "text": "\\(\\beta_j\\) as a regression of \\(y\\) on the residuals of \\(x_j\\) on \\(x_{-j}\\)\nLet \\[y=\\beta_0+\\sum \\beta_i x_i + u\\] Running the regression \\(x_j = \\gamma_0 + \\sum_{i\\neq j} \\gamma_i x_i + r_j\\) to obtain \\(r_j = x_j - \\gamma_0-\\sum_{i\\neq j} \\gamma_i x_i\\).\nMultiply the first equation by \\(r_j\\) and sum\n\\[\\sum r_j y = \\sum \\beta_0 r_j + \\sum \\beta_i x_i r_j + \\sum u r_j\\]\nRemember that \\(E[r_j]=0\\) and that \\(r_j\\) is orthogonal to \\(x_{-j}\\) and therefore the previous expression reduces to:\n\\[\\sum r_j y = \\beta_j \\sum  x_j r_j + \\sum u r_j\\]\nRecall that \\(r_j=x_j-\\gamma_0 - \\sum_{i\\neq j} \\gamma_i x_i\\), that is a linear combination of \\(x\\)s, and \\(u\\) is orthogonal to all of them. The constant is dealt with \\(E[u]=0\\).\nThis leaves us with:\n\\[\\sum r_j y = \\beta_j \\sum  x_j r_j\\]\nNow consider the original regression of \\(x_j\\) on \\(x_{-j}\\), multiply both sides by \\(r_j\\) to obtain:\n\\[r_jx_j = \\gamma_0r_j + \\sum_{i\\neq j} \\gamma_i x_i r_j + r_j ^2\\]\nAgain, \\(r_j\\) is orthogonal to all the \\(x_{-j}\\) and therefore the second term would dissapear when adding up. The first term dissapears because \\(E[r_j]=0\\), and therefore summing up on both sides we get:\n\\[\\sum r_j x_j = \\sum r_j^2\\]\nWhich we can replace in the previous equation to obtain:\n\\[\\sum r_j y = \\beta_j \\sum r_j^2\\]\nAnd finally\n\\[\\beta_j = \\frac{\\sum r_j y}{\\sum r_j^2}\\]"
  },
  {
    "objectID": "04_OVB_IV_review.html",
    "href": "04_OVB_IV_review.html",
    "title": "Review for lecture SLR",
    "section": "",
    "text": "Consider the true model \\[\nY=X\\beta+W\\gamma+\\epsilon\n\\]\nNow suppose you ignore the variable \\(W\\), is this a problem?\nIf you ingore \\(W\\) you estimate \\(Y=X\\tilde{\\beta}+u\\). Is \\(\\tilde{\\beta}\\) unbiased?\n\\[\nE[\\tilde{\\beta}]=(X'X)^{-1}X'Y=(X'X)^{-1}X'(X\\beta+\\underbrace{W\\gamma+\\epsilon}_{u})=\\beta+(X'X)^{-1}X'W\\gamma\n\\]\nNote that \\((X'X)^{-1}X'W\\) is the estimator for \\(\\alpha\\) of \\(W=X\\alpha+\\epsilon\\), and then\n\\[\nE[\\tilde{\\beta}]=\\beta+\\alpha\\gamma\n\\]\nSo \\(\\tilde{\\beta}\\) is biased if: \\(\\alpha\\neq 0\\) and \\(\\gamma\\neq 0\\), which means that \\(W\\) is uncorrelated with \\(X\\) (\\(\\alpha\\)) and \\(Y\\) (\\(\\gamma\\)). The sign of the bias \\(\\alpha\\gamma\\) will depend, then, on if \\(\\alpha\\) and \\(\\gamma\\) share the same sign or not.\nIf bias is positive, then \\(E[\\tilde{\\beta}]&gt;\\beta\\) so the true \\(\\beta\\) is smaller than your estimate. This is particularly troublesome if you got a slightly positive \\(\\beta\\) (you cannot be sure if the true \\(\\beta\\) is zero or even negative!).\nIf bias is negative, then \\(E[\\tilde{\\beta}]&lt;\\beta\\) so the true \\(\\beta\\) is larger than your estimate. This is particularly troublesome if you got a slightly negative \\(\\beta\\) (you cannot be sure if the true \\(\\beta\\) is zero or even positive!)."
  },
  {
    "objectID": "04_OVB_IV_review.html#ovb",
    "href": "04_OVB_IV_review.html#ovb",
    "title": "Review for lecture SLR",
    "section": "",
    "text": "Consider the true model \\[\nY=X\\beta+W\\gamma+\\epsilon\n\\]\nNow suppose you ignore the variable \\(W\\), is this a problem?\nIf you ingore \\(W\\) you estimate \\(Y=X\\tilde{\\beta}+u\\). Is \\(\\tilde{\\beta}\\) unbiased?\n\\[\nE[\\tilde{\\beta}]=(X'X)^{-1}X'Y=(X'X)^{-1}X'(X\\beta+\\underbrace{W\\gamma+\\epsilon}_{u})=\\beta+(X'X)^{-1}X'W\\gamma\n\\]\nNote that \\((X'X)^{-1}X'W\\) is the estimator for \\(\\alpha\\) of \\(W=X\\alpha+\\epsilon\\), and then\n\\[\nE[\\tilde{\\beta}]=\\beta+\\alpha\\gamma\n\\]\nSo \\(\\tilde{\\beta}\\) is biased if: \\(\\alpha\\neq 0\\) and \\(\\gamma\\neq 0\\), which means that \\(W\\) is uncorrelated with \\(X\\) (\\(\\alpha\\)) and \\(Y\\) (\\(\\gamma\\)). The sign of the bias \\(\\alpha\\gamma\\) will depend, then, on if \\(\\alpha\\) and \\(\\gamma\\) share the same sign or not.\nIf bias is positive, then \\(E[\\tilde{\\beta}]&gt;\\beta\\) so the true \\(\\beta\\) is smaller than your estimate. This is particularly troublesome if you got a slightly positive \\(\\beta\\) (you cannot be sure if the true \\(\\beta\\) is zero or even negative!).\nIf bias is negative, then \\(E[\\tilde{\\beta}]&lt;\\beta\\) so the true \\(\\beta\\) is larger than your estimate. This is particularly troublesome if you got a slightly negative \\(\\beta\\) (you cannot be sure if the true \\(\\beta\\) is zero or even positive!)."
  },
  {
    "objectID": "04_OVB_IV_review.html#iv-estimator",
    "href": "04_OVB_IV_review.html#iv-estimator",
    "title": "Review for lecture SLR",
    "section": "IV Estimator",
    "text": "IV Estimator\nWe need an IV estimator when in the model\n\\[\nY=X\\beta+u\n\\]\nwe believe that \\(X\\) might be correlated with \\(u\\). This will cause \\(\\beta\\) to be biased/inconsistent (remember that the OVB is a particular case, when \\(X\\) is uncorrelated with \\(W\\) there is no problem at all!).\nWe could ‚Äúsolve‚Äù the problem above if we could find an ‚Äúinstrument,‚Äù that is another variable \\(Z\\) such that \\(E[ZX]\\neq 0\\) (the instrument is relevant) and \\(E[Zu]=0\\) (the instrument is exogenous). \\(E[ZX]\\neq 0\\) can be tested, \\(E[Zu]=0\\) cannot. You need to reason on why this is the case.\nIf this holds true, then the estimator \\(ZX\\) is unbiased.\nNote\n\\[\nE[Z'u]=0 \\quad \\Leftrightarrow\\quad E[Z'(y-X\\beta)]=0 \\quad \\Leftrightarrow \\quad E[Z'X]\\beta=E[Z'y]\n\\]\nOr simply \\[Z'X\\beta=Z'y\\] from where solving for \\(\\beta\\) we get:\n\\[\n\\hat{\\beta}_{IV} = (\\underbrace{Z'X}_{cov(Z,X)})^{-1}\\underbrace{Z'y}_{cov(Z,Y)}\n\\]\nLet‚Äôs check this is unbiased/consistent:\n\\[\n\\begin{aligned}\nE[\\hat{\\beta}_{IV}]&=E\\left[(Z'X)^{-1}Z'y\\right]=E\\left[(Z'X)^{-1}Z'(X\\beta+u)\\right]\\\\\n&=\\beta+E[(Z'X)^{-1}Z'u] = \\beta\n\\end{aligned}\n\\]\nBecause \\(E[Z'u]=0\\) by assumption."
  },
  {
    "objectID": "06_IV.html#review-1",
    "href": "06_IV.html#review-1",
    "title": "IV",
    "section": "Review",
    "text": "Review\n2SLS estimator"
  },
  {
    "objectID": "06_IV.html#exercises",
    "href": "06_IV.html#exercises",
    "title": "IV",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "06_IV.html#exercise-6.1",
    "href": "06_IV.html#exercise-6.1",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\n\nBy how much taxes must be increased ot reach a certain reduction in cigarette consumption? Using the price elasticity for the demand, we are interested in estimating \\(\\beta_1\\) in:\n\\[\n  \\ln\\left(Q_i^{cigarettes}\\right)=\\beta_0+\\beta_1\\ln \\left(P_i^{cigarettes}\\right)+u_i\n\\]\nwhere \\(\\ln \\left(Q_i^{cigarettes}\\right)\\) is the number of cigarette packs per capita sold and \\(\\ln\\left(P_i^{cigarettes}\\right)\\) is the after-tax average real price per pack of cigarettes in state \\(i\\). We use the data set \\(CigarettesSW\\) - data on cigarette consumption for the 48 continental US States in 1995."
  },
  {
    "objectID": "06_IV.html#exercises-6.1",
    "href": "06_IV.html#exercises-6.1",
    "title": "IV",
    "section": "Exercises 6.1",
    "text": "Exercises 6.1\n\nIs it reasonable to use an OLS regression of log quantity on log price to estimate the price elasticity for the demand? Why?\n\n\nAn OLS regression of log quantity on log price cannot be used to estimate the effect of price since there is simultaneous causality between demand and supply. A possible solution is to use \\(IV\\) regression."
  },
  {
    "objectID": "06_IV.html#exercise-6.1-1",
    "href": "06_IV.html#exercise-6.1-1",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\n\nAssume \\(\\ln \\left(SalesTax_i\\right)\\) is used as an instrumental variable for the endogenous regressionr, \\(\\ln\\left(P_i^{cigarettes}\\right)\\). \\(\\ln\\left(SalesTax_i\\right)\\) is the portion of taxes on cigarettes arising from the general sales tax. Discuss the two conditions that need to be satisfied for \\(\\ln\\left(SalesTax_i\\right)\\) to be a valid instrument.\n\n\nIt is relevant if:\n\nRelevance condition: \\(X\\) and its instrument \\(Z\\) must be correlated. The tax is included in the (after-tax) average price per pack.\nInstrument exogeneity condition: The instrument \\(Z\\) must not be correlated with the error term \\(u\\). It is plausible that it is exogenous since the sales tax does not influence quantity sold directly but indirectly through the price."
  },
  {
    "objectID": "06_IV.html#exercise-6.1-2",
    "href": "06_IV.html#exercise-6.1-2",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\nThe output in the next slide presents estiamtes for three different specifications: 1. is a simple linear regression for the price elasticity; 2. is the first-stage of the 2SLS Estimator 3. the second stage of the 2SLS."
  },
  {
    "objectID": "06_IV.html#exercise-6.1-3",
    "href": "06_IV.html#exercise-6.1-3",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\n\nRegression"
  },
  {
    "objectID": "06_IV.html#exercise-6.1-4",
    "href": "06_IV.html#exercise-6.1-4",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\n\nHow much of the observed variation in \\(\\ln\\left(P_i^{cigarettes}\\right)\\) is explained by the instrument \\(\\ln\\left(SalesTax_i\\right)\\) Relate the result with the concept of weak instrument.\n\n\nThiso can be answered by looking at the regression‚Äôs \\(R^2\\) which states that about \\(47%\\) of the variation in after tax prices is explained by the variation of the sales tax across states. Threfore, there is evidence against being a weak instrument. A weak instrument explains little varaition int he endogenous regressor X (which might lead to inaccurate estimates as the distribution of the estiamtor deviates considerably from a normal distribution, even in large samples)."
  },
  {
    "objectID": "06_IV.html#exercise-6.1-5",
    "href": "06_IV.html#exercise-6.1-5",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\n\nExplain the difference between teh second-stage regression (3) and the SLR(1).\n\n\nIn (1) we have a SLR between quantity and price. While in (3) we use the estiamted fitted values obtaiend by the first stage regression and use them as the regressors for the price elasticity equation."
  },
  {
    "objectID": "06_IV.html#exercise-6.1-6",
    "href": "06_IV.html#exercise-6.1-6",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\n\nInterpret the coefficient for (3). Compare it with (1) and comment.\n\n\nThe 2SLS estiamte for \\(\\beta_1\\) suggests that an increase in cigarette prices by 1% reduces cigarette consumption by roughly 1.08%, which is fairly elastic. Lower than (1), but still not what we expect for demand of cigarettes."
  },
  {
    "objectID": "06_IV.html#exercise-6.1-7",
    "href": "06_IV.html#exercise-6.1-7",
    "title": "IV",
    "section": "Exercise 6.1",
    "text": "Exercise 6.1\n\nIntuitively, what do you point out as the main limitations of these models?\n\n\nEven though the elasticity was estimated with IV regression, there still might be a bias due to omitted variables. For example, there ar economic factors, like state income, which impact the demand for cigarettes and correlate with the sales tax. Thus, a multiple IV regression approach should be considered."
  },
  {
    "objectID": "06_IV.html#app-t-table",
    "href": "06_IV.html#app-t-table",
    "title": "IV",
    "section": "t table",
    "text": "t table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf-\\(\\alpha/2\\)\n0.50\n0.25\n0.20\n0.15\n0.10\n0.05\n0.025\n0.01\n0.005\n0.001\n0.0005\n\n\n\n\n1\n0.000\n1.000\n1.376\n1.963\n3.078\n6.314\n12.71\n31.82\n63.66\n318.31\n636.62\n\n\n2\n0.000\n0.816\n1.061\n1.386\n1.886\n2.920\n4.303\n6.965\n9.925\n22.327\n31.599\n\n\n3\n0.000\n0.765\n0.978\n1.250\n1.638\n2.353\n3.182\n4.541\n5.841\n10.215\n12.924\n\n\n4\n0.000\n0.741\n0.941\n1.190\n1.533\n2.132\n2.776\n3.747\n4.604\n7.173\n8.610\n\n\n5\n0.000\n0.727\n0.920\n1.156\n1.476\n2.015\n2.571\n3.365\n4.032\n5.893\n6.869\n\n\n6\n0.000\n0.718\n0.906\n1.134\n1.440\n1.943\n2.447\n3.143\n3.707\n5.208\n5.959\n\n\n7\n0.000\n0.711\n0.896\n1.119\n1.415\n1.895\n2.365\n2.998\n3.499\n4.785\n5.408\n\n\n8\n0.000\n0.706\n0.889\n1.108\n1.397\n1.860\n2.306\n2.896\n3.355\n4.501\n5.041\n\n\n9\n0.000\n0.703\n0.883\n1.100\n1.383\n1.833\n2.262\n2.821\n3.250\n4.297\n4.781\n\n\n10\n0.000\n0.700\n0.879\n1.093\n1.372\n1.812\n2.228\n2.764\n3.169\n4.144\n4.587\n\n\n11\n0.000\n0.697\n0.876\n1.088\n1.363\n1.796\n2.201\n2.718\n3.106\n4.025\n4.437\n\n\n12\n0.000\n0.695\n0.873\n1.083\n1.356\n1.782\n2.179\n2.681\n3.055\n3.930\n4.318\n\n\n13\n0.000\n0.694\n0.870\n1.079\n1.350\n1.771\n2.160\n2.650\n3.012\n3.852\n4.221\n\n\n14\n0.000\n0.692\n0.868\n1.076\n1.345\n1.761\n2.145\n2.624\n2.977\n3.787\n4.140\n\n\n15\n0.000\n0.691\n0.866\n1.074\n1.341\n1.753\n2.131\n2.602\n2.947\n3.733\n4.073\n\n\n16\n0.000\n0.690\n0.865\n1.071\n1.337\n1.746\n2.120\n2.583\n2.921\n3.686\n4.015\n\n\n17\n0.000\n0.689\n0.863\n1.069\n1.333\n1.740\n2.110\n2.567\n2.898\n3.646\n3.965\n\n\n18\n0.000\n0.688\n0.862\n1.067\n1.330\n1.734\n2.101\n2.552\n2.878\n3.610\n3.922\n\n\n19\n0.000\n0.688\n0.861\n1.066\n1.328\n1.729\n2.093\n2.539\n2.861\n3.579\n3.883\n\n\n20\n0.000\n0.687\n0.860\n1.064\n1.325\n1.725\n2.086\n2.528\n2.845\n3.552\n3.850\n\n\n21\n0.000\n0.686\n0.859\n1.063\n1.323\n1.721\n2.080\n2.518\n2.831\n3.527\n3.819\n\n\n22\n0.000\n0.686\n0.858\n1.061\n1.321\n1.717\n2.074\n2.508\n2.819\n3.505\n3.792\n\n\n23\n0.000\n0.685\n0.858\n1.060\n1.319\n1.714\n2.069\n2.500\n2.807\n3.485\n3.768\n\n\n24\n0.000\n0.685\n0.857\n1.059\n1.318\n1.711\n2.064\n2.492\n2.797\n3.467\n3.745\n\n\n25\n0.000\n0.684\n0.856\n1.058\n1.316\n1.708\n2.060\n2.485\n2.787\n3.450\n3.725\n\n\n26\n0.000\n0.684\n0.856\n1.058\n1.315\n1.706\n2.056\n2.479\n2.779\n3.435\n3.707\n\n\n27\n0.000\n0.684\n0.855\n1.057\n1.314\n1.703\n2.052\n2.473\n2.771\n3.421\n3.690\n\n\n28\n0.000\n0.683\n0.855\n1.056\n1.313\n1.701\n2.048\n2.467\n2.763\n3.408\n3.674\n\n\n29\n0.000\n0.683\n0.854\n1.055\n1.311\n1.699\n2.045\n2.462\n2.756\n3.396\n3.659\n\n\n30\n0.000\n0.683\n0.854\n1.055\n1.310\n1.697\n2.042\n2.457\n2.750\n3.385\n3.646\n\n\n40\n0.000\n0.681\n0.851\n1.050\n1.303\n1.684\n2.021\n2.423\n2.704\n3.307\n3.551\n\n\n60\n0.000\n0.679\n0.848\n1.045\n1.296\n1.671\n2.000\n2.390\n2.660\n3.232\n3.460\n\n\n80\n0.000\n0.678\n0.846\n1.043\n1.292\n1.664\n1.990\n2.374\n2.639\n3.195\n3.416\n\n\n100\n0.000\n0.677\n0.845\n1.042\n1.290\n1.660\n1.984\n2.364\n2.626\n3.174\n3.390\n\n\n1000\n0.000\n0.675\n0.842\n1.037\n1.282\n1.646\n1.962\n2.330\n2.581\n3.098\n3.300\n\n\nZ\n0.000\n0.674\n0.842\n1.036\n1.282\n1.645\n1.960\n2.326\n2.576\n3.090\n3.291"
  },
  {
    "objectID": "06_IV.html#app-f-table",
    "href": "06_IV.html#app-f-table",
    "title": "IV",
    "section": "F table",
    "text": "F table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf2-Œ±=0.05\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\n1.000\n161.448\n199.500\n215.707\n224.583\n230.162\n233.986\n236.768\n238.883\n240.543\n241.882\n\n\n2.000\n18.513\n19.000\n19.164\n19.247\n19.296\n19.330\n19.353\n19.371\n19.385\n19.396\n\n\n3.000\n10.128\n9.552\n9.277\n9.117\n9.013\n8.941\n8.887\n8.845\n8.812\n8.786\n\n\n4.000\n7.709\n6.944\n6.591\n6.388\n6.256\n6.163\n6.094\n6.041\n5.999\n5.964\n\n\n5.000\n6.608\n5.786\n5.409\n5.192\n5.050\n4.950\n4.876\n4.818\n4.772\n4.735\n\n\n6.000\n5.987\n5.143\n4.757\n4.534\n4.387\n4.284\n4.207\n4.147\n4.099\n4.060\n\n\n7.000\n5.591\n4.737\n4.347\n4.120\n3.972\n3.866\n3.787\n3.726\n3.677\n3.637\n\n\n8.000\n5.318\n4.459\n4.066\n3.838\n3.687\n3.581\n3.500\n3.438\n3.388\n3.347\n\n\n9.000\n5.117\n4.256\n3.863\n3.633\n3.482\n3.374\n3.293\n3.230\n3.179\n3.137\n\n\n10.000\n4.965\n4.103\n3.708\n3.478\n3.326\n3.217\n3.135\n3.072\n3.020\n2.978\n\n\n11.000\n4.844\n3.982\n3.587\n3.357\n3.204\n3.095\n3.012\n2.948\n2.896\n2.854\n\n\n12.000\n4.747\n3.885\n3.490\n3.259\n3.106\n2.996\n2.913\n2.849\n2.796\n2.753\n\n\n13.000\n4.667\n3.806\n3.411\n3.179\n3.025\n2.915\n2.832\n2.767\n2.714\n2.671\n\n\n14.000\n4.600\n3.739\n3.344\n3.112\n2.958\n2.848\n2.764\n2.699\n2.646\n2.602\n\n\n15.000\n4.543\n3.682\n3.287\n3.056\n2.901\n2.790\n2.707\n2.641\n2.588\n2.544\n\n\n16.000\n4.494\n3.634\n3.239\n3.007\n2.852\n2.741\n2.657\n2.591\n2.538\n2.494\n\n\n17.000\n4.451\n3.592\n3.197\n2.965\n2.810\n2.699\n2.614\n2.548\n2.494\n2.450\n\n\n18.000\n4.414\n3.555\n3.160\n2.928\n2.773\n2.661\n2.577\n2.510\n2.456\n2.412\n\n\n19.000\n4.381\n3.522\n3.127\n2.895\n2.740\n2.628\n2.544\n2.477\n2.423\n2.378\n\n\n20.000\n4.351\n3.493\n3.098\n2.866\n2.711\n2.599\n2.514\n2.447\n2.393\n2.348\n\n\n21.000\n4.325\n3.467\n3.072\n2.840\n2.685\n2.573\n2.488\n2.420\n2.366\n2.321\n\n\n22.000\n4.301\n3.443\n3.049\n2.817\n2.661\n2.549\n2.464\n2.397\n2.342\n2.297\n\n\n23.000\n4.279\n3.422\n3.028\n2.796\n2.640\n2.528\n2.442\n2.375\n2.320\n2.275\n\n\n24.000\n4.260\n3.403\n3.009\n2.776\n2.621\n2.508\n2.423\n2.355\n2.300\n2.255\n\n\n25.000\n4.242\n3.385\n2.991\n2.759\n2.603\n2.490\n2.405\n2.337\n2.282\n2.236\n\n\n26.000\n4.225\n3.369\n2.975\n2.743\n2.587\n2.474\n2.388\n2.321\n2.265\n2.220\n\n\n27.000\n4.210\n3.354\n2.960\n2.728\n2.572\n2.459\n2.373\n2.305\n2.250\n2.204\n\n\n28.000\n4.196\n3.340\n2.947\n2.714\n2.558\n2.445\n2.359\n2.291\n2.236\n2.190\n\n\n29.000\n4.183\n3.328\n2.934\n2.701\n2.545\n2.432\n2.346\n2.278\n2.223\n2.177\n\n\n30.000\n4.171\n3.316\n2.922\n2.690\n2.534\n2.421\n2.334\n2.266\n2.211\n2.165\n\n\n40.000\n4.085\n3.232\n2.839\n2.606\n2.449\n2.336\n2.249\n2.180\n2.124\n2.077\n\n\n60.000\n4.001\n3.150\n2.758\n2.525\n2.368\n2.254\n2.167\n2.097\n2.040\n1.993\n\n\n120.000\n3.920\n3.072\n2.680\n2.447\n2.290\n2.175\n2.087\n2.016\n1.959\n1.910\n\n\n1000000000.000\n3.841\n2.996\n2.605\n2.372\n2.214\n2.099\n2.010\n1.938\n1.880\n1.831"
  },
  {
    "objectID": "07_Heteroskedasticity.html#review-1",
    "href": "07_Heteroskedasticity.html#review-1",
    "title": "Heteroskedasticity",
    "section": "Review",
    "text": "Review\n\nTesting for Heteroskedasticity\nBreusch-Pagan test\nWhite test\nLazy White test\nWeighted Least Squares (WLS)\nRobust Standard Errors\nFeasible Generalized Least Squares"
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercises",
    "href": "07_Heteroskedasticity.html#exercises",
    "title": "Heteroskedasticity",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercise-7.1",
    "href": "07_Heteroskedasticity.html#exercise-7.1",
    "title": "Heteroskedasticity",
    "section": "Exercise 7.1",
    "text": "Exercise 7.1\nWhich of the following are consequences of heteroskedasticity? a) The OLS estimators, \\(\\hat{\\beta}_j\\), are inconsistent. b) The usual F statistics no longer has an \\(F\\) distribution. c) The OLS estimators are no longer BLUE.\n\n\nMLR5 is not needed for consistency, only MLR1-MLR4\nIf MLR5 fails, usual SEs are wrong and \\(t\\) and \\(F\\) statistics do not have \\(t\\) and \\(F\\) distributions, respetively, thus inference as usual is invalid.\nMLR1-MLR5 need to hold for OLS to be BLUE"
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercise-7.3",
    "href": "07_Heteroskedasticity.html#exercise-7.3",
    "title": "Heteroskedasticity",
    "section": "Exercise 7.3",
    "text": "Exercise 7.3\nConsider a linear model to explain omnthly beer consumption:\n\n\\[beer = \\beta_0+\\beta_1 inc + \\beta_2 price + \\beta_3 educ + \\beta_4 female + u\\]\n\\(E[u|X]=0\\), \\(V[u|X]=\\sigma^2 inc^2\\).\n\nWrite the transformed equation that has a homoskedastic error term.\n\nIn this case we know the expression of the variance of the model. Therefore we can use \\(WLS\\) where: \\[V[u|X]=\\sigma^2 inc^2=\\sigma^2 h(x)\\] The transformed equation is: \\[beer/\\sqrt{inc^2}=\\beta_0/\\sqrt{inc^2}+... + u/\\sqrt{inc^2}\\] The new error is homoskedastic: \\(V[u/inc|X]=\\sigma^2 inc^2/inc^2=\\sigma^2\\)"
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercise-7.7",
    "href": "07_Heteroskedasticity.html#exercise-7.7",
    "title": "Heteroskedasticity",
    "section": "Exercise 7.7",
    "text": "Exercise 7.7\nFrom a sample of 88 observations the following regression was obtained to explain the price of housing in a particular US state:\n\\[\n\\hat{p}_i = \\underset{\\underset{[37.14]}{(29.48)}}{-21.77}+\\underset{\\underset{[0.0013]}{0.0006}}{0.002}ls_i+\\underset{\\underset{[0.018]}{0.013}}{0.123}hs_i+\\underset{\\underset{[8.48]}{(9.01)}}{13.85} nr_i\n\\]\nwhere, for house \\(i\\), \\(p\\) represents price (in thousands), \\(ls\\) is the lot size (in \\(m^2\\)), \\(hs\\) is the size of the house (in \\(m^2\\)) and \\(nr\\) the number of rooms. The usual and heteroskedasticity-robust standard errors are in parenthesis and squared brackets, respetively."
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercise-7.7-1",
    "href": "07_Heteroskedasticity.html#exercise-7.7-1",
    "title": "Heteroskedasticity",
    "section": "Exercise 7.7",
    "text": "Exercise 7.7\n\nConsider the following regression\n\n\\[\n\\hat{u}_i=-5,522.79+0.202 ls_i + 1.69 hs_i + 1,041.76 nr_i\n\\]\nwith \\(R^2=0.160\\) and \\(F=5.339\\)\nHow many degrees of freedom has the \\(F\\) distribution from where you obtain the critical value? Given the output above, analyze whether the errors of the initial model are heteroskedastic. Use both \\(F\\) and \\(LM\\) versions of the test to derive your conclusion.\n\nThe output above is the Breush-Pagan test for heteroskedasticity: \\(H_0:V[u|X]=\\sigma^2\\), \\(H_a:V[u|X]=\\sigma^2_i\\).\n\\(F=[(R^2)/K]/[(1-R^2)/(N-K-1)]=[(0.16)/(3)]/[(1-0.16)/(88-3-1)]\\approx 5.339&gt;2.71=F^c_{3,88-3-1}\\)\n\\(LM=nR^2=88\\cdot 0.16=14.88&gt;7.815=\\chi_3^2\\) Reject \\(H_0\\)! There is evidence of heteroskedasticity."
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercise-7.7-2",
    "href": "07_Heteroskedasticity.html#exercise-7.7-2",
    "title": "Heteroskedasticity",
    "section": "Exercise 7.7",
    "text": "Exercise 7.7\n\nExplain why the model is estimated with heteroskedasticity-robust standard errors. Compare these results with those obtained with the usual standard errors.\n\n\nWe found evidence of heteroskedasticity. Therefore, usual standard errors are wrong. But the coefficients are still unbiased if MLR1-MLR4 hold. Thus, we can use estimated heteroskedasticity-robust standard errors to conduct inference.\n\\(ls\\) - usual \\(t=3.33^*\\) - robust \\(t=1.54\\) \\(hs\\) - usual \\(t=9.46^*\\) - robust \\(t=6.83^*\\) \\(nr\\) - usual \\(t=1.54\\) - robust \\(t=1.63\\)\n\\(ls\\) turns out to be not statistically significant at the 5% level. \\(hs\\) is the only statistically significant variable."
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercise-7.7-3",
    "href": "07_Heteroskedasticity.html#exercise-7.7-3",
    "title": "Heteroskedasticity",
    "section": "Exercise 7.7",
    "text": "Exercise 7.7\n\nOne of the econometricians in charge suggested that, given his knowledge of the market and the problem detected in (a), it would make sense to estiamte a model in which the elasticities price-lot size and price-house size were constant. The results of such model are:\n\n\\[\n\\widehat{ln(p_i)}=\\underset{(0.65)}{-1.30}+\\underset{(0.038)}{0.168}\\ln (ls_i)+\\underset{(0.093)}{0.70}\\ln (hs_i) + \\underset{(0.028)}{0.037}nr_i\n\\]\nwith \\(R^2=0.643\\).\nExplain why these auxiliary regressions were estimated. What do you conclude?\n\nTaking the log of the dependent variable usually reduces heteroskedasticity because the spread of the distribution is usually reduced and the influence of outliers is mitigated."
  },
  {
    "objectID": "07_Heteroskedasticity.html#exercise-7.7-4",
    "href": "07_Heteroskedasticity.html#exercise-7.7-4",
    "title": "Heteroskedasticity",
    "section": "Exercise 7.7",
    "text": "Exercise 7.7\n\nThe following auxiliary regressions were also estimated, using the residuals of the last model:\n\n\n\\[\n\\begin{aligned}\n\\hat{u}_i^2 =& 12.21 - 1.27 \\ln (ls_i) - 1.76 \\ln (hs_i) + 0.29 nr_i + 0.02 \\ln (ls_i)^2 +\\\\\n+& 0.04 \\ln (hs_i)^2-0.005 nr_i^2 + 0.12 [\\ln (ls_i)\\times \\ln (hs_i)]\\\\\n-&0.03[\\ln (ls_i)\\times nr_i] - 0.001[\\ln (hs_i)\\times nr_i]\n\\end{aligned}\n\\]\nWith \\(R^2=0.109\\) and\n\\[\n\\hat{u}_i^2=5.05-1.71\\ln \\widehat{\\ln (p_i)}+0.145\\widehat{\\ln (p_i)}^2\n\\] With \\(R^2=0.039\\).\n\nExplain why these auxiliary regressions were estimated. What do you conclude?\n\nThese auxiliary regressions were estimated to perform a White test and a ‚ÄúLazy‚Äù/Alternative White test. \\(H_0: V[u|X]=\\sigma^2\\), \\(H_a:V[u|X]=\\sigma_i^2\\)\n\\(LM_1=n\\cdot R^2=88\\cdot 0.109 = 9.592&lt;16.92=\\chi_9^2\\) \\(LM_2=n\\cdot R^2=88\\cdot 0.0309 = 3.432&lt;5.99=\\chi_2^2\\) There is no evidence of heteroskedasticity anymore!"
  },
  {
    "objectID": "08_R.html#clean-work-environment",
    "href": "08_R.html#clean-work-environment",
    "title": "R Class",
    "section": "Clean work environment",
    "text": "Clean work environment\n\nrm(list = ls()) \n\nUSE with CAUTION: this will delete everything in your environment"
  },
  {
    "objectID": "08_R.html#load-packages",
    "href": "08_R.html#load-packages",
    "title": "R Class",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(stargazer)\nlibrary(skimr)\nlibrary(caret)\nlibrary(xtable)\nlibrary(GGally)\nlibrary(ggthemes)\nlibrary(car)"
  },
  {
    "objectID": "08_R.html#set-working-directory",
    "href": "08_R.html#set-working-directory",
    "title": "R Class",
    "section": "Set working directory",
    "text": "Set working directory\n\nsetwd(\"/your/path/to/the/files\")"
  },
  {
    "objectID": "08_R.html#load-data",
    "href": "08_R.html#load-data",
    "title": "R Class",
    "section": "Load data",
    "text": "Load data\n\nDM &lt;- read.csv('datasets/DirectMarketing.csv')"
  },
  {
    "objectID": "08_R.html#data-set-overview",
    "href": "08_R.html#data-set-overview",
    "title": "R Class",
    "section": "Data set overview",
    "text": "Data set overview\n\nskim(DM) %&gt;% yank(\"numeric\")\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nSalary\n0\n1\n56103.90\n30616.31\n10100\n29975.00\n53700\n77025.0\n168800\n‚ñá‚ñá‚ñÖ‚ñÇ‚ñÅ\n\n\nChildren\n0\n1\n0.93\n1.05\n0\n0.00\n1\n2.0\n3\n‚ñá‚ñÖ‚ñÅ‚ñÇ‚ñÇ\n\n\nCatalogs\n0\n1\n14.68\n6.62\n6\n6.00\n12\n18.0\n24\n‚ñá‚ñá‚ñÅ‚ñÜ‚ñÜ\n\n\nAmountSpent\n0\n1\n1216.77\n961.07\n38\n488.25\n962\n1688.5\n6217\n‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ"
  },
  {
    "objectID": "08_R.html#deal-with-nas",
    "href": "08_R.html#deal-with-nas",
    "title": "R Class",
    "section": "Deal with NAs",
    "text": "Deal with NAs\nCreate a new category ‚ÄúNo past purchases‚Äù.\n\nDM$History[is.na(DM$History)] &lt;- \"No past purchases\""
  },
  {
    "objectID": "08_R.html#create-new-variable-pastpurchases",
    "href": "08_R.html#create-new-variable-pastpurchases",
    "title": "R Class",
    "section": "Create new variable PastPurchases",
    "text": "Create new variable PastPurchases\nMake it equal to ‚ÄúPast purchases‚Äù if it is an old customer\n\nDM$PastPurchases &lt;- ifelse(DM$History == \"No past purchases\", \n                           \"No past purchases\", \"Past purchases\")"
  },
  {
    "objectID": "08_R.html#quick-summary-statistics-table",
    "href": "08_R.html#quick-summary-statistics-table",
    "title": "R Class",
    "section": "Quick summary statistics table",
    "text": "Quick summary statistics table\n\nstargazer(as.data.frame(DM), type = \"text\", \n    title=\"Descriptive stats\")\n\n\nDescriptive stats\n======================================================\nStatistic     N      Mean     St. Dev.   Min     Max  \n------------------------------------------------------\nSalary      1,000 56,103.900 30,616.310 10,100 168,800\nChildren    1,000   0.934      1.051      0       3   \nCatalogs    1,000   14.682     6.623      6      24   \nAmountSpent 1,000 1,216.770   961.069     38    6,217 \n------------------------------------------------------"
  },
  {
    "objectID": "08_R.html#quick-summary-statistics-table-1",
    "href": "08_R.html#quick-summary-statistics-table-1",
    "title": "R Class",
    "section": "Quick summary statistics table",
    "text": "Quick summary statistics table\n\nstargazer(as.data.frame(DM), type = \"html\", \n    title=\"Descriptive stats\", out=\"DS.html\")\n\nThis will create a new file DS.html that you can include in your final document."
  },
  {
    "objectID": "08_R.html#new-data-frame-all-numeric",
    "href": "08_R.html#new-data-frame-all-numeric",
    "title": "R Class",
    "section": "New data frame all numeric",
    "text": "New data frame all numeric\n\nDM_aux &lt;- dummyVars(\" ~ .\", data = DM)\nDM_num &lt;- data.frame(predict(DM_aux, newdata = DM))"
  },
  {
    "objectID": "08_R.html#summary-statistics-table-with-numeric",
    "href": "08_R.html#summary-statistics-table-with-numeric",
    "title": "R Class",
    "section": "Summary statistics table with numeric",
    "text": "Summary statistics table with numeric\n\nstargazer(as.data.frame(DM_num), type = \"text\", \n          title=\"Descriptive stats: all numeric data\")\n\n\nDescriptive stats: all numeric data\n=========================================================================\nStatistic                        N      Mean     St. Dev.   Min     Max  \n-------------------------------------------------------------------------\nAgeMiddle                      1,000   0.508      0.500      0       1   \nAgeOld                         1,000   0.205      0.404      0       1   \nAgeYoung                       1,000   0.287      0.453      0       1   \nGenderFemale                   1,000   0.506      0.500      0       1   \nGenderMale                     1,000   0.494      0.500      0       1   \nOwnHomeOwn                     1,000   0.516      0.500      0       1   \nOwnHomeRent                    1,000   0.484      0.500      0       1   \nMarriedMarried                 1,000   0.502      0.500      0       1   \nMarriedSingle                  1,000   0.498      0.500      0       1   \nLocationClose                  1,000   0.710      0.454      0       1   \nLocationFar                    1,000   0.290      0.454      0       1   \nSalary                         1,000 56,103.900 30,616.310 10,100 168,800\nChildren                       1,000   0.934      1.051      0       3   \nHistoryHigh                    1,000   0.255      0.436      0       1   \nHistoryLow                     1,000   0.230      0.421      0       1   \nHistoryMedium                  1,000   0.212      0.409      0       1   \nHistoryNo.past.purchases       1,000   0.303      0.460      0       1   \nCatalogs                       1,000   14.682     6.623      6      24   \nAmountSpent                    1,000 1,216.770   961.069     38    6,217 \nPastPurchasesNo.past.purchases 1,000   0.303      0.460      0       1   \nPastPurchasesPast.purchases    1,000   0.697      0.460      0       1   \n-------------------------------------------------------------------------"
  },
  {
    "objectID": "08_R.html#store-the-summary-statistics-in-a-file",
    "href": "08_R.html#store-the-summary-statistics-in-a-file",
    "title": "R Class",
    "section": "Store the summary statistics in a file",
    "text": "Store the summary statistics in a file\n\nstargazer(as.data.frame(DM_num), type = \"html\", \n          title=\"Descriptive stats: all numeric data\",\n          out=\"DS2.html\")"
  },
  {
    "objectID": "08_R.html#create-correlation-matrix",
    "href": "08_R.html#create-correlation-matrix",
    "title": "R Class",
    "section": "Create correlation matrix",
    "text": "Create correlation matrix\n\ncorrDM = cor(\n    DM[\n        ,c(\"AmountSpent\", \"Salary\", \"Catalogs\", \"Children\")\n        ]\n    )"
  },
  {
    "objectID": "08_R.html#display-correlation-matrix-with-title",
    "href": "08_R.html#display-correlation-matrix-with-title",
    "title": "R Class",
    "section": "Display correlation matrix with title",
    "text": "Display correlation matrix with title\n\nstargazer(corrDM, title=\"Correlation Matrix\", \n          digits = 2, type=\"text\")\n\n\nCorrelation Matrix\n================================================\n            AmountSpent Salary Catalogs Children\n------------------------------------------------\nAmountSpent      1       0.70    0.47    -0.22  \nSalary         0.70       1      0.18     0.05  \nCatalogs       0.47      0.18     1      -0.11  \nChildren       -0.22     0.05   -0.11      1    \n------------------------------------------------"
  },
  {
    "objectID": "08_R.html#store-the-correlation-matrix-in-a-file",
    "href": "08_R.html#store-the-correlation-matrix-in-a-file",
    "title": "R Class",
    "section": "Store the correlation matrix in a file",
    "text": "Store the correlation matrix in a file\n\nstargazer(corrDM, title=\"Correlation Matrix\", \n          digits = 2, type=\"html\",\n          out=\"corr.html\")"
  },
  {
    "objectID": "08_R.html#histogram-of-amountspent",
    "href": "08_R.html#histogram-of-amountspent",
    "title": "R Class",
    "section": "Histogram of AmountSpent",
    "text": "Histogram of AmountSpent\n\nggplot(data = DM, aes(x = AmountSpent)) + geom_histogram()"
  },
  {
    "objectID": "08_R.html#histogram-of-salary",
    "href": "08_R.html#histogram-of-salary",
    "title": "R Class",
    "section": "Histogram of Salary",
    "text": "Histogram of Salary\n\nggplot(data = DM, aes(x = Salary)) + geom_histogram()"
  },
  {
    "objectID": "08_R.html#histogram-of-catalogs",
    "href": "08_R.html#histogram-of-catalogs",
    "title": "R Class",
    "section": "Histogram of Catalogs",
    "text": "Histogram of Catalogs\n\nggplot(data = DM, aes(x = Catalogs)) + geom_histogram()"
  },
  {
    "objectID": "08_R.html#histogram-of-children",
    "href": "08_R.html#histogram-of-children",
    "title": "R Class",
    "section": "Histogram of Children",
    "text": "Histogram of Children\n\nggplot(data = DM, aes(x = Children)) + geom_histogram()"
  },
  {
    "objectID": "08_R.html#amount-spent-by-gender",
    "href": "08_R.html#amount-spent-by-gender",
    "title": "R Class",
    "section": "Amount spent by gender",
    "text": "Amount spent by gender\n\nggplot(data = DM, aes(x = factor(Gender), y = AmountSpent)) +\n  geom_boxplot() + \n  theme(text = element_text(size = 25))"
  },
  {
    "objectID": "08_R.html#distribution-of-amount-spent-by-location",
    "href": "08_R.html#distribution-of-amount-spent-by-location",
    "title": "R Class",
    "section": "Distribution of amount spent by location",
    "text": "Distribution of amount spent by location\n\nggplot(data = DM, aes(x = factor(Location), y = AmountSpent)) + geom_boxplot() + \n  theme(text = element_text(size = 25))"
  },
  {
    "objectID": "08_R.html#distribution-of-amount-spent-by-age",
    "href": "08_R.html#distribution-of-amount-spent-by-age",
    "title": "R Class",
    "section": "Distribution of amount spent by age",
    "text": "Distribution of amount spent by age\n\nggplot(data = DM, aes(x = factor(Age), y = AmountSpent)) +\n  geom_boxplot() +\n  theme(text = element_text(size = 25))"
  },
  {
    "objectID": "08_R.html#distribution-of-amount-spent-by-history",
    "href": "08_R.html#distribution-of-amount-spent-by-history",
    "title": "R Class",
    "section": "Distribution of amount spent by history",
    "text": "Distribution of amount spent by history\n\nggplot(data = DM, aes(x = factor(History), y = AmountSpent)) +\n  geom_boxplot() +\n  theme(text = element_text(size = 25))"
  },
  {
    "objectID": "08_R.html#scatterplot-of-amount-spent-and-salary",
    "href": "08_R.html#scatterplot-of-amount-spent-and-salary",
    "title": "R Class",
    "section": "Scatterplot of amount spent and salary",
    "text": "Scatterplot of amount spent and salary\n\nggplot(data = DM, aes(x = Salary, y = AmountSpent)) +\n  geom_point() +\n  stat_smooth(method = lm) +\n  theme(text = element_text(size = 18))"
  },
  {
    "objectID": "08_R.html#impact-of-salary-on-amountspent",
    "href": "08_R.html#impact-of-salary-on-amountspent",
    "title": "R Class",
    "section": "Impact of Salary on AmountSpent",
    "text": "Impact of Salary on AmountSpent\n\nlm1 &lt;- lm(AmountSpent ~ Salary, data = DM)"
  },
  {
    "objectID": "08_R.html#regression-results",
    "href": "08_R.html#regression-results",
    "title": "R Class",
    "section": "Regression results",
    "text": "Regression results\n\nstargazer(lm1, type = \"text\", no.space = TRUE, digits=3, \n          title = \"Impact of Salary on Amount Spent\")\n\n\nImpact of Salary on Amount Spent\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                            AmountSpent        \n-----------------------------------------------\nSalary                       0.022***          \n                              (0.001)          \nConstant                      -15.318          \n                             (45.374)          \n-----------------------------------------------\nObservations                   1,000           \nR2                             0.489           \nAdjusted R2                    0.489           \nResidual Std. Error     687.065 (df = 998)     \nF Statistic          956.694*** (df = 1; 998)  \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "08_R.html#regression-results-1",
    "href": "08_R.html#regression-results-1",
    "title": "R Class",
    "section": "Regression results",
    "text": "Regression results\n\nstargazer(lm1, type = \"html\", no.space = TRUE, digits=3, \n          title = \"Impact of Salary on Amount Spent\",\n          out=\"SLR.html\")"
  },
  {
    "objectID": "08_R.html#extract-and-store-model-coefficients",
    "href": "08_R.html#extract-and-store-model-coefficients",
    "title": "R Class",
    "section": "Extract and store model coefficients",
    "text": "Extract and store model coefficients\n\nlm1.coeffs &lt;- coefficients(lm1)\nlm1.coeffs\n\n (Intercept)       Salary \n-15.31782541   0.02196082"
  },
  {
    "objectID": "08_R.html#predict-amount-spent-when-profits-100000",
    "href": "08_R.html#predict-amount-spent-when-profits-100000",
    "title": "R Class",
    "section": "Predict amount spent when profits = 100,000",
    "text": "Predict amount spent when profits = 100,000\n\nlm1.coeffs[1] + lm1.coeffs[2]*100000\n\n(Intercept) \n   2180.764"
  },
  {
    "objectID": "08_R.html#add-new-column-to-data-set-with-predicted-salary-and-residuals",
    "href": "08_R.html#add-new-column-to-data-set-with-predicted-salary-and-residuals",
    "title": "R Class",
    "section": "Add new column to data set with predicted salary and residuals",
    "text": "Add new column to data set with predicted salary and residuals\n\nDM$lm1_y = predict(lm1)\nDM$lm1_u = residuals(lm1)"
  },
  {
    "objectID": "08_R.html#define-multiple-regression-models-incrementally",
    "href": "08_R.html#define-multiple-regression-models-incrementally",
    "title": "R Class",
    "section": "Define multiple regression models, incrementally",
    "text": "Define multiple regression models, incrementally\n\nm1 &lt;- AmountSpent ~ Salary \nm2 &lt;- AmountSpent ~ Salary + Catalogs + Children \nm3 &lt;- AmountSpent ~ Salary + Catalogs + Children + Gender + History + Location"
  },
  {
    "objectID": "08_R.html#run-models-1-to-3",
    "href": "08_R.html#run-models-1-to-3",
    "title": "R Class",
    "section": "Run models 1 to 3",
    "text": "Run models 1 to 3\n\nlm.spend1 &lt;- lm(m1, data = DM)\nlm.spend2 &lt;- lm(m2, data = DM)\nlm.spend3 &lt;- lm(m3, data = DM)"
  },
  {
    "objectID": "08_R.html#output-table-with-models-1-to-3",
    "href": "08_R.html#output-table-with-models-1-to-3",
    "title": "R Class",
    "section": "Output table with Models 1 to 3",
    "text": "Output table with Models 1 to 3\n\nstargazer(lm.spend1, lm.spend2, lm.spend3, \n          header = FALSE , type = \"text\", column.sep.width = \"1pt\", \n          no.space = TRUE, df = FALSE, digits = 2, \n          title = \"Regression Analysis - Determinants of Amount Spent\")\n\n\nRegression Analysis - Determinants of Amount Spent\n========================================================\n                               Dependent variable:      \n                         -------------------------------\n                                   AmountSpent          \n                            (1)       (2)        (3)    \n--------------------------------------------------------\nSalary                    0.02***   0.02***    0.02***  \n                          (0.001)   (0.001)    (0.001)  \nCatalogs                            47.70***   41.75*** \n                                     (2.76)     (2.45)  \nChildren                           -198.69*** -171.98***\n                                    (17.09)    (16.70)  \nGenderMale                                     -54.28*  \n                                               (32.17)  \nHistoryLow                                    -355.06***\n                                               (65.43)  \nHistoryMedium                                 -408.81***\n                                               (52.37)  \nHistoryNo past purchases                        -0.04   \n                                               (51.06)  \nLocationFar                                   436.05*** \n                                               (35.86)  \nConstant                  -15.32   -442.76*** -228.38***\n                          (45.37)   (53.72)    (79.90)  \n--------------------------------------------------------\nObservations               1,000     1,000      1,000   \nR2                         0.49       0.66       0.75   \nAdjusted R2                0.49       0.66       0.74   \nResidual Std. Error       687.06     562.53     485.43  \nF Statistic              956.69*** 640.00***  365.60*** \n========================================================\nNote:                        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "08_R.html#output-table-with-models-1-to-3-1",
    "href": "08_R.html#output-table-with-models-1-to-3-1",
    "title": "R Class",
    "section": "Output table with Models 1 to 3",
    "text": "Output table with Models 1 to 3\n\nstargazer(lm.spend1, lm.spend2, lm.spend3, \n          header = FALSE , type = \"html\", column.sep.width = \"1pt\", \n          no.space = TRUE, df = FALSE, digits = 2, \n          title = \"Regression Analysis - Determinants of Amount Spent\",\n          out=\"MLR.html\")"
  },
  {
    "objectID": "08_R.html#test-in-model-2-if-the-coefficient-of-salary-if-statistically-significant",
    "href": "08_R.html#test-in-model-2-if-the-coefficient-of-salary-if-statistically-significant",
    "title": "R Class",
    "section": "Test in model 2 if the coefficient of Salary if statistically significant",
    "text": "Test in model 2 if the coefficient of Salary if statistically significant\n\nlinearHypothesis(lm.spend2, c(\"Salary=0\"))\n\n\nLinear hypothesis test:\nSalary = 0\n\nModel 1: restricted model\nModel 2: AmountSpent ~ Salary + Catalogs + Children\n\n  Res.Df       RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    997 689995620                                  \n2    996 315171647  1 374823972 1184.5 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "08_R.html#test-in-model-2-if-the-effect-of-4-catalogs-is-the-symmetric-of-the-effect-of-1-child",
    "href": "08_R.html#test-in-model-2-if-the-effect-of-4-catalogs-is-the-symmetric-of-the-effect-of-1-child",
    "title": "R Class",
    "section": "Test in model 2 if the effect of 4 catalogs is the symmetric of the effect of 1 child",
    "text": "Test in model 2 if the effect of 4 catalogs is the symmetric of the effect of 1 child\n\nlinearHypothesis(lm.spend2, c(\"4*Catalogs=-Children\"))\n\n\nLinear hypothesis test:\n4 Catalogs  + Children = 0\n\nModel 1: restricted model\nModel 2: AmountSpent ~ Salary + Catalogs + Children\n\n  Res.Df       RSS Df Sum of Sq     F Pr(&gt;F)\n1    997 315214682                          \n2    996 315171647  1     43034 0.136 0.7124"
  },
  {
    "objectID": "08_R.html#test-in-model-2-if-the-coefficients-associated-with-catalogs-and-children-are-jointly-significant",
    "href": "08_R.html#test-in-model-2-if-the-coefficients-associated-with-catalogs-and-children-are-jointly-significant",
    "title": "R Class",
    "section": "Test in model 2 if the coefficients associated with Catalogs and Children are jointly significant",
    "text": "Test in model 2 if the coefficients associated with Catalogs and Children are jointly significant\n\nlinearHypothesis(lm.spend2, c(\"Catalogs=0\",\"Children=0\"))\n\n\nLinear hypothesis test:\nCatalogs = 0\nChildren = 0\n\nModel 1: restricted model\nModel 2: AmountSpent ~ Salary + Catalogs + Children\n\n  Res.Df       RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    998 471114029                                 \n2    996 315171647  2 155942381 246.4 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "10_Time Series Panel.html#exercises",
    "href": "10_Time Series Panel.html#exercises",
    "title": "Time Series II + Panel",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-11.5",
    "href": "10_Time Series Panel.html#exercise-11.5",
    "title": "Time Series II + Panel",
    "section": "Exercise 11.5",
    "text": "Exercise 11.5\nUsing US data for hte period 1960-2005, the following regression of wages on productivty in the business sector was estimated:\n\\[\\widehat{\\ln Comp_t}=\\underset{(0.0547)}{1.6067} + \\underset{(0.0124)}{0.6522} \\ln Prod_t\\]"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-11.5-a",
    "href": "10_Time Series Panel.html#exercise-11.5-a",
    "title": "Time Series II + Panel",
    "section": "Exercise 11.5-a",
    "text": "Exercise 11.5-a\nBelow you have the plot of the residuals from the regression along time (left) and the plot of the residuals against their first lag (right). What can you conclude regarding serial correlation?\n\n\nPlot 1: the residuals are systematically positive/negative Plot 2: The residuasl and their lads are (strongly) positively correlated There is evidence of serial correlation (at least of order 1)"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-11.5-b",
    "href": "10_Time Series Panel.html#exercise-11.5-b",
    "title": "Time Series II + Panel",
    "section": "Exercise 11.5-b",
    "text": "Exercise 11.5-b\nThe Durbin-Watson (DW) for the above regression is 0.218. What do you conclude? State the null and the appropriate alternative for this test. What are the critical values that you consider for this test? Why?\n\nUnder the null and alternative hypotheses: \\(H_0=\\rho=0\\) and \\(H_a: \\rho &gt;0\\) In the table of the DW we have two values \\(d_L\\) and \\(d_u\\). In the exercise \\(n=46\\), \\(k=1\\), \\(\\alpha=5%\\): \\(d_L=1.48\\) and \\(d_U=1.58\\).\nSince \\(DW=0.218 &lt; d_L\\) we reject the \\(H_0\\) and there is evidence of \\(AR(1)\\) serial correlation."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-11.5-c",
    "href": "10_Time Series Panel.html#exercise-11.5-c",
    "title": "Time Series II + Panel",
    "section": "Exercise 11.5-c",
    "text": "Exercise 11.5-c\n\nA higher order of autocorrelation (6) was tested using the Breusch-Godfrey tets. What do you conclude from the output of this test, provided below?\n\n\n\nBG of higher order of serial correlation (order 6). Including the independent variables (\\(lnprod\\)) makes the test valid with or without strict exogeneity.\nNote \\(LN=(n-q)R^2\\). \\(n-q\\) is the number of observations left when taking into account the obs lost due to the lags.\n\\(LM=(46-6)\\times 0.7635=30.54&gt;12.59\\) (critical value from \\(\\chi^2_6\\) at 5%)\nReject the null of no serial correlation! (Only \\(\\hat{u}_{t-1}\\) individually significant, suggest \\(AR(1)\\))"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-4.5-d",
    "href": "10_Time Series Panel.html#exercise-4.5-d",
    "title": "Time Series II + Panel",
    "section": "Exercise 4.5-d",
    "text": "Exercise 4.5-d\nSuspecting model misspecification, a trend was added to the initial model. Considering the reported estimates below, what do you conclude? Is there still eevidence for autocorrelation?\n\\[\\widehat{\\ln Comp_t}=\\underset{(0.307)}{0.121}-\\underset{(0.002)}{0.008}t+\\underset{(0.0776)}{1.0283} \\ln Prod_t\\]\nWith \\(DW=0.45\\)\n\nIn the exercise \\(n=46\\), \\(k=2\\), \\(\\alpha=5%\\), \\(d_L=1.43\\), \\(d_U=1.62\\). Since \\(DW&lt;d_L\\) we reject the \\(H_0\\) and there is still evidence of \\(AR(1)\\) serial autocorrelation."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-4.5-e",
    "href": "10_Time Series Panel.html#exercise-4.5-e",
    "title": "Time Series II + Panel",
    "section": "Exercise 4.5-e",
    "text": "Exercise 4.5-e\n\nThree alternatives were cosnidered in order to corect for autocorrelation:\n\nInitial model in first-differences \\(\\widehat{\\Delta \\ln Comp_t}=\\underset{(0.057)}{0.654}\\Delta \\ln Prod_t\\) with \\(DW=1.74\\)\nModel with trend in first-differences \\(\\widehat{\\Delta \\ln Comp_t} = \\underset{(0.003)}{0.001}+\\underset{(0.109)}{0.619}\\Delta\\ln Prod_t\\) with \\(DW=1.71\\).\nQuasi-differenced initial model (Cochrane-Orcutt esitmation): \\[\\widehat{\\ln Comp_t}=\\underset{(0.192)}{1.955}+\\underset{(0.042)}{0.577} \\ln Prod_t^*\\] with \\(\\hat{\\rho}=0.869\\), \\(DW=1.70\\)\n\nIs the problem of autocorrelation solved?\n\n\nIn the exercise \\(n=45\\), \\(k=1\\), \\(\\alpha=5%\\), \\(d_U=1.48\\), \\(d_L=1.57\\)\nSince for all alternatives \\(4-d_U&gt;DW&gt;d_U\\) we fail to reject the \\(H_0\\) and there is not enough evidence to reject the absence of autocorrelation."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-4.5-f",
    "href": "10_Time Series Panel.html#exercise-4.5-f",
    "title": "Time Series II + Panel",
    "section": "Exercise 4.5-f",
    "text": "Exercise 4.5-f\nConsider the following heteroskedasticity test for the initial model:\n\\[\\hat{u}_t^2=\\underset{(23.64)}{94.27}-\\underset{(0.514)}{1.836}\\widehat{\\ln y_t} + \\underset{(0.003)}{0.009}\\widehat{\\ln y_t}^2\\] with \\(n=46\\) and \\(R^2=0.296\\)\nWhat do you conclude? Is there a solution for both serial correlation and heteroskedasticity?\n\n\\(LM=46\\times 0.296=13.754&gt;5.99\\) (critical value from \\(\\chi_2^2\\) at \\(5%\\))\nThere is evidence of heteroskedasticity\nNewey-West s.e. account for both, heteroskedasticity and serial correlation!"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-4.5-g",
    "href": "10_Time Series Panel.html#exercise-4.5-g",
    "title": "Time Series II + Panel",
    "section": "Exercise 4.5-g",
    "text": "Exercise 4.5-g\nConsider the following estimates for the initial model with \\(Newey-West\\) standard errors. How do they compare with theusual standard errors?\n\\[\\widehat{\\ln Comp_t}=\\underset{(0.079)^*}{1.607}+\\underset{(0.018)^*}{0.652}\\ln Prod_t\\]\n\nThey are higher than usual standard errors. Still, the coefficient is statisticall significant. \\(t=0.652/0.018=36\\)."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1",
    "href": "10_Time Series Panel.html#exercise-12.1",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1",
    "text": "Exercise 12.1\nYou are provided with a panel dataset of 400 firms observed over 5 years (from 2015 to 2019). The dataset includes the following variables:\n\n\nirm id: Firm identifier.\nyear: Year of observation.\nR&D expense: Annual R&D expenditure (in millions of dollars).\nprofit: Annual profit before tax (in millions of dollars).\ncapital: Total capital stock (in millions of dollars).\nindustry: Categorical variable indicating the industry sector (values 1 to 8).\nage: Age of the firm (in years).\nCEO tenure: Tenure of the CEO (in years)."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-1",
    "href": "10_Time Series Panel.html#exercise-12.1-1",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1",
    "text": "Exercise 12.1\n\nYou are interested in studying the determinants of R&D expenditure, focusing on how profit, capital stock, and industry sector influence a firm‚Äôs investment in R&D. Below are outputs from statistical software for three models estimated using this dataset.\n\n\nPooled"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-2",
    "href": "10_Time Series Panel.html#exercise-12.1-2",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1",
    "text": "Exercise 12.1\n\nYou are interested in studying the determinants of R&D expenditure, focusing on how profit, capital stock, and industry sector influence a firm‚Äôs investment in R&D. Below are outputs from statistical software for three models estimated using this dataset.\n\n\nFixed Effects"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-a",
    "href": "10_Time Series Panel.html#exercise-12.1-a",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1-a",
    "text": "Exercise 12.1-a\nExplain the main differences between the pooled OLS model, the fixed effects model, and the random effects model in the context of this study on R&D expenditure. Discuss the assumptions underlying each model, especially regarding the inclusion of the industry variable.\n\nPooled OLS Model: * Treats the panel data as a large cross-sectional dataset, ignoring the panel structure (time and individual dimensions). * Includes all variables, including time-invariant variables like industry, as regressors. * Assumes there are no omitted variables that vary across firms or over time and are correlated with the regressors. * Assumes exogeneity: Regressors, including industry, are uncorrelated with the error term.\nFixed Effects: * Controls for unobserved time-invariant heterogeneity by allowing each firm to have its own intercept (firm-specific effects). * Focuses on within-firm variations over time. * Time-invariant variables like industry are omitted because their effects are absorbed by the fixed effects. * Assumes unobserved firm-specific effects are constant over time. * Allows for correlation between unobserved effects and the regressors. * Provides consistent estimates even when unobserved firm characteristics are correlated with the regressors. * Cannot estimate the effect of time-invariant variables like industry."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-b",
    "href": "10_Time Series Panel.html#exercise-12.1-b",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1-b",
    "text": "Exercise 12.1-b\nBased on the outputs provided, interpret the estimated coefficients for profit and capital in all three models. Discuss why the estimates for these variables might differ among the pooled OLS, fixed effects, and random effects models.\n\nPooled OLS Model: * profit: An increase of $1 million in profit is associated with an increase of $0.28 million in R&D expenditure, ceteris paribus. * capital: An increase of $1 million in capital stack is associated with an increase of $0.45 million in R&D expenditure, ceteris paribus.\nFixed Effects: * profit: An increase of $1 million in profit is associated with an increase of $0.1 million in R&D expenditure, ceteris paribus. * capital: An increase of $1 million in capital stack is associated with an increase of $0.2 million in R&D expenditure, ceteris paribus.\nThe Pooled OLS coefficients for profit and capital are higher than those in the FE model. This suggests that unobserved firm-specific characteristics correlated with profit and capital may inflate the estimated effects in Pooled OLS model. By accounting for time-invariant firm characteristics, the FE model reduces potential bias, resulting in lower (and potentially more accurate) estimates."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-c",
    "href": "10_Time Series Panel.html#exercise-12.1-c",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1-c",
    "text": "Exercise 12.1-c\nThe fixed effects model does not include the industry variables in the output. Explain why this occurs and discuss the implications for interpreting the effect of industry sector on R&D expenditure.\n\nThe industry variables are time-invariant for each firm (firms do not change industries over time). In the FE model, time-invariant dummies are perfectly collinear with the firm-specific fixed effects. This means FE cannot estimate the coefficients of industry as the effect of industry on R&D expenditure (together with all other time-invariant characteristics) is absorbed into the fixed effects"
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-d",
    "href": "10_Time Series Panel.html#exercise-12.1-d",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1-d",
    "text": "Exercise 12.1-d\nExplain why the constant term is omitted in the fixed effects model. How are the firm-specific intercepts and time-invariant variables handled in this model?\n\nThe Fixed Effects model includes firm-specific intercepts (fixed effects) for each firm. Including a general constant term would cause perfect multicollinearity since the sum of the fixed effects and the constant would not be identifiable. Each firm has its own intercept, capturing all time-invariant characteristics unique to that firm."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-e",
    "href": "10_Time Series Panel.html#exercise-12.1-e",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1-e",
    "text": "Exercise 12.1-e\nConsidering that CEO tenure may change over time for each firm, discuss its estimated effect in each model. Based on the outputs, does CEO tenure have a significant impact on R&D expenditure? How might the interpretation differ across models?\n\nPooled OLS Model: * Coefficient(0.05), p-value = 0.01: CEO tenure is statistically significant. A one-year increase in CEO tenure is associated with an increase of $0.05 million in R&D expenditure.\nFixed Effects: * Coefficient(0.02), p-value = 0.18: CEO tenure is not statistically significant at any reasonable level. Within a firm, changes in CEO tenure over time do not significantly affect R&D expenditure.\nIn Pooled OLS, unobserved firm-specific characteristics correlated with CEO tenure may inflate its estimated effect. The FE model controls for these unobserved characteristics, re- ducing both the coefficient and its significance. This means the impact from CEO tenure may actually be driven by between-firm variation. Within-firm changes in CEO tenure may not significantly impact R&D expenditure."
  },
  {
    "objectID": "10_Time Series Panel.html#exercise-12.1-f",
    "href": "10_Time Series Panel.html#exercise-12.1-f",
    "title": "Time Series II + Panel",
    "section": "Exercise 12.1-f",
    "text": "Exercise 12.1-f\nDiscuss how unobserved heterogeneity might affect the estimates in the pooled OLS and random effects models, particularly regarding the industry variable. Why might the fixed effects model provide more reliable estimates in the presence of unobserved heterogeneity?\n\nUnobserved firm-specific characteristics (e.g., management quality, innovation culture) that are time-invariant and affect R&D expenditure may be correlated with regressors like profit, capital, and industry. Pooled OLS does not adequately control for unobserved heterogeneity if it is correlated with the regressors. This can lead to biased and inconsistent estimates.\nFixed Effects controls for all time-invariant unobserved heterogeneity by using within-firm variation. This provides consistent estimates even when unobserved firm-specific effects are correlated with the regressors.\nRegarding industry variables: In Pooled OLS model, the effect of industry may capture both observed and unobserved industry characteristics. If unobserved heterogeneity is present, the estimates for industry may still be biased in these models."
  },
  {
    "objectID": "R.html#the-first-thing-to-do",
    "href": "R.html#the-first-thing-to-do",
    "title": "Regression with R",
    "section": "The first thing to do",
    "text": "The first thing to do\nThe first thing to do is to clean the environment\n\nrm(list = ls())\n\n\nUse with CAUTION: this will delete everything in your environment"
  },
  {
    "objectID": "R.html#load-libraries",
    "href": "R.html#load-libraries",
    "title": "Regression with R",
    "section": "Load libraries",
    "text": "Load libraries\nYou do not need to this immediately, but it is good practice. For this lecture, we need to load the following packages:\n\nlibrary(tidyverse)\nlibrary(stargazer)\nlibrary(skimr)\nlibrary(caret)\nlibrary(xtable)\nlibrary(GGally)\nlibrary(ggthemes)\nlibrary(car)"
  },
  {
    "objectID": "R.html#install-libraries",
    "href": "R.html#install-libraries",
    "title": "Regression with R",
    "section": "Install libraries",
    "text": "Install libraries\nIf you fail to load one of these libraries, you can install it with:\n\ninstall.packages{'tidyverse'}\n\nFor example, with this we install the library tidyverse"
  },
  {
    "objectID": "R.html#set-working-directory",
    "href": "R.html#set-working-directory",
    "title": "Regression with R",
    "section": "Set working directory",
    "text": "Set working directory\nIn here, we will tell R where our files (data) are, and by default, where it will store any file we might want to export (images for example.)\n\nsetwd(\"/the/path/to/your/files\")"
  },
  {
    "objectID": "R.html#load-and-work-data",
    "href": "R.html#load-and-work-data",
    "title": "Regression with R",
    "section": "Load and work data",
    "text": "Load and work data\n\nDM &lt;- read.csv('DirectMarketing.csv')\nskim_without_charts(DM)\nDM$History[is.na(DM$History)] &lt;- \"No past purchases\"\nDM$PastPurchases &lt;- ifelse(DM$History == \"No past purchases\",\n    \"No past purchases\", \"Past purchases\")"
  },
  {
    "objectID": "R.html#quick-summary-statistics-table",
    "href": "R.html#quick-summary-statistics-table",
    "title": "Regression with R",
    "section": "Quick summary statistics table",
    "text": "Quick summary statistics table\n\nstargazer(as.data.frame(DM), type = \"text\", title=\"Descriptive stats\")\nstargazer(as.data.frame(DM), type = \"html\",\n    title=\"Descriptive stats\", out=\"DS.html\")"
  },
  {
    "objectID": "R.html#if-you-have-non-numeric-data",
    "href": "R.html#if-you-have-non-numeric-data",
    "title": "Regression with R",
    "section": "If you have non-numeric data",
    "text": "If you have non-numeric data\n\nDM_aux &lt;- dummyVars(\" ~ .\", data = DM)\nDM_num &lt;- data.frame(predict(DM_aux, newdata = DM))\nstargazer(as.data.frame(DM_num), type = \"text\", \n          title=\"Descriptive stats: all numeric data\")\n\nstargazer(as.data.frame(DM_num), type = \"html\", \n          title=\"Descriptive stats: all numeric data\",\n          out=\"DS2.html\")"
  },
  {
    "objectID": "R.html#correlation-matrix",
    "href": "R.html#correlation-matrix",
    "title": "Regression with R",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\ncorrDM = cor(DM[,c(\"AmountSpent\", \"Salary\", \"Catalogs\", \"Children\")])\n\nstargazer(corrDM, title=\"Correlation Matrix\", \n          digits = 2, type=\"text\")\n\nstargazer(corrDM, title=\"Correlation Matrix\", \n          digits = 2, type=\"html\",\n          out=\"corr.html\")"
  },
  {
    "objectID": "R.html#histogram",
    "href": "R.html#histogram",
    "title": "Regression with R",
    "section": "Histogram",
    "text": "Histogram\n\nggplot(data = DM, aes(x = AmountSpent)) +\n    geom_histogram()\nggplot(data = DM, aes(x = Salary)) +\n    geom_histogram()\nggplot(data = DM, aes(x = Catalogs)) +\n    geom_histogram()\nggplot(data = DM, aes(x = Children)) +\n    geom_histogram()"
  },
  {
    "objectID": "R.html#data-distribution",
    "href": "R.html#data-distribution",
    "title": "Regression with R",
    "section": "Data distribution",
    "text": "Data distribution\n\nggplot(data = DM, aes(x = factor(Gender), y = AmountSpent)) +\n    geom_boxplot() + \n    theme(text = element_text(size = 25))\nggplot(data = DM, aes(x = factor(Location), y = AmountSpent)) +\n    geom_boxplot() +\n    theme(text = element_text(size = 25))\nggplot(data = DM, aes(x = factor(Age), y = AmountSpent)) +\n    geom_boxplot() +\n    theme(text = element_text(size = 25))\nggplot(data = DM, aes(x = factor(History), y = AmountSpent)) +\n    geom_boxplot() +\n    theme(text = element_text(size = 25))"
  },
  {
    "objectID": "R.html#scatterplots",
    "href": "R.html#scatterplots",
    "title": "Regression with R",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nggplot(data = DM, aes(x = Salary, y = AmountSpent)) +\n    geom_point() + \n    stat_smooth(method = lm) +\n    theme(text = element_text(size = 18))"
  },
  {
    "objectID": "R.html#linear-regression",
    "href": "R.html#linear-regression",
    "title": "Regression with R",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nlm1 &lt;- lm(AmountSpent ~ Salary, data = DM)\n\nstargazer(lm1, type = \"text\", no.space = TRUE, digits=3, \n          title = \"Impact of Salary on Amount Spent\")\n\nstargazer(lm1, type = \"html\", no.space = TRUE, digits=3, \n          title = \"Impact of Salary on Amount Spent\",\n          out=\"SLR.html\")"
  },
  {
    "objectID": "R.html#work-with-the-coefficients",
    "href": "R.html#work-with-the-coefficients",
    "title": "Regression with R",
    "section": "Work with the coefficients",
    "text": "Work with the coefficients\n\nlm1.coeffs &lt;- coefficients(lm1)\nlm1.coeffs\n\nlm1.coeffs[1] + lm1.coeffs[2]*100000"
  },
  {
    "objectID": "R.html#predicted-values-haty-hatu",
    "href": "R.html#predicted-values-haty-hatu",
    "title": "Regression with R",
    "section": "Predicted values (\\(\\hat{y}\\), \\(\\hat{u}\\))",
    "text": "Predicted values (\\(\\hat{y}\\), \\(\\hat{u}\\))\nThis add a new variable to our original dataset:\n\nDM$lm1_y = predict(lm1)\nDM$lm1_u = residuals(lm1)"
  },
  {
    "objectID": "R.html#linear-regression-with-more-htan-one-variable",
    "href": "R.html#linear-regression-with-more-htan-one-variable",
    "title": "Regression with R",
    "section": "Linear regression with more htan one variable",
    "text": "Linear regression with more htan one variable\nYou can store the models, and estimate them later.\n\nm1 &lt;- AmountSpent ~ Salary \nm2 &lt;- AmountSpent ~ Salary + Catalogs + Children \nm3 &lt;- AmountSpent ~ Salary + Catalogs + Children +\n    Gender + History + Location"
  },
  {
    "objectID": "R.html#estimate-models",
    "href": "R.html#estimate-models",
    "title": "Regression with R",
    "section": "Estimate models",
    "text": "Estimate models\n\nlm.spend1 &lt;- lm(m1, data = DM)\nlm.spend2 &lt;- lm(m2, data = DM)\nlm.spend3 &lt;- lm(m3, data = DM)"
  },
  {
    "objectID": "R.html#output-table-with-models-1-to-3",
    "href": "R.html#output-table-with-models-1-to-3",
    "title": "Regression with R",
    "section": "Output table with Models 1 to 3",
    "text": "Output table with Models 1 to 3\n\nstargazer(lm.spend1, lm.spend2, lm.spend3, \n          header = FALSE , type = \"text\", column.sep.width = \"1pt\", \n          no.space = TRUE, df = FALSE, digits = 2, \n          title = \"Regression Analysis - Determinants of Amount Spent\")\n\nstargazer(lm.spend1, lm.spend2, lm.spend3, \n          header = FALSE , type = \"html\", column.sep.width = \"1pt\", \n          no.space = TRUE, df = FALSE, digits = 2, \n          title = \"Regression Analysis - Determinants of Amount Spent\",\n          out=\"MLR.html\")"
  },
  {
    "objectID": "R.html#running-some-tests-on-the-models",
    "href": "R.html#running-some-tests-on-the-models",
    "title": "Regression with R",
    "section": "Running some tests on the models:",
    "text": "Running some tests on the models:\n\nlinearHypothesis(lm.spend2, c(\"Salary=0\"))\nlinearHypothesis(lm.spend2, c(\"4*Catalogs=-Children\"))\nlinearHypothesis(lm.spend2, c(\"Catalogs=0\",\"Children=0\"))"
  },
  {
    "objectID": "theory_time_series.html#assumptions",
    "href": "theory_time_series.html#assumptions",
    "title": "Time Series 3",
    "section": "Assumptions",
    "text": "Assumptions\n\n\n\n\nSLR\nMLR\nTS\n\n\n\n\nLinearity in \\(\\beta\\)\nSLR1\nMLR1\nTS1\n\n\nRandom sampling\nSLR2\nMLR2\n\n\n\nSample variation in \\(X\\)\nSLR3\n\n\n\n\nNo perfect collinearity\n\nMLR4\nTS2\n\n\n\\(E[u|X]=0\\)\nSLR4\nMLR4\nTS3\n\n\n\\(V[u|X]=\\sigma^2\\)\nSLR5\nMLR5\nTS4\n\n\n\\(Corr[u_su_t|X]=0\\) for \\(t\\neq s\\)\n\n\nTS5"
  },
  {
    "objectID": "theory_time_series.html#the-problem",
    "href": "theory_time_series.html#the-problem",
    "title": "Time Series 3",
    "section": "The problem",
    "text": "The problem\nLarge samples in time series (normal frequency time series) are hard to come by!\n\nWhy we need large samples? to use large sample approximations results (asymptotics for example)."
  },
  {
    "objectID": "theory_time_series.html#stationary-process",
    "href": "theory_time_series.html#stationary-process",
    "title": "Time Series 3",
    "section": "Stationary Process",
    "text": "Stationary Process\n\n\n\n\n\n\nStationary process\n\n\nThe stochastic process \\(\\{x_t\\}_{t\\in\\mathbb{N}}\\) is stationray if for any monotonically increasing function \\(g:\\mathbb{N}\\mapsto\\mathbb{N}\\), the joint distribution of \\[(x_{g(1)},x_{g(2)},\\dots,x_{g(m)})\\] is equal to the distribution of \\[(x_{g(1)+h},x_{g(2)+h},\\dots,x_{g(m)}+h)\\] for any \\(h\\in\\mathbb{N}\\).\n\n\n\n\nMaybe it helps the plot with a time series, take a subset, shift it with a different color and illustrate the consequence. A stationary process implies that the probability distributions of the data are stable over time. If we take some data points in our sample, and we shift it by \\(h\\) the joint probability of the data remains unchanged."
  },
  {
    "objectID": "theory_time_series.html#stationary-process-2",
    "href": "theory_time_series.html#stationary-process-2",
    "title": "Time Series 3",
    "section": "Stationary Process",
    "text": "Stationary Process\nA stochastic process that is not stationary is said to be nonstationary.ü•Å\nIt is difficult to identify a stationary process because you do not have all the data! However it can be very simple to identify a non-stationary process, for example a time series with a trend.\n\nFor some results, a weaker form of stationary is enough‚Ä¶\n\nAnother way to read a stationary stochastic process is to say that it is identically distributed, but it says even more, for example the joint distribution of \\((x_1,x_2)\\) is the same as for \\((x_t,x_{t+1})\\) \\(\\forall t\\)."
  },
  {
    "objectID": "theory_time_series.html#covariance-stationary-process",
    "href": "theory_time_series.html#covariance-stationary-process",
    "title": "Time Series 3",
    "section": "Covariance Stationary Process",
    "text": "Covariance Stationary Process\n\n\n\n\n\n\nImportant\n\n\nA stochastic process \\(\\{x_t, t\\in\\mathbb{N}\\}\\) with a finite second moment \\(E[x_t^2]&lt;\\infty\\), is covariance stationary if:\n\n\\(E[x_t]\\) is constant\n\\(V[x_t]\\) is constant\n\\(\\forall t,h\\in\\mathbb{N}\\) we have \\(Cov(x_t,x_{t+h})=\\phi(h)\\), i.e.¬†it depends only on \\(h\\), and not \\(t\\).\n\n\n\n\n\nCovariance stationarity focuses only on the first two moments of a stochastic process: the mean and variance, and the covariance is constant in the same distance between two terms of the sequence, not its location. Obviously this carries to the correlation as well."
  },
  {
    "objectID": "theory_time_series.html#covariance-stationary-process-1",
    "href": "theory_time_series.html#covariance-stationary-process-1",
    "title": "Time Series 3",
    "section": "Covariance Stationary Process",
    "text": "Covariance Stationary Process\nIf a stationary process has a finite second moment, then it is Covariance Stationary as well. Note that converse is not true, i.e.¬†a Covariance Stationary Process needs not be Stationary.\n\nThe intuition on why we need these things to do econometrics in time series is that we need to learn from the data!, if what happened with the data in the past is no good to tell anything on how the data will behave in the future there is no point in trying to learn. We need to expect some kind of regularity or stability with the data.\nOn a technical leve, we need these to use the asymptotic results, the law of large numbers and the central limit theorem.\nNote that TS4 (homoskedasticity) and TS5(no serial correlation) imply some kind of stationarity!"
  },
  {
    "objectID": "theory_time_series.html#weakly-dependent-time-series",
    "href": "theory_time_series.html#weakly-dependent-time-series",
    "title": "Time Series 3",
    "section": "Weakly dependent Time Series",
    "text": "Weakly dependent Time Series\n\n\n\n\n\n\nImportant\n\n\nA stationary time series process \\(\\{x_t, t\\in\\mathbb{N}\\}\\) is said to be weakly dependent if \\(x_t\\) and \\(x_{t+h}\\) are ‚Äúalmost independent‚Äù as \\(h\\rightarrow\\infty\\).\n\n\n\n\nWe could say that intuitively a time series is weakly dependent if the correlation between \\(x_t\\) and \\(x_{t+h}\\) gets smaller and smaller as \\(h\\) increases.\n\nThe definition is vague, because no good and rigorous mathematical definition covers all the cases that can be of interest. There are many specific forms of weak dependence, but those are beyond the scope of this course."
  },
  {
    "objectID": "theory_time_series.html#weakly-dependent-time-series-1",
    "href": "theory_time_series.html#weakly-dependent-time-series-1",
    "title": "Time Series 3",
    "section": "Weakly dependent Time Series",
    "text": "Weakly dependent Time Series\n\n\n\n\n\n\nImportant\n\n\nA stochastic process \\(\\{x_t, t\\in\\mathbb{N}\\}\\) is said to be asymtotically uncorrelated if\n\\[Corr(x_t,x_{t+h})\\underset{h\\rightarrow\\infty}{\\rightarrow}0\\]\n\n\n\nThis is basically how we can characterize weakly dependnece.\n\nWhy is this important? Well, essentially it replaces the assumption of random sampling in implying that the law of large numbers LLN and hte central limit theorem CLT hold. Themost well-known CLT for time series data requires stationarity and some form of weak dependence."
  },
  {
    "objectID": "theory_time_series.html#moving-average-process",
    "href": "theory_time_series.html#moving-average-process",
    "title": "Time Series 3",
    "section": "Moving average process",
    "text": "Moving average process\nThe moving average process is an example of a weakly dependent process:\n\\[x_t = e_t + \\alpha_1 e_{t-1},\\quad t\\in\\mathbb{N}\\]\nwhere \\(\\{e_t\\}_{t\\in\\mathbb{N}}\\) is an iid sequence with zero mean and variance \\(\\sigma_e^2\\). In particular, this process is called a moving average proces of order one MA(1)\n\nWe can see that \\(x_t\\) is a weighted avreage of \\(e_t\\) and \\(e_{t-1}\\), in the next period we drop \\(x_{t-1}\\) and incorporate \\(e_{t+1}\\).\nCheck that MA(1) is weakly dependent:\n\n\\(Cov(x_t,x_{t+1})=\\alpha_1\\sigma_2^2\\) and then it does not depend on \\(t\\).\nIt follows that \\(Corr(x_t,x_{t+1})=\\alpha_1/(1+\\alpha_1^2)\\).\nCheck that \\(x_{t}\\) and \\(x_{t+3}\\) for example is zero.\n\nConclude that given \\(e_t\\) is identically distributed, this can be shown to be actually stationary. Then the LLN and CLT can be applied to this."
  },
  {
    "objectID": "theory_time_series.html#autoregressive-processes",
    "href": "theory_time_series.html#autoregressive-processes",
    "title": "Time Series 3",
    "section": "Autoregressive processes",
    "text": "Autoregressive processes\nConsider now the following process\n\n\n\n\n\n\nImportant\n\n\n##Autoregressive procees of order one AR(1) \\[y_t=\\rho y_{t-1}+e_t,\\quad t\\in\\mathbb{N}\\] with \\(e_t\\) and iid sequence with \\(E[e]=0\\) and \\(V[e]=\\sigma_e^2\\). Let \\(e_t\\) be independent of \\(y_0\\) and \\(E[y_0]=0\\). This is called an autoregressive process of order one AR(1)\nWe need a crucial assumption for the weak dependence of the AR(1) process, which is \\(|\\rho|&lt;1\\). In this case we say \\(\\{y_t\\}\\) is a stable AR(1) process.\n\n\n\n\nTo see that a stable AR(1) is asymptotically uncorrelated, it is useful to assume that the process is covariance stationary. In fact it can generally be shown that \\(\\{y_t\\}\\) is strictly stationary. Then \\(E[y_t]=E[y_{t-1}]\\) and with \\(\\rho\\neq 1\\) this happens only if \\(E[y_t]=0\\). Take the variance of the AR(1) and show that \\(\\sigma_y^2=\\rho^2\\sigma_y^2+\\sigma_e^2\\), solving for \\(\\sigma_y^2\\) you get \\(\\sigma_2^2/(1-\\rho^2)\\).\nShow before that \\(E[y_t]=0\\) by taking expectation on the AR(1)."
  },
  {
    "objectID": "theory_time_series.html#autoregressive-processes-1",
    "href": "theory_time_series.html#autoregressive-processes-1",
    "title": "Time Series 3",
    "section": "Autoregressive processes",
    "text": "Autoregressive processes\nwe can show easily that \\[Corr(y_t,y_{t+h})=\\frac{Cov(y_t,y_{t+h})}{\\sigma_y^2}=\\rho_1^h\\]\nNote that a consequence of this is that for any \\(t\\) \\(Corr(y_t,y_{t+1})=\\rho\\)! and moreover, as \\(|\\rho|&lt;1\\), \\(\\rho^h\\underset{h\\rightarrow\\infty}{\\rightarrow}0\\).\nWe have that the AR(1) process is weakly dependent.\n\nFor that write \\(y_{t+h}=\\rho y_{t+h-1}+e_{t+h}\\). Replace \\(y_{t+h-1}\\) recursively, \\(\\rho(\\rho y_{t+h-2}+e_{t+h-1})+e_{t+h}\\) which equals \\(\\rho^2 y_{t+h-2}+\\rho e_{t+h-1} + e_{t+h}\\) doing it again and again until we get \\(\\rho^{h}y_t + \\rho^{h-1}e_{t+1}+\\dots+\\rho e_{t+h-1}+e_{t+h}\\)\nThen multiply by \\(y_t\\) and take expectation, because \\(E[y]=0\\) this is the covariance, and \\(E[ye]=0\\), so you end up with \\(\\rho ^h \\sigma^y^2\\)."
  },
  {
    "objectID": "theory_time_series.html#random-walk",
    "href": "theory_time_series.html#random-walk",
    "title": "Time Series 3",
    "section": "Random Walk",
    "text": "Random Walk\nWhat happens if in the AR(1) model \\(|\\rho|\\geq 1\\)? We needed that for weakly dependence‚Ä¶\nSadly, many economic series are characterized with a \\(\\rho = 1\\). Was it all for nothing then?\n\nLet‚Äôs see, the model would be something like\n\\[ y_t = y_{t-1}+e_t\\] with a well behaved error term. This process is known as a random walk."
  },
  {
    "objectID": "theory_time_series.html#random-walk-1",
    "href": "theory_time_series.html#random-walk-1",
    "title": "Time Series 3",
    "section": "Random Walk",
    "text": "Random Walk\nLet‚Äôs characterize this process, what is \\(E[y_t]\\)?\n\\[y_t=y_{t-1}+e_t\\quad \\Rightarrow\\quad E[y_t]=E[y_0],\\ V[y_t]=\\sigma_e^2 t\\]\nNote the consequences of this! \\(y_0\\) affects all the future values of \\(y\\), and moreover, the best predition for the future is the current value, \\(E[y_{t+h}|y_t]=y_t\\).\nAlso, the variance explodes! It can be shown further that \\[Corr(y_t,y_{t+h})=\\sqrt{\\frac{t}{(t+h)}}\\]\n\nFor expected value, use substitution for \\(y_{t-1}\\) and take expectation, that kills all the \\(e\\)\nFor variance, the same thing, but take variance. Note that \\(V[y_0]=0\\) as it is fixed.\nContast with the prediction in the AR(1) \\(E[y_{t+h}|y_t]=\\rho^h y_t\\), so it goes to zero as \\(h\\) increasese (the unconditional mean of \\(y\\)), and \\(y_t\\) loses importance.\nAlso it is clear that the correlation approaches 1 as \\(h\\rightarrow\\infty\\)."
  },
  {
    "objectID": "theory_time_series.html#random-walk-2",
    "href": "theory_time_series.html#random-walk-2",
    "title": "Time Series 3",
    "section": "Random Walk",
    "text": "Random Walk\nA random walk does not satisfy the requirement of an asymptotically uncorrelated sequence.\nIndeed, a random walk is a special case of what is known as unit root process (so \\(\\rho=1\\) in an AR(1) model).\nRemember that a trending and a highly persisten series are not the same, however many times time series have both features, they are highly persistent and they have a trend. let‚Äôs see an exmaple of this."
  },
  {
    "objectID": "theory_time_series.html#random-walk-with-drift",
    "href": "theory_time_series.html#random-walk-with-drift",
    "title": "Time Series 3",
    "section": "Random walk with drift",
    "text": "Random walk with drift\nA random walk with drift is a time series process characterized by the following expression:\n\\[ y_t = \\alpha_0 + y_{t-1} + e_t,\\quad t\\in\\mathbb{N}\\]\nLet \\(\\{e_t\\}\\) and \\(y_0\\) satisfy the properties for the normal random walk process model. Now if we iterate as we did previously:\n\\[y_t = \\alpha_0 t + e_t + e_{t-1}+\\dots+e_1+y_0\\] and if \\(y_0=0\\), \\(E[y_t]=\\alpha_0 t\\). That is, the expected value of \\(y_t\\) is growing with \\(t\\) (or decreasing if \\(\\alpha_0&lt;1\\))\n\nNote that \\(E[y_{t+h|y_t}=\\alpha_0 h + y_t]\\) and so the best prediction for \\(y_{t+h}\\) at \\(t\\) is \\(y_t\\) plus the drift \\(\\alpha_0 h\\). The variance suffers no change, as \\(\\alpha\\) is a constant."
  },
  {
    "objectID": "theory_time_series.html#references",
    "href": "theory_time_series.html#references",
    "title": "Time Series 3",
    "section": "References",
    "text": "References\nYou can find these contents and examples in chapter 11, sections 1 and 3 of the book."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Material",
    "section": "",
    "text": "Statistical Revision\nSLR\nMLR\nInference\nOVB_IV\nDummies\nIV 2SLS\nHeteroskedasticity\nR\nTime Series\nTime Series - Panel\n\nR Extra"
  },
  {
    "objectID": "materials.html#slides",
    "href": "materials.html#slides",
    "title": "Material",
    "section": "",
    "text": "Statistical Revision\nSLR\nMLR\nInference\nOVB_IV\nDummies\nIV 2SLS\nHeteroskedasticity\nR\nTime Series\nTime Series - Panel\n\nR Extra"
  },
  {
    "objectID": "materials.html#notes",
    "href": "materials.html#notes",
    "title": "Material",
    "section": "Notes",
    "text": "Notes\n\nSLR\nMLR\nOVB IV\nIV 2SLS\nHeteroskedasticity"
  },
  {
    "objectID": "01_SLR.html#review-1",
    "href": "01_SLR.html#review-1",
    "title": "SLR",
    "section": "Review",
    "text": "Review\n\nSLR\nLog transformations\n\\(SST = SSR + SSE\\)"
  },
  {
    "objectID": "01_SLR.html#exercises",
    "href": "01_SLR.html#exercises",
    "title": "SLR",
    "section": "Exercises",
    "text": "Exercises\nhttps://moodle.novasbe.pt/mod/folder/view.php?id=4021"
  },
  {
    "objectID": "01_SLR.html#question-1.1-1",
    "href": "01_SLR.html#question-1.1-1",
    "title": "SLR",
    "section": "Question 1.1",
    "text": "Question 1.1\nEurope is in the midst of an unprecedented human migration with hundreds of thousands flocking to Europe‚Äôs shores. You want to study the root causes of this issue and decide to do some research."
  },
  {
    "objectID": "01_SLR.html#question-1.1-2",
    "href": "01_SLR.html#question-1.1-2",
    "title": "SLR",
    "section": "Question 1.1",
    "text": "Question 1.1\n\nWrite an equation that would allow you to test whether a country‚Äôs GDP has a linear influence in its number of asylum seekers.\nCan you say there is an exact linear relationship between both variables?\nCan you easily compute the population parameters for the regression model you presented before?\nWhat do you suggest then?\n\n\n\n\\(num_asy_i=\\beta_0+\\beta_1 GDP_i + u_i\\)\nIt is unlikely that if we get \\(\\beta_0\\) and \\(\\beta_1\\) we would obtain the exact number of asylum seekers. There might be many other factors influencing this.\nCan never get the population, then \\(\\beta_0\\) and \\(\\beta_1\\) are unknown.\nGet a sample, and estimate \\(\\beta_0\\) and \\(\\beta_1\\)."
  },
  {
    "objectID": "01_SLR.html#question-1.2-1",
    "href": "01_SLR.html#question-1.2-1",
    "title": "SLR",
    "section": "Question 1.2",
    "text": "Question 1.2\nYou move on with your endeavour and collect a set of cross-sectional data for 166 countries on their number of asylum seekers and GDP per capita in 1999 and estimate the following models by OLS:"
  },
  {
    "objectID": "01_SLR.html#question-1.2-2",
    "href": "01_SLR.html#question-1.2-2",
    "title": "SLR",
    "section": "Question 1.2",
    "text": "Question 1.2"
  },
  {
    "objectID": "01_SLR.html#question-1.2-3",
    "href": "01_SLR.html#question-1.2-3",
    "title": "SLR",
    "section": "Question 1.2",
    "text": "Question 1.2\n\nLevel-level: \\(\\widehat{asylum}=2,569-0.12 GDP\\) with \\(n=166\\) and \\(R^2=0.0447\\)\nLog-level : \\(\\widehat{log(asylum)}=5.64-0.0002 GDP\\) with \\(n=166\\) and \\(R^2=0.2585\\)\nLevel-log : \\(\\widehat{asylum}=7,804-731.97 log(GDP)\\) with \\(n=166\\) and \\(R^2=0.0368\\)\nLog-log : \\(\\widehat{log(asylum)}=17.56-1.65 log(GDP)\\) with \\(n=166\\) and \\(R^2=0.2673\\)"
  },
  {
    "objectID": "01_SLR.html#question-1.2-4",
    "href": "01_SLR.html#question-1.2-4",
    "title": "SLR",
    "section": "Question 1.2",
    "text": "Question 1.2\n\nInterpret the coefficient on the regressor of each model.\n\n\n\nIf \\(GDP_pc\\) increases by $1, the number of asylum seekers decreases by 0.12 persons, on average, ceteris paribus.\nIf \\(GDP_pc\\) increases by $1, the number of asylum seekers decreases by 0.02%, on average, ceteris paribus.\nIf \\(GDP_pc\\) increases by 1%, the number of asylum seekers decreases by 7.32 persons, on average, ceteris paribus.\nIf \\(GDP_pc\\) increases by 1%, the number of asylum seekers decreases by 1.64%, on average, ceteris paribus."
  },
  {
    "objectID": "01_SLR.html#question-1.5-1",
    "href": "01_SLR.html#question-1.5-1",
    "title": "SLR",
    "section": "Question 1.5",
    "text": "Question 1.5\n\nShow that \\(E[u|inc] = 0\\), so that the key zero conditional mean assumption (SLR.4) is satisfied. [Hint: If \\(e\\) is independent of \\(inc\\), then \\(E[e|inc] = E[e]\\)].\nSuppose now that \\(E[u] = \\alpha \\neq 0\\). Show that the model can always be rewritten with the same slope, but a new intercept and error, where the new error has a zero expected value.\nShow that \\(V[u|inc] = \\sigma_e^2 inc\\), so that homoskedasticity (SLR.5) is violated. In particular, the variance of \\(sav\\) increases with \\(inc\\). [Hint: \\(V[e|inc] = V[e]\\), if \\(e\\) and \\(inc\\) are independent.]\n\n\n\n\\(E[u|inc]=E[e\\sqrt{inc}|inc]=\\sqrt{inc}E[e|inc]=\\sqrt{inc}0=0\\)\n\\(sav=\\beta_0+\\beta_1 inc + u = \\beta_0+\\beta_1 inc + u +\\alpha-\\alpha =(\\beta_0+\\alpha)+\\beta_1inc +(u-\\alpha)\\)\n\\(V[u|inc]=V[e\\sqrt{inc}|inc]=inc V[e|inc]=inc V[e]=inc \\sigma_e^2\\)"
  },
  {
    "objectID": "01_SLR.html#question-1.5-2",
    "href": "01_SLR.html#question-1.5-2",
    "title": "SLR",
    "section": "Question 1.5",
    "text": "Question 1.5\n\nProvide a discussion that supports the assumption that the variance of savings increases with family income. Do you find evidence of that in the graph below?\n\n\n\n\n\nPoor households (do not have enough money to make a living and just buy necessities) vs Richer households (have more discretion)."
  },
  {
    "objectID": "01_SLR.html#question-1.9",
    "href": "01_SLR.html#question-1.9",
    "title": "SLR",
    "section": "Question 1.9",
    "text": "Question 1.9\nConsider the standard SLR model \\(y=\\beta_0+\\beta_1 x + u\\) under all the GM assumptions. Let \\(\\tilde{\\beta}_1\\) be the estimator of \\(\\beta_1\\) obtained by assuming \\(\\beta_0=0\\).\n\nCompute \\(\\tilde{\\beta}_1\\) by OLS.\nCheck if and when \\(\\tilde{\\beta}_1\\) is unbiased.\nFind \\(V[\\tilde{\\beta}_1]\\) (always conditional on \\(X\\))\nShow that \\(V[\\tilde{\\beta}_1]\\leq V[\\hat{\\beta}_1]\\) where \\(\\hat{\\beta}\\) is the traditional estimator for \\(\\beta_1\\) under OLS.\nDiscuss the tradeoff between \\(\\tilde{\\beta}_1\\) and \\(\\hat{\\beta}_1\\) regarding bias and variance."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#roadmap",
    "href": "R_SLR_Lecture_Slides.html#roadmap",
    "title": "Simple Linear Regression",
    "section": "Roadmap",
    "text": "Roadmap\nIn the previous lectures we covered the algebra of Simple Linear Regression on the whiteboard.\nToday we bring it to life with data, code, and simulations.\n\nKaggle\nQuick R warm-up\nExploring a real dataset\nThe OLS estimator ‚Äî by hand\nUsing lm() ‚Äî R does it for us\nInference ‚Äî by hand vs.¬†summary()\n‚≠ê Simulation: \\(\\hat{\\beta}_1\\) is a random variable\nWhat affects precision?\nResidual diagnostics\nConfidence and prediction bands"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#vectors-the-building-block-of-r",
    "href": "R_SLR_Lecture_Slides.html#vectors-the-building-block-of-r",
    "title": "Simple Linear Regression",
    "section": "Vectors: the building block of R",
    "text": "Vectors: the building block of R\n\n# A vector is the most basic data structure in R\nx &lt;- c(1, 2, 3, 4, 5)\nx\n\n[1] 1 2 3 4 5\n\n\n\n\n# R is vectorized: operations apply element by element\nx^2\n\n[1]  1  4  9 16 25\n\n\n\n\n\n# Summary statistics\ncat(\"Mean:\", mean(x), \" | Variance:\", var(x), \" | Std Dev:\", sd(x), \"\\n\")\n\nMean: 3  | Variance: 2.5  | Std Dev: 1.581139 \n\n\n\n\n\n\n\n\nNote\n\n\nvar() uses \\(n-1\\) (sample variance), not \\(n\\)."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#quick-plotting",
    "href": "R_SLR_Lecture_Slides.html#quick-plotting",
    "title": "Simple Linear Regression",
    "section": "Quick plotting",
    "text": "Quick plotting\n\ny &lt;- c(2.1, 3.9, 6.2, 7.8, 10.1)\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     main = \"My First Scatter Plot\", xlab = \"x\", ylab = \"y\")\ngrid()"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#the-mtcars-dataset",
    "href": "R_SLR_Lecture_Slides.html#the-mtcars-dataset",
    "title": "Simple Linear Regression",
    "section": "The mtcars dataset",
    "text": "The mtcars dataset\nBuilt into R: data on 32 automobiles (1974 Motor Trend magazine).\nOur question: How does the weight of a car affect its fuel consumption?\n\\[\\text{mpg}_i = \\beta_0 + \\beta_1 \\cdot \\text{wt}_i + u_i\\]\n\ndata(mtcars)\nhead(mtcars[, c(\"mpg\", \"wt\", \"hp\", \"cyl\")], 8)\n\n                   mpg    wt  hp cyl\nMazda RX4         21.0 2.620 110   6\nMazda RX4 Wag     21.0 2.875 110   6\nDatsun 710        22.8 2.320  93   4\nHornet 4 Drive    21.4 3.215 110   6\nHornet Sportabout 18.7 3.440 175   8\nValiant           18.1 3.460 105   6\nDuster 360        14.3 3.570 245   8\nMerc 240D         24.4 3.190  62   4"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#descriptive-statistics",
    "href": "R_SLR_Lecture_Slides.html#descriptive-statistics",
    "title": "Simple Linear Regression",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\ny &lt;- mtcars$mpg   # miles per gallon (dependent variable)\nx &lt;- mtcars$wt    # weight in 1000 lbs (independent variable)\n\n\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  10.40   15.43   19.20   20.09   22.80   33.90 \n\n\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.513   2.581   3.325   3.217   3.610   5.424"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#visual-exploration",
    "href": "R_SLR_Lecture_Slides.html#visual-exploration",
    "title": "Simple Linear Regression",
    "section": "Visual exploration",
    "text": "Visual exploration\n\npar(mfrow = c(1, 3), mar = c(4, 4, 3, 1))\n\nhist(y, breaks = 10, col = \"steelblue\", border = \"white\",\n     main = \"Distribution of mpg\", xlab = \"Miles per Gallon\")\n\nhist(x, breaks = 10, col = \"coral\", border = \"white\",\n     main = \"Distribution of Weight\", xlab = \"Weight (1000 lbs)\")\n\nplot(x, y, pch = 19, col = \"steelblue\",\n     main = \"mpg vs Weight\", xlab = \"Weight (1000 lbs)\", ylab = \"mpg\")\ngrid()"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#correlation",
    "href": "R_SLR_Lecture_Slides.html#correlation",
    "title": "Simple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\n\ncor(x, y)\n\n[1] -0.8676594\n\n\nStrong negative linear relationship. Heavier cars tend to have lower fuel efficiency.\n\n\nQuestion: Looking at the scatter plot, does a straight line seem reasonable? What sign should the slope have?"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#the-formulas-from-the-whiteboard",
    "href": "R_SLR_Lecture_Slides.html#the-formulas-from-the-whiteboard",
    "title": "Simple Linear Regression",
    "section": "The formulas (from the whiteboard)",
    "text": "The formulas (from the whiteboard)\n\\[\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\\]\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\nLet‚Äôs compute this step by step."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#step-1-means",
    "href": "R_SLR_Lecture_Slides.html#step-1-means",
    "title": "Simple Linear Regression",
    "section": "Step 1: Means",
    "text": "Step 1: Means\n\nn &lt;- length(x)\nx_bar &lt;- mean(x)\ny_bar &lt;- mean(y)\ncat(\"n =\", n, \"\\n\")\n\nn = 32 \n\ncat(\"xÃÑ =\", x_bar, \"\\n\")\n\nxÃÑ = 3.21725 \n\ncat(\"»≥ =\", y_bar, \"\\n\")\n\n»≥ = 20.09062"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#step-2-deviations-from-the-mean",
    "href": "R_SLR_Lecture_Slides.html#step-2-deviations-from-the-mean",
    "title": "Simple Linear Regression",
    "section": "Step 2: Deviations from the mean",
    "text": "Step 2: Deviations from the mean\n\ndx &lt;- x - x_bar\ndy &lt;- y - y_bar\n\n# First 6 observations\ndata.frame(\n  car = rownames(mtcars)[1:6],\n  x = x[1:6], y = y[1:6],\n  dx = round(dx[1:6], 3), dy = round(dy[1:6], 3),\n  dx_times_dy = round(dx[1:6] * dy[1:6], 3)\n)\n\n                car     x    y     dx     dy dx_times_dy\n1         Mazda RX4 2.620 21.0 -0.597  0.909      -0.543\n2     Mazda RX4 Wag 2.875 21.0 -0.342  0.909      -0.311\n3        Datsun 710 2.320 22.8 -0.897  2.709      -2.431\n4    Hornet 4 Drive 3.215 21.4 -0.002  1.309      -0.003\n5 Hornet Sportabout 3.440 18.7  0.223 -1.391      -0.310\n6           Valiant 3.460 18.1  0.243 -1.991      -0.483"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#step-3-s_xy-and-s_xx",
    "href": "R_SLR_Lecture_Slides.html#step-3-s_xy-and-s_xx",
    "title": "Simple Linear Regression",
    "section": "Step 3: \\(S_{xy}\\) and \\(S_{xx}\\)",
    "text": "Step 3: \\(S_{xy}\\) and \\(S_{xx}\\)\n\nSxy &lt;- sum(dx * dy)\nSxx &lt;- sum(dx^2)\ncat(\"Sxy =\", Sxy, \"\\n\")\n\nSxy = -158.6172 \n\ncat(\"Sxx =\", Sxx, \"\\n\")\n\nSxx = 29.67875"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#step-4-the-estimates",
    "href": "R_SLR_Lecture_Slides.html#step-4-the-estimates",
    "title": "Simple Linear Regression",
    "section": "Step 4: The estimates",
    "text": "Step 4: The estimates\n\nbeta1_hat &lt;- Sxy / Sxx\nbeta0_hat &lt;- y_bar - beta1_hat * x_bar\n\ncat(\"Œ≤ÃÇ‚ÇÄ =\", round(beta0_hat, 4), \"\\n\")\n\nŒ≤ÃÇ‚ÇÄ = 37.2851 \n\ncat(\"Œ≤ÃÇ‚ÇÅ =\", round(beta1_hat, 4), \"\\n\")\n\nŒ≤ÃÇ‚ÇÅ = -5.3445 \n\ncat(\"\\nFor each additional 1000 lbs of weight,\\n\")\n\n\nFor each additional 1000 lbs of weight,\n\ncat(\"mpg decreases by\", round(abs(beta1_hat), 2), \"miles per gallon\\n\")\n\nmpg decreases by 5.34 miles per gallon\n\ncat(\"on average, ceteris paribus.\")\n\non average, ceteris paribus."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#the-fitted-line",
    "href": "R_SLR_Lecture_Slides.html#the-fitted-line",
    "title": "Simple Linear Regression",
    "section": "The fitted line",
    "text": "The fitted line\n\n\nCode\nplot(x, y, pch = 19, col = \"steelblue\", cex = 1.3,\n     main = \"OLS Regression Line (computed by hand)\",\n     xlab = \"Weight (1000 lbs)\", ylab = \"Miles per Gallon\")\n\nabline(a = beta0_hat, b = beta1_hat, col = \"red\", lwd = 2.5)\n\npoints(x_bar, y_bar, pch = 4, col = \"red\", cex = 2.5, lwd = 3)\n\ntext(x_bar + 0.15, y_bar + 1.2,\n     expression(paste(\"(\", bar(x), \", \", bar(y), \")\")), col = \"red\", cex = 1.1)\n\ngrid()\n\nlegend(\"topright\",\n       legend = c(paste0(\"≈∑ = \", round(beta0_hat, 2), \" + (\",\n                         round(beta1_hat, 2), \") √ó wt\"), \"Point of means\"),\n       col = c(\"red\", \"red\"), lty = c(1, NA), pch = c(NA, 4),\n       lwd = c(2.5, 3), bty = \"n\")"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#important-property",
    "href": "R_SLR_Lecture_Slides.html#important-property",
    "title": "Simple Linear Regression",
    "section": "Important property",
    "text": "Important property\nThe OLS regression line always passes through the point of means \\((\\bar{x}, \\bar{y})\\).\n\nWhy? From the formula:\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\nSubstitute \\(x = \\bar{x}\\):\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x} = (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 \\bar{x} = \\bar{y}\\]"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#visualizing-the-residuals",
    "href": "R_SLR_Lecture_Slides.html#visualizing-the-residuals",
    "title": "Simple Linear Regression",
    "section": "Visualizing the residuals",
    "text": "Visualizing the residuals\n\n\nCode\ny_hat &lt;- beta0_hat + beta1_hat * x\nresid &lt;- y - y_hat\n\nplot(x, y, pch = 19, col = \"steelblue\", cex = 1.3,\n     main = \"Residuals: the vertical distances\",\n     xlab = \"Weight (1000 lbs)\", ylab = \"Miles per Gallon\")\nabline(a = beta0_hat, b = beta1_hat, col = \"red\", lwd = 2)\nsegments(x0 = x, y0 = y, x1 = x, y1 = y_hat, col = \"gray40\", lty = 2)\npoints(x, y_hat, pch = 1, col = \"red\", cex = 1)\ngrid()\nlegend(\"topright\",\n       legend = c(\"Observed (y)\", \"Fitted (≈∑)\", \"Residual (y ‚àí ≈∑)\"),\n       col = c(\"steelblue\", \"red\", \"gray40\"),\n       pch = c(19, 1, NA), lty = c(NA, NA, 2), bty = \"n\")"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#properties-of-residuals",
    "href": "R_SLR_Lecture_Slides.html#properties-of-residuals",
    "title": "Simple Linear Regression",
    "section": "Properties of residuals",
    "text": "Properties of residuals\n\ncat(\"Sum of residuals:\", round(sum(resid), 10), \"\\n\")\n\nSum of residuals: 0 \n\ncat(\"Cor(x, residuals):\", round(cor(x, resid), 10), \"\\n\")\n\nCor(x, residuals): 0 \n\ncat(\"Mean of fitted values:\", round(mean(y_hat), 4), \"\\n\")\n\nMean of fitted values: 20.0906 \n\ncat(\"Mean of y:            \", round(mean(y), 4), \"\\n\")\n\nMean of y:             20.0906 \n\ncat(\"  ‚Üí They are equal!\\n\")\n\n  ‚Üí They are equal!"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#fitting-and-verifying",
    "href": "R_SLR_Lecture_Slides.html#fitting-and-verifying",
    "title": "Simple Linear Regression",
    "section": "Fitting and verifying",
    "text": "Fitting and verifying\n\nmodel &lt;- lm(mpg ~ wt, data = mtcars)\ncoef(model)\n\n(Intercept)          wt \n  37.285126   -5.344472 \n\n\n\n\ncat(\"By hand:  Œ≤ÃÇ‚ÇÄ =\", round(beta0_hat, 4), \"  Œ≤ÃÇ‚ÇÅ =\", round(beta1_hat, 4), \"\\n\")\n\nBy hand:  Œ≤ÃÇ‚ÇÄ = 37.2851   Œ≤ÃÇ‚ÇÅ = -5.3445 \n\ncat(\"lm():     Œ≤ÃÇ‚ÇÄ =\", round(coef(model)[1], 4),\n    \"  Œ≤ÃÇ‚ÇÅ =\", round(coef(model)[2], 4), \"\\n\")\n\nlm():     Œ≤ÃÇ‚ÇÄ = 37.2851   Œ≤ÃÇ‚ÇÅ = -5.3445 \n\ncat(\"\\n‚úì They match!\")\n\n\n‚úì They match!"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#step-1-estimate-sigma2",
    "href": "R_SLR_Lecture_Slides.html#step-1-estimate-sigma2",
    "title": "Simple Linear Regression",
    "section": "Step 1: Estimate \\(\\sigma^2\\)",
    "text": "Step 1: Estimate \\(\\sigma^2\\)\n\\[\\hat{\\sigma}^2 = \\frac{\\text{SSR}}{n - 2} = \\frac{\\sum_{i=1}^{n} \\hat{u}_i^2}{n - 2}\\]\n\nSSR &lt;- sum(resid^2)\ndf &lt;- n - 2\nsigma2_hat &lt;- SSR / df\nsigma_hat &lt;- sqrt(sigma2_hat)\n\ncat(\"SSR =\", round(SSR, 4), \"\\n\")\n\nSSR = 278.3219 \n\ncat(\"df  = n - 2 =\", df, \"\\n\")\n\ndf  = n - 2 = 30 \n\ncat(\"œÉÃÇ¬≤  =\", round(sigma2_hat, 4), \"\\n\")\n\nœÉÃÇ¬≤  = 9.2774 \n\ncat(\"œÉÃÇ   =\", round(sigma_hat, 4), \" (Residual Standard Error)\\n\")\n\nœÉÃÇ   = 3.0459  (Residual Standard Error)"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#step-2-standard-errors",
    "href": "R_SLR_Lecture_Slides.html#step-2-standard-errors",
    "title": "Simple Linear Regression",
    "section": "Step 2: Standard errors",
    "text": "Step 2: Standard errors\n\\[\\text{se}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{\\sqrt{S_{xx}}}\\]\n\nse_beta1 &lt;- sigma_hat / sqrt(Sxx)\n\ncat(\"se(Œ≤ÃÇ‚ÇÅ) =\", round(se_beta1, 4), \"\\n\")\n\nse(Œ≤ÃÇ‚ÇÅ) = 0.5591"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#step-3-t-statistics-and-p-values",
    "href": "R_SLR_Lecture_Slides.html#step-3-t-statistics-and-p-values",
    "title": "Simple Linear Regression",
    "section": "Step 3: t-statistics and p-values",
    "text": "Step 3: t-statistics and p-values\nUnder \\(H_0: \\beta_j = 0\\): \\(\\quad t = \\frac{\\hat{\\beta}_j}{\\text{se}(\\hat{\\beta}_j)} \\sim t_{n-2}\\)\n\nt_beta1 &lt;- beta1_hat / se_beta1\np_beta1 &lt;- 2 * pt(abs(t_beta1), df = df, lower.tail = FALSE)\n\ncat(sprintf(\"         Estimate   Std.Error   t-value    p-value\\n\"))\n\n         Estimate   Std.Error   t-value    p-value\n\ncat(sprintf(\"Œ≤ÃÇ‚ÇÅ  %10.4f  %10.4f  %8.3f   %.2e\\n\",\n            beta1_hat, se_beta1, t_beta1, p_beta1))\n\nŒ≤ÃÇ‚ÇÅ     -5.3445      0.5591    -9.559   1.29e-10"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#visualizing-the-t-test-for-beta_1",
    "href": "R_SLR_Lecture_Slides.html#visualizing-the-t-test-for-beta_1",
    "title": "Simple Linear Regression",
    "section": "Visualizing the t-test for \\(\\beta_1\\)",
    "text": "Visualizing the t-test for \\(\\beta_1\\)\n\n\nCode\nalpha &lt;- 0.05\nt_crit &lt;- qt(1 - alpha/2, df = df)\n\ncurve(dt(x, df = df), from = -15, to = 15, n = 500, lwd = 2, col = \"steelblue\",\n      main = expression(paste(\"t-test for \", H[0], \": \", beta[1], \" = 0\")),\n      xlab = \"t\", ylab = \"Density\")\n\nx_left &lt;- seq(-15, -t_crit, length.out = 200)\nx_right &lt;- seq(t_crit, 15, length.out = 200)\npolygon(c(x_left, rev(x_left)), c(dt(x_left, df), rep(0, 200)),\n        col = rgb(1, 0, 0, 0.3), border = NA)\npolygon(c(x_right, rev(x_right)), c(dt(x_right, df), rep(0, 200)),\n        col = rgb(1, 0, 0, 0.3), border = NA)\n\nabline(v = t_beta1, col = \"red\", lwd = 2.5, lty = 2)\ntext(t_beta1 + 1.5, 0.15, paste0(\"t = \", round(t_beta1, 2)),\n     col = \"red\", cex = 1.2, font = 2)\nabline(v = c(-t_crit, t_crit), col = \"gray40\", lty = 3)\nlegend(\"topleft\",\n       legend = c(\"t(30) distribution\", \"Rejection region (Œ± = 0.05)\", \"Our t-statistic\"),\n       col = c(\"steelblue\", rgb(1,0,0,0.3), \"red\"),\n       lwd = c(2, 8, 2.5), lty = c(1, 1, 2), bty = \"n\")"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#confidence-intervals-by-hand",
    "href": "R_SLR_Lecture_Slides.html#confidence-intervals-by-hand",
    "title": "Simple Linear Regression",
    "section": "Confidence intervals (by hand)",
    "text": "Confidence intervals (by hand)\n\\[\\hat{\\beta}_j \\pm t_{\\alpha/2, \\, n-2} \\cdot \\text{se}(\\hat{\\beta}_j)\\]\n\ncat(\"Critical value t_{0.025, 30} =\", round(t_crit, 4), \"\\n\\n\")\n\nCritical value t_{0.025, 30} = 2.0423 \n\nci_beta1 &lt;- beta1_hat + c(-1, 1) * t_crit * se_beta1\n\ncat(\"95% CI for Œ≤‚ÇÅ: [\", round(ci_beta1[1], 4), \",\", round(ci_beta1[2], 4), \"]\\n\")\n\n95% CI for Œ≤‚ÇÅ: [ -6.4863 , -4.2026 ]\n\ncat(\"\\nNote: 0 is NOT inside the CI for Œ≤‚ÇÅ ‚Üí we reject H‚ÇÄ: Œ≤‚ÇÅ = 0\")\n\n\nNote: 0 is NOT inside the CI for Œ≤‚ÇÅ ‚Üí we reject H‚ÇÄ: Œ≤‚ÇÅ = 0"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#comparing-with-summary",
    "href": "R_SLR_Lecture_Slides.html#comparing-with-summary",
    "title": "Simple Linear Regression",
    "section": "Comparing with summary()",
    "text": "Comparing with summary()\n\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#confidence-intervals-automatic",
    "href": "R_SLR_Lecture_Slides.html#confidence-intervals-automatic",
    "title": "Simple Linear Regression",
    "section": "Confidence intervals: automatic",
    "text": "Confidence intervals: automatic\n\nconfint(model)\n\n                2.5 %    97.5 %\n(Intercept) 33.450500 41.119753\nwt          -6.486308 -4.202635\n\n\n\nEverything matches our hand calculations."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#r2-decomposition",
    "href": "R_SLR_Lecture_Slides.html#r2-decomposition",
    "title": "Simple Linear Regression",
    "section": "\\(R^2\\) decomposition",
    "text": "\\(R^2\\) decomposition\n\nSST &lt;- sum((y - y_bar)^2)\nSSE &lt;- sum((y_hat - y_bar)^2)\nR2 &lt;- 1 - SSR / SST\ncat(\"R¬≤ =\", round(R2, 4), \"\\n\")\n\nR¬≤ = 0.7528 \n\ncat(\"Weight explains\", round(R2 * 100, 1), \"% of the variation in mpg.\\n\")\n\nWeight explains 75.3 % of the variation in mpg."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#the-key-insight",
    "href": "R_SLR_Lecture_Slides.html#the-key-insight",
    "title": "Simple Linear Regression",
    "section": "The key insight",
    "text": "The key insight\nWe said \\(\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}\\) depends on the sample.\n\nIf we took a different sample from the same population, we would get a different \\(\\hat{\\beta}_1\\).\n\n\nLet‚Äôs simulate this!\n\nTrue model: \\(y = 5 + 3x + u\\), where \\(u \\sim N(0, 16)\\)\nDraw 1000 samples of size \\(n = 50\\)\nEstimate \\(\\hat{\\beta}_1\\) each time"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#running-the-simulation",
    "href": "R_SLR_Lecture_Slides.html#running-the-simulation",
    "title": "Simple Linear Regression",
    "section": "Running the simulation",
    "text": "Running the simulation\n\nbeta0_true &lt;- 5;  beta1_true &lt;- 3;  sigma_true &lt;- 4\nset.seed(42)\n\nn_sim &lt;- 1000;  n_obs &lt;- 50\nx_sim &lt;- runif(n_obs, min = 0, max = 10)\n\nbeta1_estimates &lt;- numeric(n_sim)\nbeta0_estimates &lt;- numeric(n_sim)\n\nfor (i in 1:n_sim) {\n  u &lt;- rnorm(n_obs, mean = 0, sd = sigma_true)\n  y_sim &lt;- beta0_true + beta1_true * x_sim + u\n  fit &lt;- lm(y_sim ~ x_sim)\n  beta0_estimates[i] &lt;- coef(fit)[1]\n  beta1_estimates[i] &lt;- coef(fit)[2]\n}\n\ncat(\"Done:\", n_sim, \"regressions, each with n =\", n_obs, \"observations.\\n\")\n\nDone: 1000 regressions, each with n = 50 observations."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#different-samples-20-different-lines",
    "href": "R_SLR_Lecture_Slides.html#different-samples-20-different-lines",
    "title": "Simple Linear Regression",
    "section": "20 different samples ‚Üí 20 different lines",
    "text": "20 different samples ‚Üí 20 different lines\n\n\nCode\nplot(NULL, xlim = c(0, 10), ylim = c(-5, 45),\n     main = \"20 Different Samples ‚Üí 20 Different Regression Lines\",\n     xlab = \"x\", ylab = \"y\")\nfor (i in 1:20) {\n  abline(a = beta0_estimates[i], b = beta1_estimates[i],\n         col = rgb(0.3, 0.3, 0.8, 0.3), lwd = 1.5)\n}\nabline(a = beta0_true, b = beta1_true, col = \"red\", lwd = 3)\nlegend(\"topleft\",\n       legend = c(\"True line: y = 5 + 3x\", \"Estimated lines (one per sample)\"),\n       col = c(\"red\", rgb(0.3, 0.3, 0.8, 0.5)), lwd = c(3, 2), bty = \"n\")\ngrid()"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#the-sampling-distribution-of-hatbeta_1",
    "href": "R_SLR_Lecture_Slides.html#the-sampling-distribution-of-hatbeta_1",
    "title": "Simple Linear Regression",
    "section": "The sampling distribution of \\(\\hat{\\beta}_1\\)",
    "text": "The sampling distribution of \\(\\hat{\\beta}_1\\)\n\n\nCode\nse_theory &lt;- sigma_true / sqrt(sum((x_sim - mean(x_sim))^2))\n\nhist(beta1_estimates, breaks = 40, freq = FALSE,\n     col = \"steelblue\", border = \"white\",\n     main = expression(paste(\"Sampling Distribution of \", hat(beta)[1],\n                             \" (1000 simulations)\")),\n     xlab = expression(hat(beta)[1]),\n     xlim = c(beta1_true - 1.5, beta1_true + 1.5))\nabline(v = beta1_true, col = \"red\", lwd = 3)\nabline(v = mean(beta1_estimates), col = \"orange\", lwd = 2, lty = 2)\ncurve(dnorm(x, mean = beta1_true, sd = se_theory), add = TRUE,\n      col = \"red\", lwd = 2, lty = 2)\nlegend(\"topright\",\n       legend = c(paste0(\"True Œ≤‚ÇÅ = \", beta1_true),\n                  paste0(\"Mean(Œ≤ÃÇ‚ÇÅ) = \", round(mean(beta1_estimates), 4)),\n                  \"Theoretical N(Œ≤‚ÇÅ, œÉ¬≤/Sxx)\"),\n       col = c(\"red\", \"orange\", \"red\"),\n       lwd = c(3, 2, 2), lty = c(1, 2, 2), bty = \"n\")"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#unbiasedness-confirmed",
    "href": "R_SLR_Lecture_Slides.html#unbiasedness-confirmed",
    "title": "Simple Linear Regression",
    "section": "Unbiasedness confirmed",
    "text": "Unbiasedness confirmed\n\ncat(\"True Œ≤‚ÇÅ              =\", beta1_true, \"\\n\")\n\nTrue Œ≤‚ÇÅ              = 3 \n\ncat(\"Mean of Œ≤ÃÇ‚ÇÅ           =\", round(mean(beta1_estimates), 4), \"\\n\")\n\nMean of Œ≤ÃÇ‚ÇÅ           = 2.9945 \n\ncat(\"  ‚Üí UNBIASED: E[Œ≤ÃÇ‚ÇÅ] ‚âà Œ≤‚ÇÅ ‚úì\\n\\n\")\n\n  ‚Üí UNBIASED: E[Œ≤ÃÇ‚ÇÅ] ‚âà Œ≤‚ÇÅ ‚úì\n\ncat(\"Std Dev of Œ≤ÃÇ‚ÇÅ (simulated) =\", round(sd(beta1_estimates), 4), \"\\n\")\n\nStd Dev of Œ≤ÃÇ‚ÇÅ (simulated) = 0.1932 \n\ncat(\"Std Dev of Œ≤ÃÇ‚ÇÅ (theory)    =\", round(se_theory, 4), \"\\n\")\n\nStd Dev of Œ≤ÃÇ‚ÇÅ (theory)    = 0.1882 \n\ncat(\"  ‚Üí MATCHES: se(Œ≤ÃÇ‚ÇÅ) = œÉ/‚àöSxx ‚úì\\n\\n\")\n\n  ‚Üí MATCHES: se(Œ≤ÃÇ‚ÇÅ) = œÉ/‚àöSxx ‚úì\n\ncat(\"True Œ≤‚ÇÄ              =\", beta0_true, \"\\n\")\n\nTrue Œ≤‚ÇÄ              = 5 \n\ncat(\"Mean of Œ≤ÃÇ‚ÇÄ           =\", round(mean(beta0_estimates), 4), \"\\n\")\n\nMean of Œ≤ÃÇ‚ÇÄ           = 5.022 \n\ncat(\"  ‚Üí UNBIASED: E[Œ≤ÃÇ‚ÇÄ] ‚âà Œ≤‚ÇÄ ‚úì\\n\")\n\n  ‚Üí UNBIASED: E[Œ≤ÃÇ‚ÇÄ] ‚âà Œ≤‚ÇÄ ‚úì"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#key-takeaways-from-the-simulation",
    "href": "R_SLR_Lecture_Slides.html#key-takeaways-from-the-simulation",
    "title": "Simple Linear Regression",
    "section": "Key takeaways from the simulation",
    "text": "Key takeaways from the simulation\n\nEach sample gives a different \\(\\hat{\\beta}_1\\) ‚Äî the estimator is a random variable\nOn average, \\(\\hat{\\beta}_1\\) equals the true \\(\\beta_1\\) ‚Üí unbiasedness\nThe spread of \\(\\hat{\\beta}_1\\) is captured by the standard error\nThe histogram matches the theoretical normal ‚Üí this justifies our t-tests and CIs"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#the-formula-tells-us-everything",
    "href": "R_SLR_Lecture_Slides.html#the-formula-tells-us-everything",
    "title": "Simple Linear Regression",
    "section": "The formula tells us everything",
    "text": "The formula tells us everything\n\\[\\text{se}(\\hat{\\beta}_1) = \\frac{\\sigma}{\\sqrt{S_{xx}}}\\]\nThree things affect precision:\n\nSample size \\(n\\) ‚Üí more data ‚Üí larger \\(S_{xx}\\) ‚Üí smaller se\nSpread of \\(x\\) ‚Üí more variation ‚Üí larger \\(S_{xx}\\) ‚Üí smaller se\nError variance \\(\\sigma^2\\) ‚Üí noisier data ‚Üí larger se"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#effect-of-sample-size",
    "href": "R_SLR_Lecture_Slides.html#effect-of-sample-size",
    "title": "Simple Linear Regression",
    "section": "Effect of sample size",
    "text": "Effect of sample size\n\n\nCode\nset.seed(123)\nsample_sizes &lt;- c(20, 50, 100, 500)\nn_reps &lt;- 1000\n\npar(mfrow = c(2, 2), mar = c(4, 4, 3, 1))\nfor (nn in sample_sizes) {\n  x_temp &lt;- runif(nn, 0, 10)\n  b1_vec &lt;- numeric(n_reps)\n  for (j in 1:n_reps) {\n    u_temp &lt;- rnorm(nn, 0, sigma_true)\n    y_temp &lt;- beta0_true + beta1_true * x_temp + u_temp\n    b1_vec[j] &lt;- coef(lm(y_temp ~ x_temp))[2]\n  }\n  hist(b1_vec, breaks = 35, freq = FALSE, col = \"steelblue\", border = \"white\",\n       main = paste0(\"n = \", nn, \"  (sd = \", round(sd(b1_vec), 3), \")\"),\n       xlab = expression(hat(beta)[1]), xlim = c(1.5, 4.5))\n  abline(v = beta1_true, col = \"red\", lwd = 2)\n}"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#effect-of-noise-level-sigma",
    "href": "R_SLR_Lecture_Slides.html#effect-of-noise-level-sigma",
    "title": "Simple Linear Regression",
    "section": "Effect of noise level \\(\\sigma\\)",
    "text": "Effect of noise level \\(\\sigma\\)\n\n\nCode\nset.seed(456)\nsigmas &lt;- c(1, 4, 8, 16)\nnn_fixed &lt;- 50;  x_temp &lt;- runif(nn_fixed, 0, 10)\n\npar(mfrow = c(2, 2), mar = c(4, 4, 3, 1))\nfor (ss in sigmas) {\n  b1_vec &lt;- numeric(n_reps)\n  for (j in 1:n_reps) {\n    u_temp &lt;- rnorm(nn_fixed, 0, ss)\n    y_temp &lt;- beta0_true + beta1_true * x_temp + u_temp\n    b1_vec[j] &lt;- coef(lm(y_temp ~ x_temp))[2]\n  }\n  hist(b1_vec, breaks = 35, freq = FALSE, col = \"coral\", border = \"white\",\n       main = bquote(sigma == .(ss) ~ \" (sd = \" * .(round(sd(b1_vec), 3)) * \")\"),\n       xlab = expression(hat(beta)[1]), xlim = c(-1, 7))\n  abline(v = beta1_true, col = \"red\", lwd = 2)\n}"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#observation",
    "href": "R_SLR_Lecture_Slides.html#observation",
    "title": "Simple Linear Regression",
    "section": "Observation",
    "text": "Observation\nMore data ‚Üí tighter distribution.\nMore noise ‚Üí wider distribution.\nIn both cases, the estimator remains centered on the true value (unbiased)."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#different-subsamples-different-estimates",
    "href": "R_SLR_Lecture_Slides.html#different-subsamples-different-estimates",
    "title": "Simple Linear Regression",
    "section": "Different subsamples, different estimates",
    "text": "Different subsamples, different estimates\n\n\nCode\nset.seed(2024)\nplot(mtcars$wt, mtcars$mpg, pch = 19, col = \"gray70\", cex = 1.2,\n     main = \"8 Random Subsamples (n = 20) from mtcars\",\n     xlab = \"Weight (1000 lbs)\", ylab = \"Miles per Gallon\")\n\ncolors &lt;- c(\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\n            \"#FF7F00\", \"#A65628\", \"#F781BF\", \"#999999\")\nsub_betas &lt;- numeric(8)\nfor (i in 1:8) {\n  idx &lt;- sample(1:32, size = 20, replace = FALSE)\n  fit_sub &lt;- lm(mpg ~ wt, data = mtcars[idx, ])\n  abline(fit_sub, col = colors[i], lwd = 2)\n  sub_betas[i] &lt;- coef(fit_sub)[2]\n}\nabline(model, col = \"black\", lwd = 3, lty = 2)\nlegend(\"topright\",\n       legend = c(\"Full sample (n=32)\", \"Subsample lines (n=20)\"),\n       col = c(\"black\", \"steelblue\"), lwd = c(3, 2), lty = c(2, 1), bty = \"n\")\ngrid()"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#four-diagnostic-plots",
    "href": "R_SLR_Lecture_Slides.html#four-diagnostic-plots",
    "title": "Simple Linear Regression",
    "section": "Four diagnostic plots",
    "text": "Four diagnostic plots\n\n\nCode\npar(mfrow = c(2, 2), mar = c(4, 4, 3, 1))\n\nplot(fitted(model), residuals(model), pch = 19, col = \"steelblue\",\n     main = \"Residuals vs Fitted\", xlab = \"Fitted Values (≈∑)\", ylab = \"Residuals (√ª)\")\nabline(h = 0, col = \"red\", lwd = 2, lty = 2); grid()\n\nhist(residuals(model), breaks = 10, freq = FALSE,\n     col = \"steelblue\", border = \"white\",\n     main = \"Distribution of Residuals\", xlab = \"Residuals\")\ncurve(dnorm(x, mean = 0, sd = sigma_hat), add = TRUE, col = \"red\", lwd = 2)\n\nqqnorm(residuals(model), pch = 19, col = \"steelblue\", main = \"Normal Q-Q Plot\")\nqqline(residuals(model), col = \"red\", lwd = 2)\n\nplot(fitted(model), sqrt(abs(rstandard(model))), pch = 19, col = \"steelblue\",\n     main = \"Scale-Location\", xlab = \"Fitted Values\",\n     ylab = expression(sqrt(\"Standardized Residuals\")))\ngrid()"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#how-to-read-them",
    "href": "R_SLR_Lecture_Slides.html#how-to-read-them",
    "title": "Simple Linear Regression",
    "section": "How to read them",
    "text": "How to read them\n\nResiduals vs Fitted: Random scatter around 0 = good. Curve or funnel = problems.\nHistogram / Q-Q: Check approximate normality.\nScale-Location: Roughly constant spread = homoscedasticity."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#two-types-of-intervals",
    "href": "R_SLR_Lecture_Slides.html#two-types-of-intervals",
    "title": "Simple Linear Regression",
    "section": "Two types of intervals",
    "text": "Two types of intervals\n\n\nCode\nx_grid &lt;- data.frame(wt = seq(min(mtcars$wt) - 0.2, max(mtcars$wt) + 0.2,\n                               length.out = 200))\npred_ci &lt;- predict(model, newdata = x_grid, interval = \"confidence\")\npred_pi &lt;- predict(model, newdata = x_grid, interval = \"prediction\")\n\nplot(mtcars$wt, mtcars$mpg, pch = 19, col = \"steelblue\", cex = 1.2,\n     main = \"Confidence and Prediction Intervals\",\n     xlab = \"Weight (1000 lbs)\", ylab = \"Miles per Gallon\",\n     ylim = c(min(pred_pi[,\"lwr\"]) - 1, max(pred_pi[,\"upr\"]) + 1))\npolygon(c(x_grid$wt, rev(x_grid$wt)),\n        c(pred_pi[,\"lwr\"], rev(pred_pi[,\"upr\"])),\n        col = rgb(1, 0.6, 0.4, 0.2), border = NA)\npolygon(c(x_grid$wt, rev(x_grid$wt)),\n        c(pred_ci[,\"lwr\"], rev(pred_ci[,\"upr\"])),\n        col = rgb(0.3, 0.5, 0.8, 0.3), border = NA)\nlines(x_grid$wt, pred_ci[,\"fit\"], col = \"red\", lwd = 2.5)\npoints(mtcars$wt, mtcars$mpg, pch = 19, col = \"steelblue\", cex = 1.2)\nlegend(\"topright\",\n       legend = c(\"OLS line\", \"95% CI for E[Y|X]\", \"95% PI for new Y\"),\n       col = c(\"red\", rgb(0.3,0.5,0.8,0.5), rgb(1,0.6,0.4,0.4)),\n       lwd = c(2.5, 8, 8), bty = \"n\")\ngrid()"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#interpretation",
    "href": "R_SLR_Lecture_Slides.html#interpretation",
    "title": "Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\nConfidence interval (blue): Where the true regression line likely is. Narrower near \\(\\bar{x}\\).\nPrediction interval (orange): Where a new observation would fall. Always wider because it includes both line uncertainty and the noise \\(\\sigma^2\\)."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#does-95-confidence-really-mean-95",
    "href": "R_SLR_Lecture_Slides.html#does-95-confidence-really-mean-95",
    "title": "Simple Linear Regression",
    "section": "Does ‚Äú95% confidence‚Äù really mean 95%?",
    "text": "Does ‚Äú95% confidence‚Äù really mean 95%?\n\nset.seed(789)\nn_cover &lt;- 200;  nn_cov &lt;- 50\nx_cov &lt;- runif(nn_cov, 0, 10)\n\nci_lower &lt;- ci_upper &lt;- numeric(n_cover)\nfor (i in 1:n_cover) {\n  u_cov &lt;- rnorm(nn_cov, 0, sigma_true)\n  y_cov &lt;- beta0_true + beta1_true * x_cov + u_cov\n  fit_cov &lt;- lm(y_cov ~ x_cov)\n  ci &lt;- confint(fit_cov, \"x_cov\", level = 0.95)\n  ci_lower[i] &lt;- ci[1]; ci_upper[i] &lt;- ci[2]\n}\ncovers &lt;- (ci_lower &lt;= beta1_true) & (ci_upper &gt;= beta1_true)\ncat(\"Coverage:\", sum(covers), \"/\", n_cover, \"=\", mean(covers)*100, \"%\\n\")\n\nCoverage: 188 / 200 = 94 %"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#visualizing-50-confidence-intervals",
    "href": "R_SLR_Lecture_Slides.html#visualizing-50-confidence-intervals",
    "title": "Simple Linear Regression",
    "section": "Visualizing 50 confidence intervals",
    "text": "Visualizing 50 confidence intervals\n\nn_show &lt;- 50\nplot(NULL, xlim = c(min(ci_lower[1:n_show]) - 0.2, max(ci_upper[1:n_show]) + 0.2),\n     ylim = c(1, n_show),\n     main = paste0(\"50 Confidence Intervals for Œ≤‚ÇÅ  (coverage: \",\n                   round(mean(covers)*100, 1), \"%)\"),\n     xlab = expression(hat(beta)[1]), ylab = \"Simulation #\", yaxt = \"n\")\nabline(v = beta1_true, col = \"red\", lwd = 2)\nfor (i in 1:n_show) {\n  col_i &lt;- ifelse(covers[i], \"steelblue\", \"red\")\n  segments(ci_lower[i], i, ci_upper[i], i, col = col_i, lwd = 1.5)\n  points((ci_lower[i] + ci_upper[i])/2, i, pch = 19, col = col_i, cex = 0.6)\n}\nlegend(\"topleft\",\n       legend = c(paste0(\"Contains Œ≤‚ÇÅ = \", beta1_true), \"Misses Œ≤‚ÇÅ\"),\n       col = c(\"steelblue\", \"red\"), lwd = 2, bty = \"n\")"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#what-95-confidence-means",
    "href": "R_SLR_Lecture_Slides.html#what-95-confidence-means",
    "title": "Simple Linear Regression",
    "section": "What ‚Äú95% confidence‚Äù means",
    "text": "What ‚Äú95% confidence‚Äù means\nEach horizontal line = one CI from one sample.\n~95% (blue) contain the true \\(\\beta_1 = 3\\).\n~5% (red) miss ‚Äî by design.\nThis is what ‚Äú95% confidence‚Äù means: if we repeated the procedure many times, about 95% of the intervals would contain the true parameter."
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#key-r-functions",
    "href": "R_SLR_Lecture_Slides.html#key-r-functions",
    "title": "Simple Linear Regression",
    "section": "Key R functions",
    "text": "Key R functions\n\n\n\nTask\nCode\n\n\n\n\nFit model\nlm(y ~ x, data = df)\n\n\nCoefficients\ncoef(model)\n\n\nFull output\nsummary(model)\n\n\nCIs\nconfint(model)\n\n\nFitted values\nfitted(model)\n\n\nResiduals\nresiduals(model)\n\n\nPredict\npredict(model, newdata)\n\n\n\\(R^2\\)\nsummary(model)$r.squared"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#takeaways",
    "href": "R_SLR_Lecture_Slides.html#takeaways",
    "title": "Simple Linear Regression",
    "section": "Takeaways",
    "text": "Takeaways\n\nOLS minimizes the sum of squared residuals\n\\(\\hat{\\beta}_1\\) is a random variable ‚Äî it changes with the sample\nUnbiasedness: \\(E[\\hat{\\beta}_1] = \\beta_1\\) ‚Äî on average, OLS gets it right\nStandard errors decrease with more data and less noise\nt-tests and CIs follow from the sampling distribution\nR makes all of this easy ‚Äî but understanding what‚Äôs behind it is essential!"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#kaggle",
    "href": "R_SLR_Lecture_Slides.html#kaggle",
    "title": "Simple Linear Regression",
    "section": "Kaggle",
    "text": "Kaggle\nGo to this website, and login however you like, I log in with my google account.\n\nhttps://www.kaggle.com"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#kaggle-1",
    "href": "R_SLR_Lecture_Slides.html#kaggle-1",
    "title": "Simple Linear Regression",
    "section": "Kaggle",
    "text": "Kaggle\nClick in ‚ÄúCreate‚Äù\n\nAnd select ‚ÄúNotebook‚Äù"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#kaggle-2",
    "href": "R_SLR_Lecture_Slides.html#kaggle-2",
    "title": "Simple Linear Regression",
    "section": "Kaggle",
    "text": "Kaggle\nSometimes it defaults to Python, but it learns your preference (use R often and it will start creating R notebooks by default)\nTo check/set an R environment:\n\nGo to the File menu\nSelect Language\nChose R\nClick the Start Session icon (a circle with a vertical line, top right corner of the notebook)"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#r2-decomposition-1",
    "href": "R_SLR_Lecture_Slides.html#r2-decomposition-1",
    "title": "Simple Linear Regression",
    "section": "\\(R^2\\) decomposition",
    "text": "\\(R^2\\) decomposition\n\n\nCode\npar(mar = c(4, 4, 3, 1))\nbarplot(c(SST = SST, SSE = SSE, SSR = SSR),\n        col = c(\"gray70\", \"steelblue\", \"coral\"),\n        main = \"Decomposition: SST = SSE + SSR\", ylab = \"Sum of Squares\", border = NA)\ntext(0.7, SST/2, paste0(\"SST\\n\", round(SST, 1)), cex = 1, font = 2)\ntext(1.9, SSE/2, paste0(\"SSE\\n\", round(SSE, 1), \"\\n(\", round(SSE/SST*100,1), \"%)\"),\n     cex = 1, font = 2)\ntext(3.1, SSR/2, paste0(\"SSR\\n\", round(SSR, 1), \"\\n(\", round(SSR/SST*100,1), \"%)\"),\n     cex = 1, font = 2)"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#samples-20-different-lines",
    "href": "R_SLR_Lecture_Slides.html#samples-20-different-lines",
    "title": "Simple Linear Regression",
    "section": "20 samples ‚Üí 20 different lines",
    "text": "20 samples ‚Üí 20 different lines\n\n\nCode\nplot(NULL, xlim = c(0, 10), ylim = c(-5, 45),\n     main = \"20 Different Samples ‚Üí 20 Different Regression Lines\",\n     xlab = \"x\", ylab = \"y\")\nfor (i in 1:20) {\n  abline(a = beta0_estimates[i], b = beta1_estimates[i],\n         col = rgb(0.3, 0.3, 0.8, 0.3), lwd = 1.5)\n}\nabline(a = beta0_true, b = beta1_true, col = \"red\", lwd = 3)\nlegend(\"topleft\",\n       legend = c(\"True line: y = 5 + 3x\", \"Estimated lines (one per sample)\"),\n       col = c(\"red\", rgb(0.3, 0.3, 0.8, 0.5)), lwd = c(3, 2), bty = \"n\")\ngrid()"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#this-lecture",
    "href": "R_SLR_Lecture_Slides.html#this-lecture",
    "title": "Simple Linear Regression",
    "section": "This lecture",
    "text": "This lecture\n\nlibrary(qrcode)\n\nplot(qr_code('https://pfagandini.github.io/novasbe_econometrics/R_SLR_Lecture_Slides.html'))"
  },
  {
    "objectID": "R_SLR_Lecture_Slides.html#parenthesis-lets-check-la",
    "href": "R_SLR_Lecture_Slides.html#parenthesis-lets-check-la",
    "title": "Simple Linear Regression",
    "section": "Parenthesis: Let‚Äôs check LA",
    "text": "Parenthesis: Let‚Äôs check LA\n\nX_vect &lt;- cbind(1, mtcars$wt)\n\n# OLS\nbeta_hat &lt;- solve(t(X_vect) %*% X_vect) %*% t(X_vect) %*% y\nbeta_hat\n\n          [,1]\n[1,] 37.285126\n[2,] -5.344472\n\n\n\nsolve() is the inverse.\nt() is the traspose\n%*% is matrix/vector multiplication."
  }
]